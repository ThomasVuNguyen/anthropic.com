<!DOCTYPE html><html lang="en" class="anthropicsans_eac0b31f-module__tjnuGq__variable anthropicserif_87b6fa7d-module__quIBbW__variable anthropicmono_fae19af3-module__c5XAsG__variable copernicus_4da799c5-module__dijTSq__variable styrenea_f8492ab1-module__HimLXW__variable styreneb_278af5c6-module__wkOAdG__variable tiempostext_4eff4b4c-module__mpviCW__variable jetbrainsmono_7d7bdbc6-module__j_XgJq__variable"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/chunks/ec368b341879b233.css" nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh" data-precedence="next"/><link rel="stylesheet" href="/_next/static/chunks/38fee8473f816a4a.css" nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh" data-precedence="next"/><link rel="stylesheet" href="/_next/static/chunks/caf680e685668b99.css" nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh" data-precedence="next"/><link rel="stylesheet" href="/_next/static/chunks/ad266d0a6bc656af.css" nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh" data-precedence="next"/><link rel="stylesheet" href="/_next/static/chunks/e2c670ea67fc2bbb.css" nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh" data-precedence="next"/><link rel="stylesheet" href="/_next/static/chunks/758311c654d998de.css" nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh" href="/_next/static/chunks/f4386f5ba7642880.js"/><script src="/_next/static/chunks/573c649abe04b34a.js" async="" nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh"></script><script src="/_next/static/chunks/d0300bffb79131f2.js" async="" nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh"></script><script src="/_next/static/chunks/08dcfc3b15383cd6.js" async="" nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh"></script><script src="/_next/static/chunks/turbopack-07052ba808d12dd9.js" async="" nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh"></script><script src="/_next/static/chunks/d96012bcfc98706a.js" async="" nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh"></script><script src="/_next/static/chunks/d80b3790a119a285.js" async="" nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh"></script><script src="/_next/static/chunks/1fb574e7be3f9a05.js" async="" nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh"></script><script src="/_next/static/chunks/9a604444e87766dd.js" async="" nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh"></script><script src="/_next/static/chunks/c1896c986be1a2e2.js" async="" nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh"></script><script src="/_next/static/chunks/f5a33d7993e253c8.js" async="" nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh"></script><script src="/_next/static/chunks/496bc8a289f448d1.js" async="" nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh"></script><script src="/_next/static/chunks/b1040bb2d2fbd1e5.js" async="" nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh"></script><script src="/_next/static/chunks/2e3229a62c65aaec.js" async="" nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh"></script><script src="/_next/static/chunks/5c1988096a7b174a.js" async="" nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh"></script><script src="/_next/static/chunks/f563a58c137d4bc2.js" async="" nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh"></script><script src="/_next/static/chunks/2fd2aa01a4bc9178.js" async="" nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh"></script><script src="/_next/static/chunks/630870b77208f43d.js" async="" nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh"></script><script src="/_next/static/chunks/010986693eb1c9c2.js" async="" nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh"></script><script src="/_next/static/chunks/dabacb64939959b3.js" async="" nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh"></script><script src="/_next/static/chunks/c0d75d4ca01ae43d.js" async="" nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh"></script><script src="/_next/static/chunks/6c680011ff6c5ba2.js" async="" nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh"></script><script src="/_next/static/chunks/81716bb24f5a6f8f.js" async="" nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh"></script><script src="/_next/static/chunks/7c80d08c36d49463.js" async="" nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh"></script><script src="/_next/static/chunks/33647e5ba6496195.js" async="" nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh"></script><script src="/_next/static/chunks/667473da0b5c11bc.js" async="" nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh"></script><script src="/_next/static/chunks/2c9eb3077aa18f16.js" async="" nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh"></script><link rel="preload" href="/_next/static/chunks/7e4146583225b449.css" as="style" nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh"/><meta name="next-size-adjust" content=""/><meta name="theme-color" content="#141413"/><script src="/_next/static/chunks/a6dad97d9634a72d.js" noModule="" nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh"></script></head><body><div hidden=""><!--$?--><template id="B:0"></template><!--/$--></div><header class="SiteHeader-module-scss-module__zKj4Ca__header" data-theme="light"><div class="SiteHeader-module-scss-module__zKj4Ca__skipLinks"><a href="#main-content" class="SiteHeader-module-scss-module__zKj4Ca__skipLink">Skip to main content</a><a href="#footer" class="SiteHeader-module-scss-module__zKj4Ca__skipLink">Skip to footer</a></div><div class="page-wrapper SiteHeader-module-scss-module__zKj4Ca__root"><a href="/" aria-label="Home"><div class="SiteHeader-module-scss-module__zKj4Ca__logoDesktop"><div class="LogoWordmark-module-scss-module__Sdgt-q__logo-wrapper"><svg class="LogoWordmark-module-scss-module__Sdgt-q__logo-static" width="570" height="64" viewBox="0 0 570 64" fill="none" xmlns="http://www.w3.org/2000/svg" aria-label="Anthropic"><path d="M139.492 12.9945H160.265V62.9392H173.525V12.9945H194.298V1.06077H139.492V12.9945Z" fill="currentColor"></path><path d="M116.066 44.3757L88.221 1.06077H73.1934V62.9392H86.011V19.6243L113.856 62.9392H128.884V1.06077H116.066V44.3757Z" fill="currentColor"></path><path d="M247.337 25.7238H218.166V1.06077H204.906V62.9392H218.166V37.6575H247.337V62.9392H260.597V1.06077H247.337V25.7238Z" fill="currentColor"></path><path d="M24.663 1.06077L0 62.9392H13.7901L18.834 49.9447H44.6365L49.6796 62.9392H63.4696L38.8066 1.06077H24.663ZM23.2946 38.453L31.7348 16.7072L40.175 38.453H23.2946Z" fill="currentColor"></path><path d="M370.475 0C352.619 0 339.978 13.2597 339.978 32.0884C339.978 50.7403 352.619 64 370.475 64C388.243 64 400.796 50.7403 400.796 32.0884C400.796 13.2597 388.243 0 370.475 0ZM370.475 51.6243C360.044 51.6243 353.68 44.1989 353.68 32.0884C353.68 19.8011 360.044 12.3757 370.475 12.3757C380.818 12.3757 387.094 19.8011 387.094 32.0884C387.094 44.1989 380.818 51.6243 370.475 51.6243Z" fill="currentColor"></path><path d="M555.845 42.1657C553.547 48.1768 548.95 51.6243 542.674 51.6243C532.243 51.6243 525.878 44.1989 525.878 32.0884C525.878 19.8011 532.243 12.3757 542.674 12.3757C548.95 12.3757 553.547 15.8232 555.845 21.8343H569.901C566.453 8.57459 556.11 0 542.674 0C524.818 0 512.177 13.2597 512.177 32.0884C512.177 50.7403 524.818 64 542.674 64C556.199 64 566.541 55.337 569.989 42.1657H555.845Z" fill="currentColor"></path><path d="M471.337 1.06077L496 62.9392H509.525L484.862 1.06077H471.337Z" fill="currentColor"></path><path d="M443.403 1.06077H413.171V62.9392H426.431V40.4862H443.403C457.459 40.4862 466.033 33.0608 466.033 20.7735C466.033 8.48619 457.459 1.06077 443.403 1.06077ZM442.784 28.5525H426.431V12.9945H442.784C449.326 12.9945 452.773 15.6464 452.773 20.7735C452.773 25.9006 449.326 28.5525 442.784 28.5525Z" fill="currentColor"></path><path d="M329.812 19.8895C329.812 8.22099 321.238 1.06077 307.182 1.06077H276.95V62.9392H290.21V38.7182H304.971L318.232 62.9392H332.906L318.223 36.8734C325.593 34.0402 329.812 28.0743 329.812 19.8895ZM290.21 12.9945H306.564C313.105 12.9945 316.552 15.3812 316.552 19.8895C316.552 24.3978 313.105 26.7845 306.564 26.7845H290.21V12.9945Z" fill="currentColor"></path></svg><div class="LogoWordmark-module-scss-module__Sdgt-q__logo-lottie"></div></div></div><svg class="Icon-module-scss-module__lqbdHG__icon SiteHeader-module-scss-module__zKj4Ca__logoMobile" width="32" height="32" viewBox="0 0 46 32"><path d="M32.73 0h-6.945L38.45 32h6.945L32.73 0ZM12.665 0 0 32h7.082l2.59-6.72h13.25l2.59 6.72h7.082L19.929 0h-7.264Zm-.702 19.337 4.334-11.246 4.334 11.246h-8.668Z" fill="currentColor"></path></svg></a><div class="SiteHeader-module-scss-module__zKj4Ca__contentWrapper"><nav class="SiteHeader-module-scss-module__zKj4Ca__nav"><ul class="SiteHeader-module-scss-module__zKj4Ca__navList"><li class="body-3 SiteHeader-module-scss-module__zKj4Ca__navItem"><a href="/research" class="SiteHeader-module-scss-module__zKj4Ca__navText">Research</a></li><li class="body-3 SiteHeader-module-scss-module__zKj4Ca__navItem"><a href="/economic-futures" class="SiteHeader-module-scss-module__zKj4Ca__navText">Economic Futures</a></li><li class="body-3 SiteHeader-module-scss-module__zKj4Ca__navItem" data-category="Commitments"><button class="SiteHeader-module-scss-module__zKj4Ca__navText" aria-haspopup="menu" aria-expanded="false" aria-controls="nav-dropdown-Commitments"><span>Commitments</span><svg class="Icon-module-scss-module__lqbdHG__icon SiteHeader-module-scss-module__zKj4Ca__caretIcon" width="12" height="6.13" viewBox="0 0 8 5"><path d="M7.3016 0.231808C7.44932 0.0678162 7.70306 0.0546398 7.86724 0.20212C8.03137 0.349888 8.04461 0.603568 7.89692 0.767766L4.29684 4.76791L4.23434 4.82417C4.16662 4.87328 4.08425 4.89995 3.99918 4.89995C3.88588 4.89989 3.77733 4.85213 3.70152 4.76791L0.10144 0.767766L0.0537825 0.702139C-0.040206 0.541753 -0.0124254 0.331356 0.131128 0.20212C0.274775 0.0728844 0.486972 0.0674593 0.636608 0.1779L0.696765 0.231808L3.99918 3.90148L7.3016 0.231808Z" fill="currentColor"></path></svg></button></li><li class="body-3 SiteHeader-module-scss-module__zKj4Ca__navItem" data-category="Learn"><button class="SiteHeader-module-scss-module__zKj4Ca__navText" aria-haspopup="menu" aria-expanded="false" aria-controls="nav-dropdown-Learn"><span>Learn</span><svg class="Icon-module-scss-module__lqbdHG__icon SiteHeader-module-scss-module__zKj4Ca__caretIcon" width="12" height="6.13" viewBox="0 0 8 5"><path d="M7.3016 0.231808C7.44932 0.0678162 7.70306 0.0546398 7.86724 0.20212C8.03137 0.349888 8.04461 0.603568 7.89692 0.767766L4.29684 4.76791L4.23434 4.82417C4.16662 4.87328 4.08425 4.89995 3.99918 4.89995C3.88588 4.89989 3.77733 4.85213 3.70152 4.76791L0.10144 0.767766L0.0537825 0.702139C-0.040206 0.541753 -0.0124254 0.331356 0.131128 0.20212C0.274775 0.0728844 0.486972 0.0674593 0.636608 0.1779L0.696765 0.231808L3.99918 3.90148L7.3016 0.231808Z" fill="currentColor"></path></svg></button></li><li class="body-3 SiteHeader-module-scss-module__zKj4Ca__navItem"><a href="/news" class="SiteHeader-module-scss-module__zKj4Ca__navText">News</a></li></ul></nav><div class="SiteHeader-module-scss-module__zKj4Ca__claudeCtaWrapper"><a href="https://claude.ai/" class="SiteHeader-module-scss-module__zKj4Ca__claudeCtaButton body-3" target="_blank" rel="noopener noreferrer">Try Claude</a><div class="SiteHeader-module-scss-module__zKj4Ca__claudeCtaDropdownTrigger"><svg class="Icon-module-scss-module__lqbdHG__icon SiteHeader-module-scss-module__zKj4Ca__claudeCtaIcon" width="12" height="6.13" viewBox="0 0 8 5"><path d="M7.3016 0.231808C7.44932 0.0678162 7.70306 0.0546398 7.86724 0.20212C8.03137 0.349888 8.04461 0.603568 7.89692 0.767766L4.29684 4.76791L4.23434 4.82417C4.16662 4.87328 4.08425 4.89995 3.99918 4.89995C3.88588 4.89989 3.77733 4.85213 3.70152 4.76791L0.10144 0.767766L0.0537825 0.702139C-0.040206 0.541753 -0.0124254 0.331356 0.131128 0.20212C0.274775 0.0728844 0.486972 0.0674593 0.636608 0.1779L0.696765 0.231808L3.99918 3.90148L7.3016 0.231808Z" fill="currentColor"></path></svg></div></div><button class="SiteHeader-module-scss-module__zKj4Ca__mobileIcon" aria-label="Navigation menu"><svg class="Icon-module-scss-module__lqbdHG__icon" width="24" height="24" viewBox="0 0 40 40"><path d="M18.75 28C19.1641 28.0002 19.5 28.3359 19.5 28.75C19.4999 29.1641 19.164 29.4998 18.75 29.5H7.91699C7.50281 29.5 7.16705 29.1642 7.16699 28.75C7.16699 28.3358 7.50278 28 7.91699 28H18.75ZM32.084 19.25C32.4979 19.2504 32.834 19.586 32.834 20C32.8339 20.4139 32.4979 20.7496 32.084 20.75H7.91699C7.50281 20.75 7.16705 20.4142 7.16699 20C7.16699 19.5858 7.50278 19.25 7.91699 19.25H32.084ZM32.084 10.5C32.4979 10.5004 32.834 10.836 32.834 11.25C32.8339 11.6639 32.4979 11.9996 32.084 12H7.91699C7.50282 12 7.16706 11.6642 7.16699 11.25C7.16699 10.8358 7.50278 10.5 7.91699 10.5H32.084Z" fill="currentColor"></path></svg></button></div></div></header><main id="main-content" class=""><section class="page-wrapper HeroEngineering-module-scss-module__j1ivRa__hero" aria-label="Engineering Article Hero"><a class="body-2 bold HeroEngineering-module-scss-module__j1ivRa__hubLink" href="/engineering">Engineering at Anthropic</a><div class="HeroEngineering-module-scss-module__j1ivRa__content"><div class="HeroEngineering-module-scss-module__j1ivRa__header"><div class="HeroEngineering-module-scss-module__j1ivRa__heroImage"><img alt="" loading="lazy" width="1000" height="1000" decoding="async" data-nimg="1" style="color:transparent" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/590360609ccdf39715a8ec6916b52447dcb31f16-1000x1000.svg"/></div><h1 class="headline-1">Demystifying evals for AI agents</h1></div><div class="HeroEngineering-module-scss-module__j1ivRa__metadata"><p class="body-2 HeroEngineering-module-scss-module__j1ivRa__date">Published <!-- -->Jan 09, 2026</p><p class="body-large-1 HeroEngineering-module-scss-module__j1ivRa__summary">The capabilities that make agents useful also make them difficult to evaluate. The strategies that work across deployments combine techniques to match the complexity of the systems they measure. 
</p></div></div></section><div class="page-wrapper"><article><div class=""><div class="Body-module-scss-module__z40yvW__body" data-theme="ivory"><h2 class="Body-module-scss-module__z40yvW__reading-column headline-5 post-section" id="introduction">Introduction</h2><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Good evaluations help teams ship AI agents more confidently. Without them, it‚Äôs easy to get stuck in reactive loops‚Äîcatching issues only in production, where fixing one failure creates others. Evals make problems and behavioral changes visible before they affect users, and their value compounds over the lifecycle of an agent.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"></p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">As we described in <a href="/www.anthropic.com/engineering/building-effective-agents">Building effective agents</a>, agents operate over many turns: calling tools, modifying state, and adapting based on intermediate results. These same capabilities that make AI agents useful‚Äîautonomy, intelligence, and flexibility‚Äîalso make them harder to evaluate.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"></p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Through our internal work and with customers at the frontier of agent development, we‚Äôve learned how to design more rigorous and useful evals for agents. Here&#x27;s what&#x27;s worked across a range of agent architectures and use cases in real-world deployment.</p><h2 class="Body-module-scss-module__z40yvW__reading-column headline-5 post-section" id="the-structure-of-an-evaluation">The structure of an evaluation</h2><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">An <strong>evaluation </strong>(‚Äúeval‚Äù) is a test for an AI system: give an AI an input, then apply grading logic to its output to measure success. In this post, we focus on <strong>automated evals </strong>that can be run during development without real users.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"></p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"><strong>Single-turn evaluations</strong> are straightforward: a prompt, a response, and grading logic. For earlier LLMs, single-turn, non-agentic evals were the main evaluation method. As AI capabilities have advanced, <strong>multi-turn evaluations</strong> have become increasingly common.</p><div class="Body-module-scss-module__z40yvW__media-column Body-module-scss-module__z40yvW__inline"><figure class="ImageWithCaption-module-scss-module__Duq99q__e-imageWithCaption"><img loading="lazy" width="4584" height="2834" decoding="async" data-nimg="1" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fbd42e7b2f3e9bb5218142796d3ede4816588dec0-4584x2834.png&amp;w=3840&amp;q=75 1x" src="/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fbd42e7b2f3e9bb5218142796d3ede4816588dec0-4584x2834.png&amp;w=3840&amp;q=75"/><figcaption class="caption">In a simple eval, an agent processes a prompt, and a grader checks if the output matches expectations. For a more complex multi-turn eval, a coding agent receives tools, a task (building an MCP server in this case), and an environment, executes an &quot;agent loop&quot; (tool calls and reasoning), and updates the environment with the implementation. Grading then uses unit tests to verify the working MCP server.</figcaption></figure></div><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"><strong>Agent evaluations </strong>are even more complex. Agents use tools across many turns, modifying state in the environment and adapting as they go‚Äîwhich means mistakes can propagate and compound. Frontier models can also find creative solutions that surpass the limits of static evals. For instance, Opus 4.5 solved a <a href="https://github.com/sierra-research/tau2-bench">ùúè2-bench</a> problem about booking a flight by <a href="/www.anthropic.com/news/claude-opus-4-5">discovering</a> a loophole in the policy. It ‚Äúfailed‚Äù the evaluation as written, but actually came up with a better solution for the user.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"></p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">When building agent evaluations, we use the following definitions:</p><ul class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"><li>A<strong> task </strong>(a.k.a <strong>problem</strong> or <strong>test case</strong>) is a single test with defined inputs and success criteria.</li><li>Each attempt at a task is a <strong>trial</strong>. Because model outputs vary between runs, we run multiple trials to produce more consistent results.</li><li>A <strong>grader </strong>is logic that scores some aspect of the agent‚Äôs performance. A task can have multiple graders, each containing multiple assertions (sometimes called <strong>checks</strong>)<strong>.</strong></li><li>A <strong>transcript </strong>(also called a <strong>trace </strong>or<strong> trajectory</strong>) is the complete record of a trial, including outputs, tool calls, reasoning, intermediate results, and any other interactions. For the Anthropic API, this is the full messages array at the end of an eval run - containing all the calls to the API and all of the returned responses during the evaluation.</li><li>The <strong>outcome</strong> is the final state in the environment at the end of the trial. A flight-booking agent might say ‚ÄúYour flight has been booked‚Äù at the end of the transcript, but the outcome is whether a reservation exists in the environment‚Äôs SQL database.</li><li>An<strong> evaluation harness</strong> is the infrastructure that runs evals end-to-end. It provides instructions and tools, runs tasks concurrently, records all the steps, grades outputs, and aggregates results.</li><li>An <strong>agent harness </strong>(or <strong>scaffold</strong>) is the system that enables a model to act as an agent: it processes inputs, orchestrates tool calls, and returns results. When we evaluate ‚Äúan agent,‚Äù we‚Äôre evaluating the harness <em>and</em> the model working together. For example, <a href="https://claude.com/product/claude-code">Claude Code</a> is a flexible agent harness, and we used its core primitives through the <a href="https://platform.claude.com/docs/en/agent-sdk/overview">Agent SDK</a> to build our <a href="/www.anthropic.com/engineering/effective-harnesses-for-long-running-agents">long-running agent harness</a>.</li><li>An <strong>evaluation suite</strong> is a collection of tasks designed to measure specific capabilities or behaviors. Tasks in a suite typically share a broad goal. For instance, a customer support eval suite might test refunds, cancellations, and escalations.</li></ul><div class="Body-module-scss-module__z40yvW__media-column Body-module-scss-module__z40yvW__inline"><figure class="ImageWithCaption-module-scss-module__Duq99q__e-imageWithCaption"><img loading="lazy" width="4584" height="2580" decoding="async" data-nimg="1" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0205b36f9639fc27f2f6566f73cb56b06f59d555-4584x2580.png&amp;w=3840&amp;q=75 1x" src="/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0205b36f9639fc27f2f6566f73cb56b06f59d555-4584x2580.png&amp;w=3840&amp;q=75"/><figcaption class="caption">Components of evaluations for agents.</figcaption></figure></div><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"></p><h2 class="Body-module-scss-module__z40yvW__reading-column headline-5 post-section" id="why-build-evaluations">Why build evaluations?</h2><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">When teams first start building agents, they can get surprisingly far through a combination of manual testing, <a href="https://en.wikipedia.org/wiki/Eating_your_own_dog_food">dogfooding</a>, and intuition. More rigorous evaluation may even seem like overhead that slows down shipping. But after the early prototyping stages, once an agent is in production and has started scaling, building without evals starts to break down.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"></p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">The breaking point often comes when users report the agent feels worse after changes, and the team is ‚Äúflying blind‚Äù with no way to verify except to guess and check. Absent evals, debugging is reactive: wait for complaints, reproduce manually, fix the bug, and hope nothing else regressed. Teams can&#x27;t distinguish real regressions from noise, automatically test changes against hundreds of scenarios before shipping, or measure improvements.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"></p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">We‚Äôve seen this progression play out many times. For instance, Claude Code started with fast iteration based on feedback from Anthropic employees and external users. Later, we added evals‚Äîfirst for narrow areas like concision and file edits, and then for more complex behaviors like over-engineering. These evals helped identify issues, guide improvements, and focus research-product collaborations. Combined with production monitoring, A/B tests, user research, and more, evals provide signals to continue improving Claude Code as it scales.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"></p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Writing evals is useful at any stage in the agent lifecycle. Early on, evals force product teams to specify what success means for the agent, while later they help uphold a consistent quality bar.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"></p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"><a href="https://www.descript.com/">Descript</a>‚Äôs agent helps users edit videos, so they built evals around three dimensions of a successful editing workflow: don‚Äôt break things, do what I asked, and do it well. They evolved from manual grading to LLM graders with criteria defined by the product team and periodic human calibration, and now regularly run two separate suites for quality benchmarking and regression testing. The <a href="https://bolt.new/">Bolt</a> AI team started building evals later, after they already had a widely used agent. In 3 months, they built an eval system that runs their agent and grades outputs with static analysis, uses browser agents to test apps, and employs LLM judges for behaviors like instruction following.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"></p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Some teams create evals at the start of development; others add them once at scale when evals become a bottleneck for improving the agent. Evals are especially useful at the start of agent development to explicitly encode expected behavior. Two engineers reading the same initial spec could come away with different interpretations on how the AI should handle edge cases. An eval suite resolves this ambiguity. Regardless of when they‚Äôre created, evals help accelerate development.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Evals also shape how quickly you can adopt new models. When more powerful models come out, teams without evals face weeks of testing while competitors with evals can quickly determine the model‚Äôs strengths, tune their prompts, and upgrade in days.¬†</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"></p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Once evals exist, you get baselines and regression tests for free: latency, token usage, cost per task, and error rates can be tracked on a static bank of tasks. Evals can also become the highest-bandwidth communication channel between product and research teams, defining metrics researchers can optimize against. Clearly, evals have wide-ranging benefits beyond tracking regressions and improvements. Their compounding value is easy to miss given that costs are visible upfront while benefits accumulate later.</p><h2 class="Body-module-scss-module__z40yvW__reading-column headline-5 post-section" id="how-to-evaluate-ai-agents-">How to evaluate AI agents </h2><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">We see several common types of agents deployed at scale today, including coding agents, research agents, computer use agents, and conversational agents. Each type may be deployed across a wide variety of industries, but they can be evaluated using similar techniques. You don‚Äôt need to invent an evaluation from scratch. The sections below describe proven techniques for several agent types. Use these methods as a foundation, then extend them to your domain.</p><h3 class="Body-module-scss-module__z40yvW__reading-column headline-6 post-subsection" id="types-of-graders-for-agents">Types of graders for agents</h3><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Agent evaluations typically combine three types of graders: code-based, model-based, and human. Each grader evaluates some portion of either the transcript or the outcome. An essential component of effective evaluation design is to choose the right graders for the job.</p><div class="Body-module-scss-module__z40yvW__media-column Body-module-scss-module__z40yvW__inline"><div class="Table-module-scss-module__Z3bHXa__root"><div class="Table-module-scss-module__Z3bHXa__tableWrapper" role="region" tabindex="0"><p class="headline-6">Code-based graders</p><table class="Table-module-scss-module__Z3bHXa__table"><tbody><tr class="Table-module-scss-module__Z3bHXa__row"><th class="body-3"><strong>Methods</strong></th><th class="body-3"><strong>Strengths</strong></th><th class="body-3"><strong>Weaknesses</strong></th></tr><tr class="Table-module-scss-module__Z3bHXa__row"><td class="body-3">‚Ä¢  String match checks (exact, regex, fuzzy, etc.)<br/>‚Ä¢  Binary tests (fail-to-pass, pass-to-pass)<br/>‚Ä¢  Static analysis (lint, type, security)<br/>‚Ä¢  Outcome verification<br/>‚Ä¢  Tool calls verification (tools used, parameters)<br/>‚Ä¢  Transcript analysis (turns taken, token usage)</td><td class="body-3">‚Ä¢  Fast<br/>‚Ä¢  Cheap<br/>‚Ä¢  Objective<br/>‚Ä¢  Reproducible<br/>‚Ä¢  Easy to debug<br/>‚Ä¢  Verify specific conditions<br/></td><td class="body-3">‚Ä¢  Brittle to valid variations that don‚Äôt match expected patterns exactly<br/>‚Ä¢  Lacking in nuance<br/>‚Ä¢  Limited for evaluating some more subjective tasks<br/></td></tr></tbody></table></div></div></div><div class="Body-module-scss-module__z40yvW__media-column Body-module-scss-module__z40yvW__inline"><div class="Table-module-scss-module__Z3bHXa__root"><div class="Table-module-scss-module__Z3bHXa__tableWrapper" role="region" tabindex="0"><p class="headline-6">Model-based graders</p><table class="Table-module-scss-module__Z3bHXa__table"><tbody><tr class="Table-module-scss-module__Z3bHXa__row"><th class="body-3"><strong>Methods</strong></th><th class="body-3"><strong>Strengths</strong></th><th class="body-3"><strong>Weaknesses</strong></th></tr><tr class="Table-module-scss-module__Z3bHXa__row"><td class="body-3"><ul class="TextBlock-module-scss-module__YX8MMq__list body-2 body-3"><li>Rubric-based scoring</li><li>Natural language assertions</li><li>Pairwise comparison</li><li>Reference-based evaluation</li><li>Multi-judge consensus</li></ul><br/></td><td class="body-3"><ul class="TextBlock-module-scss-module__YX8MMq__list body-2 body-3"><li>Flexible</li><li>Scalable</li><li>Captures nuance</li><li>Handles open-ended tasks</li><li>Handles freeform output</li></ul><br/></td><td class="body-3"><ul class="TextBlock-module-scss-module__YX8MMq__list body-2 body-3"><li>Non-deterministic</li><li>More expensive than code</li><li>Requires calibration with human graders for accuracy</li></ul><br/></td></tr></tbody></table></div></div></div><div class="Body-module-scss-module__z40yvW__media-column Body-module-scss-module__z40yvW__inline"><div class="Table-module-scss-module__Z3bHXa__root"><div class="Table-module-scss-module__Z3bHXa__tableWrapper" role="region" tabindex="0"><p class="headline-6">Human graders</p><table class="Table-module-scss-module__Z3bHXa__table"><tbody><tr class="Table-module-scss-module__Z3bHXa__row"><th class="body-3"><strong>Methods</strong></th><th class="body-3"><strong>Strengths</strong></th><th class="body-3"><strong>Weaknesses</strong></th></tr><tr class="Table-module-scss-module__Z3bHXa__row"><td class="body-3"><ul class="TextBlock-module-scss-module__YX8MMq__list body-2 body-3"><li>SME review</li><li>Crowdsourced judgment</li><li>Spot-check sampling</li><li>A/B testing</li><li>Inter-annotator agreement</li></ul><br/></td><td class="body-3"><ul class="TextBlock-module-scss-module__YX8MMq__list body-2 body-3"><li>Gold standard quality</li><li>Matches expert user judgment</li><li>Used to calibrate model-based graders</li></ul><br/></td><td class="body-3"><ul class="TextBlock-module-scss-module__YX8MMq__list body-2 body-3"><li>Expensive</li><li>Slow</li><li>Often requires access to human experts at scale</li></ul><br/></td></tr></tbody></table></div></div></div><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">For each task, scoring can be weighted (combined grader scores must hit a threshold), binary (all graders must pass), or a hybrid.</p><h3 class="Body-module-scss-module__z40yvW__reading-column headline-6 post-subsection" id="capability-vs-regression-evals">Capability vs. regression evals</h3><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"><strong>Capability or ‚Äúquality‚Äù evals</strong> ask, ‚ÄúWhat can this agent do well?‚Äù They should start at a low pass rate, targeting tasks the agent struggles with and giving teams a hill to climb.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"><strong>Regression evals</strong> ask, ‚ÄúDoes the agent still handle all the tasks it used to?‚Äù and should have a nearly 100% pass rate. They protect against backsliding, as a decline in score signals that something is broken and needs to be improved. As teams hill-climb on capability evals, it‚Äôs important to also run regression evals to make sure changes don‚Äôt cause issues elsewhere.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">After an agent is launched and optimized, capability evals with high pass rates can ‚Äúgraduate‚Äù to become a regression suite that is run continuously to catch any drift. Tasks that once measured ‚ÄúCan we do this at all?‚Äù then measure ‚ÄúCan we still do this reliably?‚Äù</p><h3 class="Body-module-scss-module__z40yvW__reading-column headline-6 post-subsection" id="evaluating-coding-agents">Evaluating coding agents</h3><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"><strong>Coding agents</strong> write, test, and debug code, navigating codebases and running commands much like a human developer. Effective evals for modern coding agents usually rely on well-specified tasks, stable test environments, and thorough tests for the generated code.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Deterministic graders are natural for coding agents because software is generally straightforward to evaluate: does the code run and do the tests pass? Two widely used coding agent benchmarks, <a href="https://www.swebench.com/SWE-bench/">SWE-bench Verified</a> and <a href="https://www.tbench.ai/">Terminal-Bench</a>, follow this approach. SWE-bench Verified gives agents GitHub issues from popular Python repositories and grades solutions by running the test suite; a solution passes only if it fixes the failing tests without breaking existing ones. LLMs have progressed from 40% to &gt;80% on this eval in just one year. Terminal-Bench takes a different track: it tests end-to-end technical tasks, such as building a Linux kernel from source or training an ML model.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Once you have a set of pass-or-fail tests for validating the key <em>outcomes</em> of a coding task, it‚Äôs often useful to also grade the transcript<em>. </em>For instance, heuristics-based code quality rules can evaluate the generated code based on more than passing tests, and model-based graders with clear rubrics can assess behaviors like how the agent calls tools or interacts with the user.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"></p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"><strong>Example: Theoretical evaluation for a coding agent</strong></p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Consider a coding task where the agent must fix an authentication bypass vulnerability. As shown in the illustrative YAML file below, one could evaluate this agent using both graders and metrics.¬†</p><div class="Body-module-scss-module__z40yvW__media-column Body-module-scss-module__z40yvW__inline"><div class="CodeBlock-module-scss-module__PbWBnq__codeBlock"><pre class="" style="--height:300px;--height-expanded:0px"><code class="plaintext">task:
  id: &quot;fix-auth-bypass_1&quot;
  desc: &quot;Fix authentication bypass when password field is empty and ...&quot;
  graders:
    - type: deterministic_tests
      required: [test_empty_pw_rejected.py, test_null_pw_rejected.py]
    - type: llm_rubric
      rubric: prompts/code_quality.md
    - type: static_analysis
      commands: [ruff, mypy, bandit]
    - type: state_check
      expect:
        security_logs: {event_type: &quot;auth_blocked&quot;}
    - type: tool_calls
      required:
        - {tool: read_file, params: {path: &quot;src/auth/*&quot;}}
        - {tool: edit_file}
        - {tool: run_tests}
  tracked_metrics:
    - type: transcript
      metrics:
        - n_turns
        - n_toolcalls
        - n_total_tokens
    - type: latency
      metrics:
        - time_to_first_token
        - output_tokens_per_sec
        - time_to_last_token</code></pre><div class="CodeBlock-module-scss-module__PbWBnq__controls"><button aria-label="Copy code"><svg class="Icon-module-scss-module__lqbdHG__icon" width="11" height="15" viewBox="0 0 11 15"><path d="M5.4 0C6.39875 0 7.26819 0.543814 7.73525 1.35H9.45C10.1956 1.35 10.8 1.95442 10.8 2.7V13.5C10.8 14.2456 10.1956 14.85 9.45 14.85H1.35C0.604415 14.85 2.17436e-08 14.2456 0 13.5V2.7C1.7395e-07 1.95442 0.604415 1.35 1.35 1.35H3.06475C3.53181 0.543814 4.40125 0 5.4 0ZM1.35 2.25C1.10147 2.25 0.9 2.45147 0.9 2.7V13.5C0.9 13.7485 1.10147 13.95 1.35 13.95H9.45C9.69853 13.95 9.9 13.7485 9.9 13.5V2.7C9.9 2.45147 9.69853 2.25 9.45 2.25H8.06221C8.08677 2.39637 8.1 2.54665 8.1 2.7V3.6C8.1 3.84853 7.89853 4.05 7.65 4.05H3.15C2.90147 4.05 2.7 3.84853 2.7 3.6V2.7C2.7 2.54665 2.71323 2.39637 2.73779 2.25H1.35ZM7.68603 10.6233C7.78376 10.395 8.04828 10.2886 8.27666 10.386C8.50499 10.4838 8.61143 10.7483 8.51396 10.9767C8.24856 11.5967 7.73014 12.15 7.01982 12.15C6.58192 12.1499 6.21722 11.9397 5.93965 11.6332C5.66215 11.9395 5.29801 12.1499 4.86035 12.15C4.42229 12.15 4.05692 11.9398 3.7793 11.6332C3.50175 11.9395 3.13773 12.15 2.7 12.15C2.45147 12.15 2.25 11.9485 2.25 11.7C2.25 11.4515 2.45147 11.25 2.7 11.25C2.8912 11.25 3.16726 11.0879 3.36621 10.6233L3.39697 10.5636C3.47806 10.4321 3.62261 10.35 3.78018 10.35C3.9602 10.3501 4.1233 10.4578 4.19414 10.6233C4.39309 11.0878 4.66917 11.25 4.86035 11.25C5.05156 11.2498 5.32773 11.0877 5.52656 10.6233L5.55732 10.5636C5.63837 10.4323 5.78229 10.3501 5.93965 10.35C6.11974 10.35 6.28275 10.4578 6.35361 10.6233C6.55251 11.0878 6.82862 11.2499 7.01982 11.25C7.21102 11.25 7.48708 11.0879 7.68603 10.6233ZM7.68603 7.02334C7.78376 6.79501 8.04828 6.68857 8.27666 6.78604C8.50499 6.88376 8.61143 7.14828 8.51396 7.37666C8.24856 7.99675 7.73014 8.55 7.01982 8.55C6.58192 8.54994 6.21722 8.3397 5.93965 8.0332C5.66215 8.33947 5.29801 8.54989 4.86035 8.55C4.42229 8.55 4.05692 8.33983 3.7793 8.0332C3.50175 8.33945 3.13773 8.55 2.7 8.55C2.45147 8.55 2.25 8.34853 2.25 8.1C2.25 7.85147 2.45147 7.65 2.7 7.65C2.8912 7.65 3.16726 7.48791 3.36621 7.02334L3.39697 6.96357C3.47806 6.83213 3.62261 6.75 3.78018 6.75C3.9602 6.75007 4.1233 6.85783 4.19414 7.02334C4.39309 7.48782 4.66917 7.65 4.86035 7.65C5.05156 7.6498 5.32773 7.48772 5.52656 7.02334L5.55732 6.96357C5.63837 6.83232 5.78229 6.75012 5.93965 6.75C6.11974 6.75 6.28275 6.85778 6.35361 7.02334C6.55251 7.48782 6.82862 7.6499 7.01982 7.65C7.21102 7.65 7.48708 7.48786 7.68603 7.02334ZM5.4 0.9C4.40589 0.9 3.6 1.70589 3.6 2.7V3.15H7.2V2.7C7.2 1.70589 6.39411 0.9 5.4 0.9Z" fill="currentColor"></path></svg><span class="body-3">Copy</span></button></div></div></div><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Note that this example showcases the full range of available graders for illustration. In practice, coding evaluations typically rely on unit tests for correctness verification and an LLM rubric for assessing overall code quality, with additional graders and metrics added only as needed.</p><h3 class="Body-module-scss-module__z40yvW__reading-column headline-6 post-subsection" id="evaluating-conversational-agents">Evaluating conversational agents</h3><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"><strong>Conversational agents </strong>interact with users in domains like support, sales, or coaching. Unlike traditional chatbots, they maintain state, use tools, and take actions mid-conversation. While coding and research agents can also involve many turns of interaction with the user, conversational agents present a distinct challenge: the quality of the interaction itself is part of what you&#x27;re evaluating. Effective evals for conversational agents usually rely on verifiable end-state outcomes and rubrics that capture both task completion and interaction quality. Unlike most other evals, they often require a second LLM to simulate the user. We use this approach in our <a href="https://alignment.anthropic.com/2025/automated-auditing/">alignment auditing agents</a> to stress-test models through extended, adversarial conversations.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Success for conversational agents can be multidimensional: is the ticket resolved (state check), did it finish in &lt;10 turns (transcript constraint), and was the tone appropriate (LLM rubric)? Two benchmarks that incorporate multidimensionality are <a href="https://arxiv.org/abs/2406.12045">ùúè-Bench</a> and its successor, <a href="https://arxiv.org/abs/2506.07982">œÑ2-Bench</a>. These simulate multi-turn interactions across domains like retail support and airline booking, where one model plays a user persona while the agent navigates realistic scenarios.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"><br/><strong>Example: Theoretical evaluation for a conversational agent</strong></p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Consider a support task where the agent must handle a refund for a frustrated customer.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"></p><div class="Body-module-scss-module__z40yvW__media-column Body-module-scss-module__z40yvW__inline"><div class="CodeBlock-module-scss-module__PbWBnq__codeBlock"><pre class="" style="--height:300px;--height-expanded:0px"><code class="plaintext">graders:
  - type: llm_rubric
    rubric: prompts/support_quality.md
    assertions:
      - &quot;Agent showed empathy for customer&#x27;s frustration&quot;
      - &quot;Resolution was clearly explained&quot;
      - &quot;Agent&#x27;s response grounded in fetch_policy tool results&quot;
  - type: state_check
    expect:
      tickets: {status: resolved}
      refunds: {status: processed}
  - type: tool_calls
    required:
      - {tool: verify_identity}
      - {tool: process_refund, params: {amount: &quot;&lt;=100&quot;}}
      - {tool: send_confirmation}
  - type: transcript
    max_turns: 10
tracked_metrics:
  - type: transcript
    metrics:
      - n_turns
      - n_toolcalls
      - n_total_tokens
  - type: latency
    metrics:
      - time_to_first_token
      - output_tokens_per_sec
      - time_to_last_token</code></pre><div class="CodeBlock-module-scss-module__PbWBnq__controls"><button aria-label="Copy code"><svg class="Icon-module-scss-module__lqbdHG__icon" width="11" height="15" viewBox="0 0 11 15"><path d="M5.4 0C6.39875 0 7.26819 0.543814 7.73525 1.35H9.45C10.1956 1.35 10.8 1.95442 10.8 2.7V13.5C10.8 14.2456 10.1956 14.85 9.45 14.85H1.35C0.604415 14.85 2.17436e-08 14.2456 0 13.5V2.7C1.7395e-07 1.95442 0.604415 1.35 1.35 1.35H3.06475C3.53181 0.543814 4.40125 0 5.4 0ZM1.35 2.25C1.10147 2.25 0.9 2.45147 0.9 2.7V13.5C0.9 13.7485 1.10147 13.95 1.35 13.95H9.45C9.69853 13.95 9.9 13.7485 9.9 13.5V2.7C9.9 2.45147 9.69853 2.25 9.45 2.25H8.06221C8.08677 2.39637 8.1 2.54665 8.1 2.7V3.6C8.1 3.84853 7.89853 4.05 7.65 4.05H3.15C2.90147 4.05 2.7 3.84853 2.7 3.6V2.7C2.7 2.54665 2.71323 2.39637 2.73779 2.25H1.35ZM7.68603 10.6233C7.78376 10.395 8.04828 10.2886 8.27666 10.386C8.50499 10.4838 8.61143 10.7483 8.51396 10.9767C8.24856 11.5967 7.73014 12.15 7.01982 12.15C6.58192 12.1499 6.21722 11.9397 5.93965 11.6332C5.66215 11.9395 5.29801 12.1499 4.86035 12.15C4.42229 12.15 4.05692 11.9398 3.7793 11.6332C3.50175 11.9395 3.13773 12.15 2.7 12.15C2.45147 12.15 2.25 11.9485 2.25 11.7C2.25 11.4515 2.45147 11.25 2.7 11.25C2.8912 11.25 3.16726 11.0879 3.36621 10.6233L3.39697 10.5636C3.47806 10.4321 3.62261 10.35 3.78018 10.35C3.9602 10.3501 4.1233 10.4578 4.19414 10.6233C4.39309 11.0878 4.66917 11.25 4.86035 11.25C5.05156 11.2498 5.32773 11.0877 5.52656 10.6233L5.55732 10.5636C5.63837 10.4323 5.78229 10.3501 5.93965 10.35C6.11974 10.35 6.28275 10.4578 6.35361 10.6233C6.55251 11.0878 6.82862 11.2499 7.01982 11.25C7.21102 11.25 7.48708 11.0879 7.68603 10.6233ZM7.68603 7.02334C7.78376 6.79501 8.04828 6.68857 8.27666 6.78604C8.50499 6.88376 8.61143 7.14828 8.51396 7.37666C8.24856 7.99675 7.73014 8.55 7.01982 8.55C6.58192 8.54994 6.21722 8.3397 5.93965 8.0332C5.66215 8.33947 5.29801 8.54989 4.86035 8.55C4.42229 8.55 4.05692 8.33983 3.7793 8.0332C3.50175 8.33945 3.13773 8.55 2.7 8.55C2.45147 8.55 2.25 8.34853 2.25 8.1C2.25 7.85147 2.45147 7.65 2.7 7.65C2.8912 7.65 3.16726 7.48791 3.36621 7.02334L3.39697 6.96357C3.47806 6.83213 3.62261 6.75 3.78018 6.75C3.9602 6.75007 4.1233 6.85783 4.19414 7.02334C4.39309 7.48782 4.66917 7.65 4.86035 7.65C5.05156 7.6498 5.32773 7.48772 5.52656 7.02334L5.55732 6.96357C5.63837 6.83232 5.78229 6.75012 5.93965 6.75C6.11974 6.75 6.28275 6.85778 6.35361 7.02334C6.55251 7.48782 6.82862 7.6499 7.01982 7.65C7.21102 7.65 7.48708 7.48786 7.68603 7.02334ZM5.4 0.9C4.40589 0.9 3.6 1.70589 3.6 2.7V3.15H7.2V2.7C7.2 1.70589 6.39411 0.9 5.4 0.9Z" fill="currentColor"></path></svg><span class="body-3">Copy</span></button></div></div></div><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">As in our coding agent example, this task showcases multiple grader types for illustration. In practice, conversational agent evaluations typically use model-based graders to assess both communication quality and goal completion, because many tasks‚Äîlike answering a question‚Äîmay have multiple ‚Äúcorrect‚Äù solutions.</p><h3 class="Body-module-scss-module__z40yvW__reading-column headline-6 post-subsection" id="evaluating-research-agents">Evaluating research agents</h3><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"><strong>Research agents</strong> gather, synthesize, and analyze information, then produce outputs like an answer or report. Unlike coding agents where unit tests provide binary pass/fail signals, research quality can only be judged relative to the task. What counts as ‚Äúcomprehensive,‚Äù ‚Äúwell-sourced,‚Äù or even ‚Äúcorrect‚Äù depends on context: a market scan, due diligence for an acquisition, and a scientific report each require different standards.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Research evals face unique challenges: experts may disagree on whether a synthesis is comprehensive, ground truth shifts as reference content changes constantly, and longer, more open-ended outputs create more room for mistakes. A benchmark like <a href="http://arxiv.org/abs/2504.12516">BrowseComp</a>, for example, tests whether AI agents can find needles in haystacks across the open web‚Äîquestions designed to be easy to verify but hard to solve.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">One strategy to build research agent evals is to combine grader types. Groundedness checks verify that claims are supported by retrieved sources, coverage checks define key facts a good answer must include, and source quality checks confirm the consulted sources are authoritative, rather than simply the first retrieved. For tasks with objectively correct answers (‚ÄúWhat was Company X‚Äôs Q3 revenue?‚Äù), exact match works. An LLM can flag unsupported claims and gaps in coverage but also verify the open-ended synthesis for coherence and completeness.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Given the subjective nature of research quality, LLM-based rubrics should be frequently calibrated against expert human judgment to grade these agents effectively.</p><h3 class="Body-module-scss-module__z40yvW__reading-column headline-6 post-subsection" id="computer-use-agents">Computer use agents</h3><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"><strong>Computer use agents</strong> interact with software through the same interface as humans‚Äîscreenshots, mouse clicks, keyboard inputs, and scrolling‚Äîrather than through APIs or code execution. They can use any application with a graphical user interface (GUI), from design tools to legacy enterprise software. Evaluation requires running the agent in a real or sandboxed environment where it can use software applications and checking whether it achieved the intended outcome. For instance, <a href="https://arxiv.org/abs/2307.13854">WebArena</a> tests browser-based tasks, using URL and page state checks to verify the agent navigated correctly, along with backend state verification for tasks that modify data (confirming an order was actually placed, not just that the confirmation page appeared). <a href="https://os-world.github.io/">OSWorld</a> extends this to full operating system control, with evaluation scripts that inspect diverse artifacts after task completion: file system state, application configs, database contents, and UI element properties.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Browser use agents require a balance between token efficiency and latency. DOM-based interactions execute quickly but consume many tokens, while screenshot-based interactions are slower but more token-efficient. For example, when asking Claude to summarize Wikipedia, it is more efficient to extract the text from the DOM. When finding a new laptop case on Amazon, it is more efficient to take screenshots (as extracting the entire DOM is token-intensive). In our Claude for Chrome product, we developed evals to check that the agent was selecting the right tool for each context. This enabled us to complete browser-based tasks faster and more accurately.</p><h3 class="Body-module-scss-module__z40yvW__reading-column headline-6 post-subsection" id="how-to-think-about-non-determinism-in-evaluations-for-agents">How to think about non-determinism in evaluations for agents</h3><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Regardless of agent type, agent behavior varies between runs, which makes evaluation results harder to interpret than they first appear. Each task has its own success rate‚Äîmaybe 90% on one task, 50% on another‚Äîand a task that passed on one eval run might fail on the next. Sometimes, what we want to measure is how <em>often</em> (what proportion of the trials) an agent succeeds for a task.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Two metrics help capture this nuance:</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"><a href="https://proceedings.neurips.cc/paper/2019/file/7298332f04ac004a0ca44cc69ecf6f6b-Paper.pdf"><strong>pass@k</strong></a> measures the likelihood that an agent gets at least one correct solution in <em>k</em> attempts. As <em>k</em> increases, pass@k score rises: more ‚Äúshots on goal‚Äù means higher odds of at least 1 success. A score of 50% pass@1 means that a model succeeds at half the tasks in the eval on its first try. In coding, we‚Äôre often most interested in the agent finding the solution on the first try‚Äîpass@1. In other cases, proposing many solutions is valid as long as one works.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"><a href="https://arxiv.org/abs/2406.12045"><strong>pass^k</strong></a> measures the probability that <em>all k</em> trials succeed. As <em>k</em> increases, pass^k falls since demanding consistency across more trials is a harder bar to clear. If your agent has a 75% per-trial success rate and you run 3 trials, the probability of passing all three is (0.75)¬≥ ‚âà 42%. This metric especially matters for customer-facing agents where users expect reliable behavior every time.</p><div class="Body-module-scss-module__z40yvW__media-column Body-module-scss-module__z40yvW__inline"><figure class="ImageWithCaption-module-scss-module__Duq99q__e-imageWithCaption"><img loading="lazy" width="4584" height="2580" decoding="async" data-nimg="1" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F3ddac5be07a0773922ec9df06afec55922f8194a-4584x2580.png&amp;w=3840&amp;q=75 1x" src="/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F3ddac5be07a0773922ec9df06afec55922f8194a-4584x2580.png&amp;w=3840&amp;q=75"/><figcaption class="caption">pass@k and pass^k diverge as trials increase. At k=1, they&#x27;re identical (both equal the per-trial success rate). By k=10, they tell opposite stories: pass@k approaches 100% while pass^k falls to 0%.</figcaption></figure></div><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Both metrics are useful, and which to use depends on product requirements: pass@k for tools where one success matters, pass^k for agents where consistency is essential.</p><h2 class="Body-module-scss-module__z40yvW__reading-column headline-5 post-section" id="going-from-zero-to-one-a-roadmap-to-great-evals-for-agents">Going from zero to one: a roadmap to great evals for agents</h2><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">This section lays out our practical, field-tested advice for going from no evals to evals you can trust. Think of this as a roadmap for eval-driven agent development: define success early, measure it clearly, and iterate continuously.</p><h3 class="Body-module-scss-module__z40yvW__reading-column headline-6 post-subsection" id="collect-tasks-for-the-initial-eval-dataset">Collect tasks for the initial eval dataset</h3><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"><strong>Step 0. Start early</strong></p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">We see teams delay building evals because they think they need hundreds of tasks. In reality, 20-50 simple tasks drawn from real failures is a great start. After all, in early agent development, each change to the system often has a clear, noticeable impact, and this large effect size means small sample sizes suffice. More mature agents may need larger, more difficult evals to detect smaller effects, but it‚Äôs best to take the 80/20 approach in the beginning. Evals get harder to build the longer you wait. Early on, product requirements naturally translate into test cases. Wait too long and you&#x27;re reverse-engineering success criteria from a live system.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"></p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"><strong>Step 1. Start with what you already test manually</strong></p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Begin with the manual checks you run during development‚Äîthe behaviors you verify before each release and common tasks end users try. If you&#x27;re already in production, look at your bug tracker and support queue. Converting user-reported failures into test cases ensures your suite reflects actual usage; prioritizing by user impact helps you invest effort where it counts.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"></p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"><strong>Step 2: Write unambiguous tasks with reference solutions</strong></p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Getting task quality right is harder than it seems. A good task is one where two domain experts would independently reach the same pass/fail verdict. Could they pass the task themselves? If not, the task needs refinement. Ambiguity in task specifications becomes noise in metrics. The same applies to criteria for model-based graders: vague rubrics produce inconsistent judgments.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Each task should be passable by an agent that follows instructions correctly. This can be subtle. For instance, auditing Terminal-Bench revealed that if a task asks the agent to write a script but doesn‚Äôt specify a filepath, and the tests assume a particular filepath for the script, the agent might fail through no fault of its own. Everything the grader checks should be clear from the task description; agents shouldn‚Äôt fail due to ambiguous specs. With frontier models, a 0% pass rate across many trials (i.e. 0% pass@100) is most often a signal of a broken task, not an incapable agent, and a sign to double-check your task specification and graders. For each task, it‚Äôs useful to create a reference solution: a known working output that passes all graders. This proves that the task is solvable and verifies graders are correctly configured.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"><strong></strong></p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"><strong>Step 3: Build balanced problem sets</strong></p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Test both the cases where a behavior <em>should</em> occur and where it <em>shouldn&#x27;t</em>. One-sided evals create one-sided optimization. For instance, if you only test whether the agent searches when it should, you might end up with an agent that searches for almost everything. Try to avoid <a href="https://developers.google.com/machine-learning/crash-course/overfitting/imbalanced-datasets">class-imbalanced</a> evals. We learned this firsthand when building evals for web search in <a href="http://claude.ai/redirect/website.v1.2ee29f6b-9891-40ef-95a4-3c7b0d20b046">Claude.ai</a>. The challenge was preventing the model from searching when it shouldn‚Äôt, while preserving its ability to do extensive research when appropriate. The team built evals covering both directions: queries where the model should search (like finding the weather) and queries where it should answer from existing knowledge (like ‚Äúwho founded Apple?‚Äù). Striking the right balance between undertriggering (not searching when it should) or overtriggering (searching when it shouldn‚Äôt) was difficult, and took many rounds of refinements to both the prompts and the eval. As more example problems come up, we continue to add to evals to improve our coverage.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"></p><h3 class="Body-module-scss-module__z40yvW__reading-column headline-6 post-subsection" id="design-the-eval-harness-and-graders">Design the eval harness and graders</h3><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"><strong>Step 4: Build a robust eval harness with a stable environment</strong></p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">It‚Äôs essential that the agent in the eval functions roughly the same as the agent used in production, and that the environment itself doesn‚Äôt introduce further noise. Each trial should be ‚Äúisolated‚Äù by starting from a clean environment. Unnecessary shared state between runs (leftover files, cached data, resource exhaustion) can cause correlated failures due to infrastructure flakiness rather than agent performance. Shared state can also artificially inflate performance. For example, in some internal evals we observed Claude gaining an unfair advantage on some tasks by examining the git history from previous trials. If multiple distinct trials fail because of the same limitation in the environment (like limited CPU memory), these trials are not independent because they‚Äôre affected by the same factor, and the eval results become unreliable for measuring agent performance.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"><strong>Step 5: Design graders thoughtfully</strong></p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"></p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">As discussed above, great eval design involves choosing the best graders for the agent and the tasks. We recommend choosing deterministic graders where possible, LLM graders where necessary or for additional flexibility, and using human graders judiciously for additional validation.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">There is a common instinct to check that agents followed very specific steps like a sequence of tool calls in the right order. We‚Äôve found this approach too rigid and results in overly brittle tests, as agents regularly find valid approaches that eval designers didn‚Äôt anticipate. So as not to unnecessarily punish creativity, it‚Äôs often better to grade what the agent produced, not the path it took.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">For tasks with multiple components, build in partial credit<strong>.</strong> A support agent that correctly identifies the problem and verifies the customer but fails to process a refund is meaningfully better than one that fails immediately. It‚Äôs important to represent this continuum of success in results.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Model grading often takes careful iteration to validate accuracy. LLM-as-judge graders should be closely calibrated with human experts to gain confidence that there is little divergence between the human grading and model grading. To avoid hallucinations, give the LLM a way out, like providing an instruction to return ‚ÄúUnknown‚Äù when it doesn‚Äôt have enough information. It can also help to create clear, structured rubrics to grade each dimension of a task, and then grade each dimension with an isolated LLM-as-judge rather than using one to grade all dimensions. Once the system is robust, it‚Äôs sufficient to use human review only occasionally.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Some evaluations have subtle failure modes that result in low scores even with good agent performance, as the agent fails to solve tasks due to grading bugs, agent harness constraints, or ambiguity. Even sophisticated teams can miss these issues. For example, <a href="https://x.com/sayashk/status/1996334941832089732?s=46&amp;t=c5pEvnVdVbMkcR_rcCHplg">Opus 4.5 initially scored 42% on CORE-Bench</a>, until an Anthropic researcher found multiple issues: rigid grading that penalized ‚Äú96.12‚Äù when expecting ‚Äú96.124991‚Ä¶‚Äù, ambiguous task specs, and stochastic tasks that were impossible to reproduce exactly. After fixing bugs and using a less constrained scaffold, Opus 4.5‚Äôs score jumped to 95%. Similarly, <a href="https://x.com/metr_evals/status/2001473506442375645?s=46">METR discovered</a> several misconfigured tasks in their time horizon benchmark that asked agents to optimize to a stated score threshold, but the grading required exceeding that threshold. This penalized models like Claude for following the instructions, while models that ignored the stated goal received better scores. Carefully double-checking tasks and graders can help avoid these problems.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Make your graders resistant to bypasses or hacks. The agent shouldn‚Äôt be able to easily ‚Äúcheat‚Äù the eval. Tasks and graders should be designed so that passing genuinely requires solving the problem rather than exploiting unintended loopholes.</p><h3 class="Body-module-scss-module__z40yvW__reading-column headline-6 post-subsection" id="maintain-and-use-the-eval-long-term">Maintain and use the eval long-term</h3><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"><strong>Step 6: Check the transcripts</strong></p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"></p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">You won&#x27;t know if your graders are working well unless you read the transcripts and grades from many trials. At Anthropic, we invested in tooling for viewing eval transcripts and we regularly take the time to read them. When a task fails, the transcript tells you whether the agent made a genuine mistake or whether your graders rejected a valid solution. It also often surfaces key details about agent and eval behavior.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"></p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Failures should seem fair: it‚Äôs clear what the agent got wrong and why. When scores don‚Äôt climb, we need confidence that it‚Äôs due to agent performance and not the eval. Reading transcripts is how you verify that your eval is measuring what actually matters, and is a critical skill for agent development.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"><strong>Step 7: Monitor for capability eval saturation</strong></p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">An eval at 100% tracks regressions but provides no signal for improvement. <strong>Eval saturation </strong>occurs when an agent passes all of the solvable tasks, leaving no room for improvement. For instance, SWE-Bench Verified scores started at 30% this year, and frontier models are now nearing saturation at &gt;80%. As evals approach saturation, progress will also slow, as only the most difficult tasks remain. This can make results deceptive, as large capability improvements appear as small increases in scores. For example, the code review startup <a href="https://www.qodo.ai/">Qodo</a> was initially unimpressed by Opus 4.5 because their one-shot coding evals didn‚Äôt capture the gains on longer, more complex tasks. In response, they developed a new agentic eval framework, providing a much clearer picture of progress.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">As a rule, we do not take eval scores at face value until someone digs into the details of the eval and reads some transcripts. If grading is unfair, tasks are ambiguous, valid solutions are penalized, or the harness constrains the model, the eval should be revised.¬†</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"></p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"><strong>Step 8: Keep evaluation suites healthy long-term through open contribution and maintenance</strong></p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">An eval suite is a living artifact that needs ongoing attention and clear ownership to remain useful.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">At Anthropic, we experimented with various approaches to eval maintenance. What proved most effective was establishing dedicated evals teams to own the core infrastructure, while domain experts and product teams contribute most eval tasks<em> </em>and run the evaluations themselves.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">For AI product teams, owning and iterating on evaluations should be as routine as maintaining unit tests. Teams can waste weeks on AI features that ‚Äúwork‚Äù in early testing but fail to meet unstated expectations that a well-designed eval would have surfaced early. Defining eval tasks is one of the best ways to stress-test whether the product requirements are concrete enough to start building.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">We recommend practicing eval-driven development: build evals to define planned capabilities before agents can fulfill them, then iterate until the agent performs well. Internally, we often build features that work ‚Äúwell enough‚Äù today but are bets on what models can do in a few months. Capability evals that start at a low pass rate make this visible. When a new model drops, running the suite quickly reveals which bets paid off.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">The people closest to product requirements and users are best positioned to define success. With current model capabilities, product managers, customer success managers, or salespeople can use Claude Code to contribute an eval task as a PR‚Äîlet them! Or, even better, actively enable them.</p><div class="Body-module-scss-module__z40yvW__media-column Body-module-scss-module__z40yvW__inline"><figure class="ImageWithCaption-module-scss-module__Duq99q__e-imageWithCaption"><img loading="lazy" width="4584" height="2580" decoding="async" data-nimg="1" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0db40cc0e14402222a179fc6297b9c8818e97c8a-4584x2580.png&amp;w=3840&amp;q=75 1x" src="/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0db40cc0e14402222a179fc6297b9c8818e97c8a-4584x2580.png&amp;w=3840&amp;q=75"/><figcaption class="caption"><em>The process of creating an effective evaluation.</em></figcaption></figure></div><h2 class="Body-module-scss-module__z40yvW__reading-column headline-5 post-section" id="how-evals-fit-with-other-methods-for-a-holistic-understanding-of-agents">How evals fit with other methods for a holistic understanding of agents</h2><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Automated evaluations can be run against an agent in thousands of tasks without deploying to production or affecting real users. But this is just one of many ways to understand agent performance. A complete picture includes production monitoring, user feedback, A/B testing, manual transcript review, and systematic human evaluation.</p><div class="Body-module-scss-module__z40yvW__media-column Body-module-scss-module__z40yvW__inline"><div class="Table-module-scss-module__Z3bHXa__root"><div class="Table-module-scss-module__Z3bHXa__tableWrapper" role="region" tabindex="0"><p class="headline-6">An overview of approaches for understanding AI agent performance</p><table class="Table-module-scss-module__Z3bHXa__table"><tbody><tr class="Table-module-scss-module__Z3bHXa__row"><th class="body-3">Method</th><th class="body-3">Pros</th><th class="body-3">Cons</th></tr><tr class="Table-module-scss-module__Z3bHXa__row"><td class="body-3"><strong>Automated evals<br/></strong><em>Running tests programmatically without real users</em><br/></td><td class="body-3"><ul class="TextBlock-module-scss-module__YX8MMq__list body-2 body-3"><li>Faster iteration</li><li>Fully reproducible</li><li>No user impact</li><li>Can run on every commit</li><li>Tests scenarios at scale without requiring a prod deployment</li></ul></td><td class="body-3"><ul class="TextBlock-module-scss-module__YX8MMq__list body-2 body-3"><li>Requires more up-front investment to build</li><li>Requires ongoing maintenance as product and model evolves to avoid drift</li><li>Can create false confidence if it doesn‚Äôt match real usage patterns</li></ul></td></tr><tr class="Table-module-scss-module__Z3bHXa__row"><td class="body-3"><strong>Production monitoring<br/></strong><em>Tracking metrics and errors in live systems</em><br/></td><td class="body-3"><ul class="TextBlock-module-scss-module__YX8MMq__list body-2 body-3"><li>Reveals real user behavior at scale</li><li>Catches issues that synthetic evals miss</li><li>Provides ground truth on how agents actually perform</li></ul></td><td class="body-3"><ul class="TextBlock-module-scss-module__YX8MMq__list body-2 body-3"><li>Reactive; problems reach users before you know about them</li><li>Signals can be noisy</li><li>Requires investment in instrumentation</li><li>Lacks ground truth for grading</li></ul></td></tr><tr class="Table-module-scss-module__Z3bHXa__row"><td class="body-3"><strong>A/B testing<br/></strong><em>Comparing variants with real user traffic</em><br/></td><td class="body-3"><ul class="TextBlock-module-scss-module__YX8MMq__list body-2 body-3"><li>Measures actual user outcomes (retention, task completion)</li><li>Controls for confounds</li><li>Scalable and systematic</li></ul></td><td class="body-3"><ul class="TextBlock-module-scss-module__YX8MMq__list body-2 body-3"><li>Slow; days or weeks to reach significance and requires sufficient traffic</li><li>Only tests changes you deploy</li><li>Less signal on the underlying ‚Äúwhy‚Äù for changes in metrics without being able to thoroughly review the transcripts</li></ul></td></tr><tr class="Table-module-scss-module__Z3bHXa__row"><td class="body-3"><strong>User feedback<br/></strong><em>Explicit signals like thumbs-down or bug reports</em><br/></td><td class="body-3"><ul class="TextBlock-module-scss-module__YX8MMq__list body-2 body-3"><li>Surfaces problems you didn&#x27;t anticipate</li><li>Comes with real examples from actual human users</li><li>The feedback often correlates with product goals</li></ul></td><td class="body-3"><ul class="TextBlock-module-scss-module__YX8MMq__list body-2 body-3"><li>Sparse and self-selected</li><li>Skews toward severe issues</li><li>Users rarely explain <em>why</em> something failed</li><li>Not automated</li><li>Relying primarily on users to catch issues can have negative user impact</li></ul></td></tr><tr class="Table-module-scss-module__Z3bHXa__row"><td class="body-3"><strong>Manual transcript review<br/></strong><em>Humans reading through agent conversations</em><br/></td><td class="body-3"><ul class="TextBlock-module-scss-module__YX8MMq__list body-2 body-3"><li>Builds intuition for failure modes</li><li>Catches subtle quality issues automated checks miss</li><li>Helps calibrate what &quot;good&quot; looks like and grasp details</li></ul></td><td class="body-3"><ul class="TextBlock-module-scss-module__YX8MMq__list body-2 body-3"><li>Time-intensive</li><li>Doesn&#x27;t scale</li><li>Coverage is inconsistent</li><li>Reviewer fatigue or different reviewers can affect the signal quality</li><li>Typically only gives qualitative signal rather than clear quantitative grading</li></ul></td></tr><tr class="Table-module-scss-module__Z3bHXa__row"><td class="body-3"><strong>Systematic human studies<br/></strong><em>Structured grading of agent outputs by trained raters</em><br/></td><td class="body-3"><ul class="TextBlock-module-scss-module__YX8MMq__list body-2 body-3"><li>Gold-standard quality judgements from multiple human raters</li><li>Handles subjective or ambiguous tasks</li><li>Provides signal for improving model-based graders</li></ul></td><td class="body-3"><ul class="TextBlock-module-scss-module__YX8MMq__list body-2 body-3"><li>Relatively expensive and slow turnaround</li><li>Hard to run frequently</li><li>Inter-rater disagreement requires reconciliation</li><li>Complex domains (legal, finance, healthcare) require human experts to conduct studies</li></ul></td></tr></tbody></table></div></div></div><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">These methods map to different stages of agent development. Automated evals are especially useful pre-launch and in CI/CD, running on each agent change and model upgrade as the first line of defense against quality problems. Production monitoring kicks in post-launch to detect distribution drift and unanticipated real-world failures. A/B testing validates significant changes once you have sufficient traffic. User feedback and transcript review are ongoing practices to fill the gaps: triage feedback constantly, sample transcripts to read weekly, and dig deeper as needed. Reserve systematic human studies for calibrating LLM graders or evaluating subjective outputs where human consensus serves as the reference standard.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"></p><div class="Body-module-scss-module__z40yvW__media-column Body-module-scss-module__z40yvW__inline"><figure class="ImageWithCaption-module-scss-module__Duq99q__e-imageWithCaption"><img loading="lazy" width="4584" height="2580" decoding="async" data-nimg="1" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fb77b8dbb7c2e57f063fbc8a087a853d5809b74b0-4584x2580.png&amp;w=3840&amp;q=75 1x" src="/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fb77b8dbb7c2e57f063fbc8a087a853d5809b74b0-4584x2580.png&amp;w=3840&amp;q=75"/><figcaption class="caption">Like the <a href="https://en.wikipedia.org/wiki/Swiss_cheese_model">Swiss Cheese Model</a> from safety engineering, no single evaluation layer catches every issue. With multiple methods combined, failures that slip through one layer are caught by another.</figcaption></figure></div><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">The most effective teams combine these methods: automated evals for fast iteration, production monitoring for ground truth, and periodic human review for calibration.</p><h2 class="Body-module-scss-module__z40yvW__reading-column headline-5 post-section" id="conclusion">Conclusion</h2><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Teams without evals get bogged down in reactive loops‚Äîfixing one failure, creating another, unable to distinguish real regressions from noise. Teams that invest early find the opposite: development accelerates as failures become test cases, test cases prevent regressions, and metrics replace guesswork. Evals give the whole team a clear hill to climb, turning ‚Äúthe agent feels worse‚Äù into something actionable. The value compounds, but only if you treat evals as a core component, not an afterthought.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">The patterns vary by agent type, but the fundamentals described here are constant. Start early and don‚Äôt wait for the perfect suite. Source realistic tasks from the failures you see. Define unambiguous, robust success criteria. Design graders thoughtfully and combine multiple types. Make sure the problems are hard enough for the model. Iterate on the evaluations to improve their signal-to-noise ratio. Read the transcripts!</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">AI agent evaluation is still a nascent, fast-evolving field. As agents take on longer tasks, collaborate in multi-agent systems, and handle increasingly subjective work, we will need to adapt our techniques. We‚Äôll keep sharing best practices as we learn more.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"></p><h3 class="Body-module-scss-module__z40yvW__reading-column headline-6 post-subsection" id="acknowledgements"><strong>Acknowledgements</strong></h3><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Written by Mikaela Grace, Jeremy Hadfield, Rodrigo Olivares, and Jiri De Jonghe. We&#x27;re also grateful to David Hershey, Gian Segato, Mike Merrill, Alex Shaw, Nicholas Carlini, Ethan Dixon, Pedram Navid, Jake Eaton, Alyssa Baum, Lina Tawfik, Karen Zhou, Alexander Bricken, Sam Kennedy, Robert Ying, and others for their contributions. Special thanks to the customers and partners we have learned from through collaborating on evals, including iGent, Cognition, Bolt, Sierra, Vals.ai, Macroscope, PromptLayer, Stripe, Shopify, the Terminal Bench team, and more. This work reflects the collective efforts of several teams who helped develop the practice of evaluations at Anthropic.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"></p><h2 class="Body-module-scss-module__z40yvW__reading-column headline-5 post-section" id="appendix-eval-frameworks">Appendix: Eval frameworks</h2><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Several open-source and commercial frameworks can help teams implement agent evaluations without building infrastructure from scratch. The right choice depends on your agent type, existing stack, and whether you need offline evaluation, production observability, or both.<br/><br/><a href="https://harborframework.com/">Harbor</a> is designed for running agents in containerized environments, with infrastructure for running trials at scale across cloud providers and a standardized format for defining tasks and graders. Popular benchmarks like Terminal-Bench 2.0 ship through the Harbor registry, making it easy to run established benchmarks along with custom eval suites.<br/><br/><a href="https://www.promptfoo.dev/">Promptfoo</a> is a lightweight, flexible, and open-source framework that focuses on declarative YAML configuration for prompt testing, with assertion types ranging from string matching to LLM-as-judge rubrics. We use a version of Promptfoo for many of our product evals. <br/><br/><a href="https://www.braintrust.dev/">Braintrust</a> is a platform that combines offline evaluation with production observability and experiment tracking‚Äîuseful for teams that need to both iterate during development and monitor quality in production. Its `autoevals` library includes pre-built scorers for factuality, relevance, and other common dimensions. <br/><br/><a href="https://docs.langchain.com/langsmith/evaluation">LangSmith</a> offers tracing, offline and online evaluations, and dataset management with tight integration into the LangChain ecosystem. <a href="https://langfuse.com/">Langfuse</a> provides similar capabilities as a self-hosted open-source alternative for teams with data residency requirements.<br/><br/>Many teams combine multiple tools, roll their own eval framework, or just use simple evaluation scripts as a starting point. We find that while frameworks can be a valuable way to accelerate progress and standardize, they‚Äôre only as good as the eval tasks you run through them. It‚Äôs often best to quickly pick a framework that fits your workflow, then invest your energy in the evals themselves by iterating on high-quality test cases and graders.</p></div></div></article><div class="NewsletterEngineering-module-scss-module__AiizZa__wrapper"><div class="NewsletterEngineering-module-scss-module__AiizZa__content"><div class="NewsletterEngineering-module-scss-module__AiizZa__textContent"><h2 class="headline-5 NewsletterEngineering-module-scss-module__AiizZa__title">Get the developer newsletter</h2><div class="NewsletterEngineering-module-scss-module__AiizZa__body"><p class="body-1 serif tight">Product updates, how-tos, community spotlights, and more. Delivered monthly to your inbox.</p></div></div><div class="NewsletterEngineering-module-scss-module__AiizZa__formContainer"><form class="NewsletterEngineering-module-scss-module__AiizZa__emailForm"><div class="NewsletterEngineering-module-scss-module__AiizZa__inputWrapper"><input type="email" placeholder="Enter your email" class="NewsletterEngineering-module-scss-module__AiizZa__emailInput" required="" name="email" value=""/><button type="submit" class="NewsletterEngineering-module-scss-module__AiizZa__submitButton"><svg class="Icon-module-scss-module__lqbdHG__icon" width="20" height="20" viewBox="0 0 21 21"><path d="M4.14585 9.87492L14.4584 9.87492L9.60419 5.04158L10.5 4.14575L16.8542 10.4999L10.5 16.8541L9.60419 15.9583L14.4584 11.1249L4.14585 11.1249L4.14585 9.87492Z" fill="#ffffff"></path></svg></button></div><p class="body-3">Please provide your email address if you‚Äôd like to receive our monthly developer newsletter. You can unsubscribe at any time.</p></form></div></div></div></div></main><footer id="footer" class="SiteFooter-module-scss-module__JdOqwq__root" role="contentinfo" aria-label="Site footer"><div class="page-wrapper SiteFooter-module-scss-module__JdOqwq__footer"><div class="SiteFooter-module-scss-module__JdOqwq__logoWrapper"><a href="/" aria-label="Return to homepage"><svg class="Icon-module-scss-module__lqbdHG__icon" width="46" height="32" viewBox="0 0 46 32"><path d="M32.73 0h-6.945L38.45 32h6.945L32.73 0ZM12.665 0 0 32h7.082l2.59-6.72h13.25l2.59 6.72h7.082L19.929 0h-7.264Zm-.702 19.337 4.334-11.246 4.334 11.246h-8.668Z" fill="#faf9f5"></path></svg></a></div><nav class="SiteFooter-module-scss-module__JdOqwq__linksWrapper" aria-label="Footer navigation"><div class="SiteFooter-module-scss-module__JdOqwq__columnSection"><div class="SiteFooter-module-scss-module__JdOqwq__listSection"><h3 class="body-2 bold">Products</h3><ul class="SiteFooter-module-scss-module__JdOqwq__list"><li><a href="https://claude.com/product/overview" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Claude</a></li><li><a href="https://claude.com/product/claude-code" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Claude Code</a></li><li><a href="https://claude.com/product/cowork" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Cowork</a></li><li><a href="https://claude.com/chrome" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Claude in Chrome</a></li><li><a href="https://claude.com/claude-in-excel" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Claude in Excel</a></li><li><a href="https://claude.com/claude-in-powerpoint" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Claude in PowerPoint</a></li><li><a href="https://claude.com/claude-in-slack" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Claude in Slack</a></li><li><a href="https://www.claude.com/skills" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Skills</a></li><li><a href="https://claude.com/pricing/max" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Max plan</a></li><li><a href="https://claude.com/pricing/team" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Team plan</a></li><li><a href="https://claude.com/pricing/enterprise" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Enterprise plan</a></li><li><a href="https://claude.ai/download" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Download app</a></li><li><a href="https://claude.com/pricing" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Pricing</a></li><li><a href="https://claude.ai/" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Log in to Claude</a></li></ul></div><div class="SiteFooter-module-scss-module__JdOqwq__listSection"><h3 class="body-2 bold">Models</h3><ul class="SiteFooter-module-scss-module__JdOqwq__list"><li><a href="/www.anthropic.com/claude/opus" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">Opus</a></li><li><a href="/www.anthropic.com/claude/sonnet" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">Sonnet</a></li><li><a href="/www.anthropic.com/claude/haiku" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">Haiku</a></li></ul></div></div><div class="SiteFooter-module-scss-module__JdOqwq__columnSection"><div class="SiteFooter-module-scss-module__JdOqwq__listSection"><h3 class="body-2 bold">Solutions</h3><ul class="SiteFooter-module-scss-module__JdOqwq__list"><li><a href="https://claude.com/solutions/agents" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">AI agents</a></li><li><a href="https://claude.com/solutions/code-modernization" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Code modernization</a></li><li><a href="https://claude.com/solutions/coding" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Coding</a></li><li><a href="https://claude.com/solutions/customer-support" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Customer support</a></li><li><a href="https://claude.com/solutions/education" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Education</a></li><li><a href="https://claude.com/solutions/financial-services" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Financial services</a></li><li><a href="https://claude.com/solutions/government" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Government</a></li><li><a href="https://claude.com/solutions/healthcare" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Healthcare</a></li><li><a href="https://claude.com/solutions/life-sciences" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Life sciences</a></li><li><a href="https://claude.com/solutions/nonprofits" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Nonprofits</a></li></ul></div><div class="SiteFooter-module-scss-module__JdOqwq__listSection"><h3 class="body-2 bold">Claude Developer Platform</h3><ul class="SiteFooter-module-scss-module__JdOqwq__list"><li><a href="https://claude.com/platform/api" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Overview</a></li><li><a href="https://platform.claude.com/docs" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Developer docs</a></li><li><a href="https://claude.com/pricing#api" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Pricing</a></li><li><a href="https://claude.com/regional-compliance" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Regional compliance</a></li><li><a href="https://claude.com/partners/amazon-bedrock" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Amazon Bedrock</a></li><li><a href="https://claude.com/partners/google-cloud-vertex-ai" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Google Cloud‚Äôs Vertex AI</a></li><li><a href="https://platform.claude.com/" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Console login</a></li></ul></div></div><div class="SiteFooter-module-scss-module__JdOqwq__columnSection"><div class="SiteFooter-module-scss-module__JdOqwq__listSection"><h3 class="body-2 bold">Learn</h3><ul class="SiteFooter-module-scss-module__JdOqwq__list"><li><a href="https://claude.com/blog" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Blog</a></li><li><a href="https://claude.com/partners" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Claude partner network</a></li><li><a href="https://claude.com/connectors" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Connectors</a></li><li><a href="/learn" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">Courses</a></li><li><a href="https://claude.com/customers" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Customer stories</a></li><li><a href="/engineering" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">Engineering at Anthropic</a></li><li><a href="/events" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">Events</a></li><li><a href="https://claude.com/plugins" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Plugins</a></li><li><a href="https://claude.com/partners/powered-by-claude" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Powered by Claude</a></li><li><a href="https://claude.com/partners/services" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Service partners</a></li><li><a href="https://claude.com/programs/startups" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Startups program</a></li><li><a href="https://claude.com/resources/tutorials" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Tutorials</a></li><li><a href="https://claude.com/resources/use-cases" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Use cases</a></li></ul></div><div class="SiteFooter-module-scss-module__JdOqwq__listSection"><h3 class="body-2 bold">Company</h3><ul class="SiteFooter-module-scss-module__JdOqwq__list"><li><a href="/company" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">Anthropic</a></li><li><a href="/careers" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">Careers</a></li><li><a href="/economic-index" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">Economic Futures</a></li><li><a href="/research" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">Research</a></li><li><a href="/news" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">News</a></li><li><a href="/constitution" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">Claude‚Äôs Constitution</a></li><li><a href="/www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">Responsible Scaling Policy</a></li><li><a href="https://trust.anthropic.com/" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">Security and compliance</a></li><li><a href="/transparency" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">Transparency</a></li></ul></div></div><div class="SiteFooter-module-scss-module__JdOqwq__columnSection"><div class="SiteFooter-module-scss-module__JdOqwq__listSection"><h3 class="body-2 bold">Help and security</h3><ul class="SiteFooter-module-scss-module__JdOqwq__list"><li><a href="/www.anthropic.com/supported-countries" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">Availability</a></li><li><a href="https://status.anthropic.com/" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">Status</a></li><li><a href="https://support.claude.com/en/" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Support center</a></li></ul></div><div class="SiteFooter-module-scss-module__JdOqwq__listSection"><h3 class="body-2 bold">Terms and policies</h3><ul class="SiteFooter-module-scss-module__JdOqwq__list"><li><a href="/www.anthropic.com/legal/privacy" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">Privacy policy</a></li><li><a href="/www.anthropic.com/legal/consumer-health-data-privacy-policy" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">Consumer health data privacy policy</a></li><li><a href="/www.anthropic.com/responsible-disclosure-policy" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">Responsible disclosure policy</a></li><li><a href="/www.anthropic.com/legal/commercial-terms" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">Terms of service: Commercial</a></li><li><a href="/www.anthropic.com/legal/consumer-terms" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">Terms of service: Consumer</a></li><li><a href="/www.anthropic.com/legal/aup" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">Usage policy</a></li></ul></div></div></nav><div class="SiteFooter-module-scss-module__JdOqwq__socialWrapper"><small class="body-2 SiteFooter-module-scss-module__JdOqwq__copyright" role="contentinfo">¬© 2026 Anthropic PBC</small><ul class="SiteFooter-module-scss-module__JdOqwq__socialIcons" role="navigation" aria-label="Social media links"><li><a href="https://www.linkedin.com/company/anthropicresearch" aria-label="Visit our LinkedIn page" target="_blank" rel="noopener noreferrer"><svg class="Icon-module-scss-module__lqbdHG__icon" width="24" height="24" viewBox="0 0 32 32"><path d="M25.8182 4H6.18182C4.97636 4 4 4.97636 4 6.18182V25.8182C4 27.0236 4.97636 28 6.18182 28H25.8182C27.0236 28 28 27.0236 28 25.8182V6.18182C28 4.97636 27.0236 4 25.8182 4ZM11.5862 23.6364H8.368V13.2815H11.5862V23.6364ZM9.94436 11.8011C8.90691 11.8011 8.068 10.96 8.068 9.92473C8.068 8.88945 8.908 8.04945 9.94436 8.04945C10.9785 8.04945 11.8196 8.89055 11.8196 9.92473C11.8196 10.96 10.9785 11.8011 9.94436 11.8011ZM23.6407 23.6364H20.4247V18.6007C20.4247 17.3996 20.4029 15.8549 18.7524 15.8549C17.0778 15.8549 16.8204 17.1629 16.8204 18.5135V23.6364H13.6044V13.2815H16.6916V14.6964H16.7353C17.1651 13.8825 18.2145 13.024 19.78 13.024C23.0385 13.024 23.6407 15.1687 23.6407 17.9571V23.6364Z" fill="#b0aea5"></path></svg></a></li><li><a href="https://x.com/AnthropicAI" aria-label="Visit our X (formerly Twitter) profile" target="_blank" rel="noopener noreferrer"><svg class="Icon-module-scss-module__lqbdHG__icon" width="24" height="24" viewBox="0 0 32 32"><path d="M28 28L18.6145 14.0124L18.6305 14.0255L27.0929 4H24.265L17.3713 12.16L11.8968 4H4.48021L13.2425 17.0593L13.2414 17.0582L4 28H6.82792L14.4921 18.9215L20.5834 28H28ZM10.7763 6.18182L23.9449 25.8182H21.7039L8.52468 6.18182H10.7763Z" fill="#b0aea5"></path></svg></a></li><li><a href="https://www.youtube.com/@anthropic-ai" aria-label="Visit our YouTube channel" target="_blank" rel="noopener noreferrer"><svg class="Icon-module-scss-module__lqbdHG__icon" width="24" height="24" viewBox="0 0 32 32"><path d="M29.2184 9.4375C28.9596 8.06299 27.7263 7.06201 26.2951 6.74951C24.1533 6.3125 20.1896 6 15.901 6C11.615 6 7.58782 6.3125 5.44354 6.74951C4.01486 7.06201 2.77905 7.99951 2.52021 9.4375C2.25884 11 2 13.1875 2 16C2 18.8125 2.25884 21 2.58365 22.5625C2.84502 23.937 4.0783 24.938 5.50698 25.2505C7.78068 25.6875 11.6784 26 15.967 26C20.2556 26 24.1533 25.6875 26.427 25.2505C27.8557 24.938 29.089 24.0005 29.3504 22.5625C29.6092 21 29.934 18.749 30 16C29.868 13.1875 29.5432 11 29.2184 9.4375ZM12.3941 20.375V11.625L20.319 16L12.3941 20.375Z" fill="#b0aea5"></path></svg></a></li></ul></div></div></footer><!--$?--><template id="B:1"></template><!--/$--><script nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh">requestAnimationFrame(function(){$RT=performance.now()});</script><script src="/_next/static/chunks/f4386f5ba7642880.js" nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh" id="_R_" async=""></script><title>Demystifying evals for AI agents \ Anthropic</title><meta name="description" content="Demystifying evals for AI agents "/><meta name="msapplication-TileColor" content="141413"/><meta name="msapplication-config" content="/browserconfig.xml"/><meta property="og:title" content="Demystifying evals for AI agents"/><meta property="og:description" content="Demystifying evals for AI agents "/><meta property="og:image" content="https://cdn.sanity.io/images/4zrzovbb/website/412be842c5c6bae6b4bcd515c191b0aa5015e05f-2400x1260.png"/><meta property="og:image:alt" content="Demystifying evals for AI agents "/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:site" content="@AnthropicAI"/><meta name="twitter:creator" content="@AnthropicAI"/><meta name="twitter:title" content="Demystifying evals for AI agents"/><meta name="twitter:description" content="Demystifying evals for AI agents "/><meta name="twitter:image" content="https://cdn.sanity.io/images/4zrzovbb/website/412be842c5c6bae6b4bcd515c191b0aa5015e05f-2400x1260.png"/><meta name="twitter:image:alt" content="Demystifying evals for AI agents "/><link rel="shortcut icon" href="/favicon.ico"/><link rel="icon" href="/images/icons/favicon-32x32.png"/><link rel="apple-touch-icon" href="/images/icons/apple-touch-icon.png"/><link rel="apple-touch-icon" href="/images/icons/apple-touch-icon.png" sizes="180x180"/><link rel="mask-icon" href="/images/icons/safari-pinned-tab.svg" color="141413"/><script nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh">document.querySelectorAll('body link[rel="icon"], body link[rel="apple-touch-icon"]').forEach(el => document.head.appendChild(el))</script><div hidden id="S:0"></div><script nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh">$RB=[];$RV=function(a){$RT=performance.now();for(var b=0;b<a.length;b+=2){var c=a[b],e=a[b+1];null!==e.parentNode&&e.parentNode.removeChild(e);var f=c.parentNode;if(f){var g=c.previousSibling,h=0;do{if(c&&8===c.nodeType){var d=c.data;if("/$"===d||"/&"===d)if(0===h)break;else h--;else"$"!==d&&"$?"!==d&&"$~"!==d&&"$!"!==d&&"&"!==d||h++}d=c.nextSibling;f.removeChild(c);c=d}while(c);for(;e.firstChild;)f.insertBefore(e.firstChild,c);g.data="$";g._reactRetry&&requestAnimationFrame(g._reactRetry)}}a.length=0};
$RC=function(a,b){if(b=document.getElementById(b))(a=document.getElementById(a))?(a.previousSibling.data="$~",$RB.push(a,b),2===$RB.length&&("number"!==typeof $RT?requestAnimationFrame($RV.bind(null,$RB)):(a=performance.now(),setTimeout($RV.bind(null,$RB),2300>a&&2E3<a?2300-a:$RT+300-a)))):b.parentNode.removeChild(b)};$RC("B:0","S:0")</script><div hidden id="S:1"></div><script nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh">$RC("B:1","S:1")</script><script nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh">(self.__next_f=self.__next_f||[]).push([0])</script><script nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh">self.__next_f.push([1,"1:\"$Sreact.fragment\"\n4:I[339756,[\"/_next/static/chunks/d96012bcfc98706a.js\",\"/_next/static/chunks/d80b3790a119a285.js\"],\"default\"]\n5:I[837457,[\"/_next/static/chunks/d96012bcfc98706a.js\",\"/_next/static/chunks/d80b3790a119a285.js\"],\"default\"]\na:I[168027,[\"/_next/static/chunks/d96012bcfc98706a.js\",\"/_next/static/chunks/d80b3790a119a285.js\"],\"default\"]\nb:I[897367,[\"/_next/static/chunks/d96012bcfc98706a.js\",\"/_next/static/chunks/d80b3790a119a285.js\"],\"OutletBoundary\"]\nc:\"$Sreact.suspense\"\ne:I[897367,[\"/_next/static/chunks/d96012bcfc98706a.js\",\"/_next/static/chunks/d80b3790a119a285.js\"],\"ViewportBoundary\"]\n10:I[897367,[\"/_next/static/chunks/d96012bcfc98706a.js\",\"/_next/static/chunks/d80b3790a119a285.js\"],\"MetadataBoundary\"]\n12:I[264900,[\"/_next/static/chunks/1fb574e7be3f9a05.js\",\"/_next/static/chunks/9a604444e87766dd.js\",\"/_next/static/chunks/c1896c986be1a2e2.js\"],\"default\"]\n13:I[649551,[\"/_next/static/chunks/1fb574e7be3f9a05.js\",\"/_next/static/chunks/9a604444e87766dd.js\",\"/_next/static/chunks/c1896c986be1a2e2.js\"],\"default\"]\n14:I[96155,[\"/_next/static/chunks/1fb574e7be3f9a05.js\",\"/_next/static/chunks/9a604444e87766dd.js\",\"/_next/static/chunks/c1896c986be1a2e2.js\"],\"default\"]\n16:I[775710,[\"/_next/static/chunks/1fb574e7be3f9a05.js\",\"/_next/static/chunks/9a604444e87766dd.js\",\"/_next/static/chunks/c1896c986be1a2e2.js\",\"/_next/static/chunks/f5a33d7993e253c8.js\",\"/_next/static/chunks/496bc8a289f448d1.js\",\"/_next/static/chunks/b1040bb2d2fbd1e5.js\",\"/_next/static/chunks/2e3229a62c65aaec.js\",\"/_next/static/chunks/5c1988096a7b174a.js\",\"/_next/static/chunks/f563a58c137d4bc2.js\",\"/_next/static/chunks/2fd2aa01a4bc9178.js\",\"/_next/static/chunks/630870b77208f43d.js\",\"/_next/static/chunks/010986693eb1c9c2.js\",\"/_next/static/chunks/dabacb64939959b3.js\",\"/_next/static/chunks/c0d75d4ca01ae43d.js\",\"/_next/static/chunks/6c680011ff6c5ba2.js\",\"/_next/static/chunks/81716bb24f5a6f8f.js\"],\"default\"]\n17:I[606617,[\"/_next/static/chunks/1fb574e7be3f9a05.js\",\"/_next/static/chunks/9a604444e87766dd.js\",\"/_next/static/chunks/c1896c986be1a2e2.js\",\"/_next/static/chunks/7c80d08c36d49463.js\",\"/_next/static/chunks/496bc8a289f448d1.js\",\"/_next/static/chunks/b1040bb2d2fbd1e5.js\",\"/_next/static/chunks/5c1988096a7b174a.js\",\"/_next/static/chunks/33647e5ba6496195.js\",\"/_next/static/chunks/667473da0b5c11bc.js\",\"/_next/static/chunks/2e3229a62c65aaec.js\",\"/_next/static/chunks/010986693eb1c9c2.js\",\"/_next/static/chunks/2c9eb3077aa18f16.js\",\"/_next/static/chunks/6c680011ff6c5ba2.js\",\"/_next/static/chunks/2fd2aa01a4bc9178.js\",\"/_next/static/chunks/630870b77208f43d.js\"],\"default\"]\n18:I[837061,[\"/_next/static/chunks/1fb574e7be3f9a05.js\",\"/_next/static/chunks/9a604444e87766dd.js\",\"/_next/static/chunks/c1896c986be1a2e2.js\",\"/_next/static/chunks/7c80d08c36d49463.js\",\"/_next/static/chunks/496bc8a289f448d1.js\",\"/_next/static/chunks/b1040bb2d2fbd1e5.js\",\"/_next/static/chunks/5c1988096a7b174a.js\",\"/_next/static/chunks/33647e5ba6496195.js\",\"/_next/static/chunks/667473da0b5c11bc.js\",\"/_next/static/chunks/2e3229a62c65aaec.js\",\"/_next/static/chunks/010986693eb1c9c2.js\",\"/_next/static/chunks/2c9eb3077aa18f16.js\",\"/_next/static/chunks/6c680011ff6c5ba2.js\",\"/_next/static/chunks/2fd2aa01a4bc9178.js\",\"/_next/static/chunks/630870b77208f43d.js\"],\"default\"]\n1a:I[307003,[\"/_next/static/chunks/1fb574e7be3f9a05.js\",\"/_next/static/chunks/9a604444e87766dd.js\",\"/_next/static/chunks/c1896c986be1a2e2.js\",\"/_next/static/chunks/7c80d08c36d49463.js\",\"/_next/static/chunks/496bc8a289f448d1.js\",\"/_next/static/chunks/b1040bb2d2fbd1e5.js\",\"/_next/static/chunks/5c1988096a7b174a.js\",\"/_next/static/chunks/33647e5ba6496195.js\",\"/_next/static/chunks/667473da0b5c11bc.js\",\"/_next/static/chunks/2e3229a62c65aaec.js\",\"/_next/static/chunks/010986693eb1c9c2.js\",\"/_next/static/chunks/2c9eb3077aa18f16.js\",\"/_next/static/chunks/6c680011ff6c5ba2.js\",\"/_next/static/chunks/2fd2aa01a4bc9178.js\",\"/_next/static/chunks/630870b77208f43d.js\"],\"default\"]\n1b:I[27201,[\"/_next/static/chunks/d96012bcfc98706a.js\",\"/_next/static/chunks/d80b3790a119a285.js\"],\"IconMark\"]\n:HL[\"/_next/static/chunks/ec368b341879b233.css\",\"style\",{\"nonce\":\"ZDEyN2N"])</script><script nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh">self.__next_f.push([1,"mZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\"}]\n:HL[\"/_next/static/chunks/38fee8473f816a4a.css\",\"style\",{\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\"}]\n:HL[\"/_next/static/media/AnthropicMono_Italic_Web-s.p.154bb54e.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/AnthropicMono_Roman_Web-s.p.e2998bbe.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/AnthropicSans_Italic_Variable-s.p.dfc8e235.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/AnthropicSans_Roman_Variable-s.p.52cc3a10.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/AnthropicSerif_Italic_Variable-s.p.9d7ca5ec.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/AnthropicSerif_Roman_Variable-s.p.55835b1f.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/Copernicus_Book-s.p.f166c0ba.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/Copernicus_Medium-s.p.59728346.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/JetBrainsMono_VF-s.p.8dac7c36.ttf\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\",\"type\":\"font/ttf\"}]\n:HL[\"/_next/static/media/StyreneA_MediumItalic_Web-s.p.e9bc3c6e.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/StyreneA_Medium_Web-s.p.e5135f7e.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/StyreneA_RegularItalic_Web-s.p.7c6a646d.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/StyreneA_Regular_Web-s.p.429c699d.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/StyreneB_Medium_Web-s.p.88fa5a67.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/StyreneB_Regular_Web-s.p.cb3cc1a3.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/TiemposText_Medium-s.p.520d99f8.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/TiemposText_MediumItalic-s.p.10f44518.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/TiemposText_Regular-s.p.7f1d46d6.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/TiemposText_RegularItalic-s.p.1a798fcf.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/chunks/caf680e685668b99.css\",\"style\",{\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\"}]\n:HL[\"/_next/static/chunks/ad266d0a6bc656af.css\",\"style\",{\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\"}]\n:HL[\"/_next/static/chunks/e2c670ea67fc2bbb.css\",\"style\",{\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\"}]\n:HL[\"/_next/static/chunks/758311c654d998de.css\",\"style\",{\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\"}]\n:HL[\"/_next/static/chunks/7e4146583225b449.css\",\"style\",{\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\"}]\n"])</script><script nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh">self.__next_f.push([1,"0:{\"P\":null,\"b\":\"Ev1insoJIZ1ve_aUh7493\",\"c\":[\"\",\"engineering\",\"demystifying-evals-for-ai-agents\"],\"q\":\"\",\"i\":false,\"f\":[[[\"\",{\"children\":[\"(site)\",{\"children\":[\"engineering\",{\"children\":[[\"slug\",\"demystifying-evals-for-ai-agents\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]}]},\"$undefined\",\"$undefined\",true],[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/ec368b341879b233.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\"}]],\"$L2\"]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/38fee8473f816a4a.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\"}],[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/1fb574e7be3f9a05.js\",\"async\":true,\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\"}],[\"$\",\"script\",\"script-1\",{\"src\":\"/_next/static/chunks/9a604444e87766dd.js\",\"async\":true,\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\"}],[\"$\",\"script\",\"script-2\",{\"src\":\"/_next/static/chunks/c1896c986be1a2e2.js\",\"async\":true,\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\"}]],\"$L3\"]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[\"$L6\",[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/caf680e685668b99.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/ad266d0a6bc656af.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\"}],[\"$\",\"link\",\"2\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/e2c670ea67fc2bbb.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\"}],[\"$\",\"link\",\"3\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/758311c654d998de.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\"}],[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/f5a33d7993e253c8.js\",\"async\":true,\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\"}],[\"$\",\"script\",\"script-1\",{\"src\":\"/_next/static/chunks/496bc8a289f448d1.js\",\"async\":true,\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\"}],[\"$\",\"script\",\"script-2\",{\"src\":\"/_next/static/chunks/b1040bb2d2fbd1e5.js\",\"async\":true,\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\"}],[\"$\",\"script\",\"script-3\",{\"src\":\"/_next/static/chunks/2e3229a62c65aaec.js\",\"async\":true,\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\"}],[\"$\",\"script\",\"script-4\",{\"src\":\"/_next/static/chunks/5c1988096a7b174a.js\",\"async\":true,\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\"}],[\"$\",\"script\",\"script-5\",{\"src\":\"/_next/static/chunks/f563a58c137d4bc2.js\",\"async\":true,\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\"}],[\"$\",\"script\",\"script-6\",{\"src\":\"/_next/static/chunks/2fd2aa01a4bc9178.js\",\"async\":true,\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\"}],[\"$\",\"script\",\"script-7\",{\"src\":\"/_next/static/chunks/630870b77208f43d.js\",\"async\":true,\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\"}],[\"$\",\"script\",\"script-8\",{\"src\":\"/_next/static/chunks/010986693eb1c9c2.js\",\"async\":true,\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\"}],[\"$\",\"script\",\"script-9\",{\"src\":\"/_next/static/chunks/dabacb64939959b3.js\",\"async\":true,\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\"}],[\"$\",\"script\",\"script-10\",{\"src\":\"/_next/static/chunks/c0d75d4ca01ae43d.js\",\"async\":true,\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\"}],[\"$\",\"script\",\"script-11\",{\"src\":\"/_next/static/chunks/6c680011ff6c5ba2.js\",\"async\":true,\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\"}],\"$L7\"],\"$L8\"]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false]},null,false,false],\"$L9\",false]],\"m\":\"$undefined\",\"G\":[\"$a\",[]],\"S\":false}\n"])</script><script nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh">self.__next_f.push([1,"7:[\"$\",\"script\",\"script-12\",{\"src\":\"/_next/static/chunks/81716bb24f5a6f8f.js\",\"async\":true,\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\"}]\n8:[\"$\",\"$Lb\",null,{\"children\":[\"$\",\"$c\",null,{\"name\":\"Next.MetadataOutlet\",\"children\":\"$@d\"}]}]\n9:[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$Le\",null,{\"children\":\"$@f\"}],[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$L10\",null,{\"children\":[\"$\",\"$c\",null,{\"name\":\"Next.Metadata\",\"children\":\"$@11\"}]}]}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}]\n"])</script><script nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh">self.__next_f.push([1,"2:[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"anthropicsans_eac0b31f-module__tjnuGq__variable anthropicserif_87b6fa7d-module__quIBbW__variable anthropicmono_fae19af3-module__c5XAsG__variable copernicus_4da799c5-module__dijTSq__variable styrenea_f8492ab1-module__HimLXW__variable styreneb_278af5c6-module__wkOAdG__variable tiempostext_4eff4b4c-module__mpviCW__variable jetbrainsmono_7d7bdbc6-module__j_XgJq__variable\",\"children\":[\"$\",\"body\",null,{\"children\":[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]\n"])</script><script nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh">self.__next_f.push([1,"3:[\"$\",\"$L12\",null,{\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\",\"children\":[\"$\",\"$L13\",null,{\"gpcDetected\":false,\"children\":[[\"$\",\"$L14\",null,{}],[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[\"$L15\",[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/caf680e685668b99.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/ad266d0a6bc656af.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\"}],[\"$\",\"link\",\"2\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/e2c670ea67fc2bbb.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\"}],[\"$\",\"link\",\"3\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/7e4146583225b449.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh\"}]]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}]}]\n"])</script><script nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh">self.__next_f.push([1,"6:[\"$\",\"$L16\",null,{\"article\":{\"_createdAt\":\"2026-01-08T23:04:51Z\",\"_id\":\"31c2076f-7327-4762-b55c-8d0644b7b94f\",\"_rev\":\"aLJWpLS1Zv36ohV3WTKLqn\",\"_system\":{\"base\":{\"id\":\"31c2076f-7327-4762-b55c-8d0644b7b94f\",\"rev\":\"vKEHx6u8BWSinbSlyRRtEo\"}},\"_type\":\"engineeringArticle\",\"_updatedAt\":\"2026-02-04T18:45:28Z\",\"body\":[{\"_key\":\"da0a6b359b36\",\"_type\":\"block\",\"children\":[{\"_key\":\"a9aa45ef0a1b\",\"_type\":\"span\",\"marks\":[],\"text\":\"Introduction\"}],\"markDefs\":[],\"style\":\"h3\"},{\"_key\":\"654a86876d91\",\"_type\":\"block\",\"children\":[{\"_key\":\"a75f5b9cf9a8\",\"_type\":\"span\",\"marks\":[],\"text\":\"Good evaluations help teams ship AI agents more confidently. Without them, it‚Äôs easy to get stuck in reactive loops‚Äîcatching issues only in production, where fixing one failure creates others. Evals make problems and behavioral changes visible before they affect users, and their value compounds over the lifecycle of an agent.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"46f77739f95a\",\"_type\":\"block\",\"children\":[{\"_key\":\"cb57fd5e0e01\",\"_type\":\"span\",\"marks\":[],\"text\":\"\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"2790553b81b7\",\"_type\":\"block\",\"children\":[{\"_key\":\"f8f353e44395\",\"_type\":\"span\",\"marks\":[],\"text\":\"As we described in \"},{\"_key\":\"23482f485432\",\"_type\":\"span\",\"marks\":[\"89a806441ea5\"],\"text\":\"Building effective agents\"},{\"_key\":\"0a55a7ce30e0\",\"_type\":\"span\",\"marks\":[],\"text\":\", agents operate over many turns: calling tools, modifying state, and adapting based on intermediate results. These same capabilities that make AI agents useful‚Äîautonomy, intelligence, and flexibility‚Äîalso make them harder to evaluate.\"}],\"markDefs\":[{\"_key\":\"89a806441ea5\",\"_type\":\"link\",\"href\":\"/www.anthropic.com/engineering/building-effective-agents\"}],\"style\":\"normal\"},{\"_key\":\"753345806684\",\"_type\":\"block\",\"children\":[{\"_key\":\"4cff2369954a\",\"_type\":\"span\",\"marks\":[],\"text\":\"\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"0eb2b6785ffe\",\"_type\":\"block\",\"children\":[{\"_key\":\"35490ae13d95\",\"_type\":\"span\",\"marks\":[],\"text\":\"Through our internal work and with customers at the frontier of agent development, we‚Äôve learned how to design more rigorous and useful evals for agents. Here's what's worked across a range of agent architectures and use cases in real-world deployment.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"583764aa3639\",\"_type\":\"block\",\"children\":[{\"_key\":\"30de2daaf49e\",\"_type\":\"span\",\"marks\":[],\"text\":\"The structure of an evaluation\"}],\"markDefs\":[],\"style\":\"h3\"},{\"_key\":\"355a50bc63cb\",\"_type\":\"block\",\"children\":[{\"_key\":\"73901bcaf2ed\",\"_type\":\"span\",\"marks\":[],\"text\":\"An \"},{\"_key\":\"faf3b8212492\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"evaluation \"},{\"_key\":\"7600b753d98b\",\"_type\":\"span\",\"marks\":[],\"text\":\"(‚Äúeval‚Äù) is a test for an AI system: give an AI an input, then apply grading logic to its output to measure success. In this post, we focus on \"},{\"_key\":\"b91568478eb2\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"automated evals \"},{\"_key\":\"6f6dcc56c741\",\"_type\":\"span\",\"marks\":[],\"text\":\"that can be run during development without real users.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"46e7d1ccca78\",\"_type\":\"block\",\"children\":[{\"_key\":\"d440ed3a2173\",\"_type\":\"span\",\"marks\":[],\"text\":\"\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"0ff102e7aa28\",\"_type\":\"block\",\"children\":[{\"_key\":\"90106a3d0781\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Single-turn evaluations\"},{\"_key\":\"7be051d2d833\",\"_type\":\"span\",\"marks\":[],\"text\":\" are straightforward: a prompt, a response, and grading logic. For earlier LLMs, single-turn, non-agentic evals were the main evaluation method. As AI capabilities have advanced, \"},{\"_key\":\"504185e6ce3f\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"multi-turn evaluations\"},{\"_key\":\"ca2e4291959c\",\"_type\":\"span\",\"marks\":[],\"text\":\" have become increasingly common.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"6c90c2458cb7\",\"_type\":\"image\",\"asset\":{\"_ref\":\"image-bd42e7b2f3e9bb5218142796d3ede4816588dec0-4584x2834-png\",\"_type\":\"reference\"},\"caption\":[{\"_key\":\"53ee5baf8367\",\"_type\":\"block\",\"children\":[{\"_key\":\"828008383a1f\",\"_type\":\"span\",\"marks\":[],\"text\":\"In a simple eval, an agent processes a prompt, and a grader checks if the output matches expectations. For a more complex multi-turn eval, a coding agent receives tools, a task (building an MCP server in this case), and an environment, executes an \\\"agent loop\\\" (tool calls and reasoning), and updates the environment with the implementation. Grading then uses unit tests to verify the working MCP server.\"}],\"markDefs\":[],\"style\":\"normal\"}],\"columnWidth\":\"inline-width\",\"height\":2834,\"markDefs\":null,\"url\":\"https://cdn.sanity.io/images/4zrzovbb/website/bd42e7b2f3e9bb5218142796d3ede4816588dec0-4584x2834.png\",\"width\":4584},{\"_key\":\"b457ad0cb003\",\"_type\":\"block\",\"children\":[{\"_key\":\"3cd40b519830\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Agent evaluations \"},{\"_key\":\"94e229954865\",\"_type\":\"span\",\"marks\":[],\"text\":\"are even more complex. Agents use tools across many turns, modifying state in the environment and adapting as they go‚Äîwhich means mistakes can propagate and compound. Frontier models can also find creative solutions that surpass the limits of static evals. For instance, Opus 4.5 solved a \"},{\"_key\":\"ad574fe4086c\",\"_type\":\"span\",\"marks\":[\"317691727ca9\"],\"text\":\"ùúè2-bench\"},{\"_key\":\"23b78a9a0441\",\"_type\":\"span\",\"marks\":[],\"text\":\" problem about booking a flight by \"},{\"_key\":\"1f53d26f69c3\",\"_type\":\"span\",\"marks\":[\"0fb636359f33\"],\"text\":\"discovering\"},{\"_key\":\"9204848eb8e2\",\"_type\":\"span\",\"marks\":[],\"text\":\" a loophole in the policy. It ‚Äúfailed‚Äù the evaluation as written, but actually came up with a better solution for the user.\"}],\"markDefs\":[{\"_key\":\"317691727ca9\",\"_type\":\"link\",\"href\":\"https://github.com/sierra-research/tau2-bench\"},{\"_key\":\"0fb636359f33\",\"_type\":\"link\",\"href\":\"/www.anthropic.com/news/claude-opus-4-5\"}],\"style\":\"normal\"},{\"_key\":\"775705cb1bac\",\"_type\":\"block\",\"children\":[{\"_key\":\"a43434660ca6\",\"_type\":\"span\",\"marks\":[],\"text\":\"\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"c7ac3f270f75\",\"_type\":\"block\",\"children\":[{\"_key\":\"54f154aa7858\",\"_type\":\"span\",\"marks\":[],\"text\":\"When building agent evaluations, we use the following definitions:\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"869ebe11b3b9\",\"_type\":\"block\",\"children\":[{\"_key\":\"6dae078cd7a9\",\"_type\":\"span\",\"marks\":[],\"text\":\"A\"},{\"_key\":\"a366dd1268c7\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\" task \"},{\"_key\":\"76ee17560237\",\"_type\":\"span\",\"marks\":[],\"text\":\"(a.k.a \"},{\"_key\":\"555299325f58\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"problem\"},{\"_key\":\"22d24b3355d9\",\"_type\":\"span\",\"marks\":[],\"text\":\" or \"},{\"_key\":\"4462e5569cd9\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"test case\"},{\"_key\":\"c74417ebe182\",\"_type\":\"span\",\"marks\":[],\"text\":\") is a single test with defined inputs and success criteria.\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"7d593a6d1faf\",\"_type\":\"block\",\"children\":[{\"_key\":\"dac1ca20ec2e\",\"_type\":\"span\",\"marks\":[],\"text\":\"Each attempt at a task is a \"},{\"_key\":\"9d6f806c0941\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"trial\"},{\"_key\":\"650965c53239\",\"_type\":\"span\",\"marks\":[],\"text\":\". Because model outputs vary between runs, we run multiple trials to produce more consistent results.\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"78850f833cb3\",\"_type\":\"block\",\"children\":[{\"_key\":\"fab88832801f\",\"_type\":\"span\",\"marks\":[],\"text\":\"A \"},{\"_key\":\"b28462c4c677\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"grader \"},{\"_key\":\"2070d392a2cb\",\"_type\":\"span\",\"marks\":[],\"text\":\"is logic that scores some aspect of the agent‚Äôs performance. A task can have multiple graders, each containing multiple assertions (sometimes called \"},{\"_key\":\"3b77817ff59b\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"checks\"},{\"_key\":\"5e338830e078\",\"_type\":\"span\",\"marks\":[],\"text\":\")\"},{\"_key\":\"96878f239abd\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\".\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"f6166769866b\",\"_type\":\"block\",\"children\":[{\"_key\":\"05d35fb09fe4\",\"_type\":\"span\",\"marks\":[],\"text\":\"A \"},{\"_key\":\"d4fbf32644e2\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"transcript \"},{\"_key\":\"30f66bf52b03\",\"_type\":\"span\",\"marks\":[],\"text\":\"(also called a \"},{\"_key\":\"df0b18c5473d\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"trace \"},{\"_key\":\"8bee5f6e61b7\",\"_type\":\"span\",\"marks\":[],\"text\":\"or\"},{\"_key\":\"d57d2b94070c\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\" trajectory\"},{\"_key\":\"e36d53185f62\",\"_type\":\"span\",\"marks\":[],\"text\":\") is the complete record of a trial, including outputs, tool calls, reasoning, intermediate results, and any other interactions. For the Anthropic API, this is the full messages array at the end of an eval run - containing all the calls to the API and all of the returned responses during the evaluation.\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"e13f18c40c34\",\"_type\":\"block\",\"children\":[{\"_key\":\"9c0416e9945a\",\"_type\":\"span\",\"marks\":[],\"text\":\"The \"},{\"_key\":\"0f2b492ea59e\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"outcome\"},{\"_key\":\"9b5024cd68cf\",\"_type\":\"span\",\"marks\":[],\"text\":\" is the final state in the environment at the end of the trial. A flight-booking agent might say ‚ÄúYour flight has been booked‚Äù at the end of the transcript, but the outcome is whether a reservation exists in the environment‚Äôs SQL database.\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"c4d8d25e7749\",\"_type\":\"block\",\"children\":[{\"_key\":\"4bf43b10bdd1\",\"_type\":\"span\",\"marks\":[],\"text\":\"An\"},{\"_key\":\"a4368fdc4d8e\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\" evaluation harness\"},{\"_key\":\"62f3cd6cd856\",\"_type\":\"span\",\"marks\":[],\"text\":\" is the infrastructure that runs evals end-to-end. It provides instructions and tools, runs tasks concurrently, records all the steps, grades outputs, and aggregates results.\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"14a7a7df0f13\",\"_type\":\"block\",\"children\":[{\"_key\":\"359ace755306\",\"_type\":\"span\",\"marks\":[],\"text\":\"An \"},{\"_key\":\"84f60290567a\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"agent harness \"},{\"_key\":\"913cc653d1a1\",\"_type\":\"span\",\"marks\":[],\"text\":\"(or \"},{\"_key\":\"be6e782d5c56\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"scaffold\"},{\"_key\":\"48980f6b5767\",\"_type\":\"span\",\"marks\":[],\"text\":\") is the system that enables a model to act as an agent: it processes inputs, orchestrates tool calls, and returns results. When we evaluate ‚Äúan agent,‚Äù we‚Äôre evaluating the harness \"},{\"_key\":\"1ce17096aaee\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"and\"},{\"_key\":\"fb92510c4951\",\"_type\":\"span\",\"marks\":[],\"text\":\" the model working together. For example, \"},{\"_key\":\"9ae97dbc1757\",\"_type\":\"span\",\"marks\":[\"b3d7782c4556\"],\"text\":\"Claude Code\"},{\"_key\":\"f7be278ec031\",\"_type\":\"span\",\"marks\":[],\"text\":\" is a flexible agent harness, and we used its core primitives through the \"},{\"_key\":\"2ffdf03c5727\",\"_type\":\"span\",\"marks\":[\"2460f22f0a56\"],\"text\":\"Agent SDK\"},{\"_key\":\"4bddab55ed5c\",\"_type\":\"span\",\"marks\":[],\"text\":\" to build our \"},{\"_key\":\"61dcd381dc03\",\"_type\":\"span\",\"marks\":[\"ecd6d3e49d59\"],\"text\":\"long-running agent harness\"},{\"_key\":\"944ef27e4208\",\"_type\":\"span\",\"marks\":[],\"text\":\".\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[{\"_key\":\"b3d7782c4556\",\"_type\":\"link\",\"href\":\"https://claude.com/product/claude-code\"},{\"_key\":\"2460f22f0a56\",\"_type\":\"link\",\"href\":\"https://platform.claude.com/docs/en/agent-sdk/overview\"},{\"_key\":\"ecd6d3e49d59\",\"_type\":\"link\",\"href\":\"/www.anthropic.com/engineering/effective-harnesses-for-long-running-agents\"}],\"style\":\"normal\"},{\"_key\":\"f83285321ce3\",\"_type\":\"block\",\"children\":[{\"_key\":\"f1ebb1f032f4\",\"_type\":\"span\",\"marks\":[],\"text\":\"An \"},{\"_key\":\"12cb859ea5d2\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"evaluation suite\"},{\"_key\":\"45f8bd6125e2\",\"_type\":\"span\",\"marks\":[],\"text\":\" is a collection of tasks designed to measure specific capabilities or behaviors. Tasks in a suite typically share a broad goal. For instance, a customer support eval suite might test refunds, cancellations, and escalations.\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"486d2f14c8d3\",\"_type\":\"image\",\"asset\":{\"_ref\":\"image-0205b36f9639fc27f2f6566f73cb56b06f59d555-4584x2580-png\",\"_type\":\"reference\"},\"caption\":[{\"_key\":\"20a630d96aef\",\"_type\":\"block\",\"children\":[{\"_key\":\"786b1816e27b\",\"_type\":\"span\",\"marks\":[],\"text\":\"Components of evaluations for agents.\"}],\"markDefs\":[],\"style\":\"normal\"}],\"columnWidth\":\"inline-width\",\"height\":2580,\"markDefs\":null,\"url\":\"https://cdn.sanity.io/images/4zrzovbb/website/0205b36f9639fc27f2f6566f73cb56b06f59d555-4584x2580.png\",\"width\":4584},{\"_key\":\"88e48373f9f9\",\"_type\":\"block\",\"children\":[{\"_key\":\"7fdc39d1b868\",\"_type\":\"span\",\"marks\":[],\"text\":\"\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"a0d381145fd7\",\"_type\":\"block\",\"children\":[{\"_key\":\"600aa3ec69d9\",\"_type\":\"span\",\"marks\":[],\"text\":\"Why build evaluations?\"}],\"markDefs\":[],\"style\":\"h3\"},{\"_key\":\"53714b62e26c\",\"_type\":\"block\",\"children\":[{\"_key\":\"b3363072213f\",\"_type\":\"span\",\"marks\":[],\"text\":\"When teams first start building agents, they can get surprisingly far through a combination of manual testing, \"},{\"_key\":\"c733c93e4f83\",\"_type\":\"span\",\"marks\":[\"2004dc3defd0\"],\"text\":\"dogfooding\"},{\"_key\":\"7519745931ec\",\"_type\":\"span\",\"marks\":[],\"text\":\", and intuition. More rigorous evaluation may even seem like overhead that slows down shipping. But after the early prototyping stages, once an agent is in production and has started scaling, building without evals starts to break down.\"}],\"markDefs\":[{\"_key\":\"2004dc3defd0\",\"_type\":\"link\",\"href\":\"https://en.wikipedia.org/wiki/Eating_your_own_dog_food\"}],\"style\":\"normal\"},{\"_key\":\"f49abf94212e\",\"_type\":\"block\",\"children\":[{\"_key\":\"0b884a803174\",\"_type\":\"span\",\"marks\":[],\"text\":\"\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"54d6ec066bb0\",\"_type\":\"block\",\"children\":[{\"_key\":\"01e02947f5cb\",\"_type\":\"span\",\"marks\":[],\"text\":\"The breaking point often comes when users report the agent feels worse after changes, and the team is ‚Äúflying blind‚Äù with no way to verify except to guess and check. Absent evals, debugging is reactive: wait for complaints, reproduce manually, fix the bug, and hope nothing else regressed. Teams can't distinguish real regressions from noise, automatically test changes against hundreds of scenarios before shipping, or measure improvements.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"f60cf46c06c4\",\"_type\":\"block\",\"children\":[{\"_key\":\"459e4ac4fec3\",\"_type\":\"span\",\"marks\":[],\"text\":\"\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"5f6cccbcecb9\",\"_type\":\"block\",\"children\":[{\"_key\":\"0858d6b2a435\",\"_type\":\"span\",\"marks\":[],\"text\":\"We‚Äôve seen this progression play out many times. For instance, Claude Code started with fast iteration based on feedback from Anthropic employees and external users. Later, we added evals‚Äîfirst for narrow areas like concision and file edits, and then for more complex behaviors like over-engineering. These evals helped identify issues, guide improvements, and focus research-product collaborations. Combined with production monitoring, A/B tests, user research, and more, evals provide signals to continue improving Claude Code as it scales.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"992b0d2b2f7e\",\"_type\":\"block\",\"children\":[{\"_key\":\"3b83249a5af3\",\"_type\":\"span\",\"marks\":[],\"text\":\"\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"eb3d34cd3e09\",\"_type\":\"block\",\"children\":[{\"_key\":\"82b56714de42\",\"_type\":\"span\",\"marks\":[],\"text\":\"Writing evals is useful at any stage in the agent lifecycle. Early on, evals force product teams to specify what success means for the agent, while later they help uphold a consistent quality bar.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"bca814c94ba4\",\"_type\":\"block\",\"children\":[{\"_key\":\"3e17d9eaa5b8\",\"_type\":\"span\",\"marks\":[],\"text\":\"\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"cbea50e4adc7\",\"_type\":\"block\",\"children\":[{\"_key\":\"ed496831812b\",\"_type\":\"span\",\"marks\":[\"0020014cf1ac\"],\"text\":\"Descript\"},{\"_key\":\"a837035eb697\",\"_type\":\"span\",\"marks\":[],\"text\":\"‚Äôs agent helps users edit videos, so they built evals around three dimensions of a successful editing workflow: don‚Äôt break things, do what I asked, and do it well. They evolved from manual grading to LLM graders with criteria defined by the product team and periodic human calibration, and now regularly run two separate suites for quality benchmarking and regression testing. The \"},{\"_key\":\"acb459f69bf4\",\"_type\":\"span\",\"marks\":[\"b080580dd658\"],\"text\":\"Bolt\"},{\"_key\":\"4673d40e3234\",\"_type\":\"span\",\"marks\":[],\"text\":\" AI team started building evals later, after they already had a widely used agent. In 3 months, they built an eval system that runs their agent and grades outputs with static analysis, uses browser agents to test apps, and employs LLM judges for behaviors like instruction following.\"}],\"markDefs\":[{\"_key\":\"0020014cf1ac\",\"_type\":\"link\",\"href\":\"https://www.descript.com/\"},{\"_key\":\"b080580dd658\",\"_type\":\"link\",\"href\":\"https://bolt.new/\"}],\"style\":\"normal\"},{\"_key\":\"78f1b73a6f93\",\"_type\":\"block\",\"children\":[{\"_key\":\"05d94c8bf4c7\",\"_type\":\"span\",\"marks\":[],\"text\":\"\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"13aab2c47982\",\"_type\":\"block\",\"children\":[{\"_key\":\"e14de50b57b2\",\"_type\":\"span\",\"marks\":[],\"text\":\"Some teams create evals at the start of development; others add them once at scale when evals become a bottleneck for improving the agent. Evals are especially useful at the start of agent development to explicitly encode expected behavior. Two engineers reading the same initial spec could come away with different interpretations on how the AI should handle edge cases. An eval suite resolves this ambiguity. Regardless of when they‚Äôre created, evals help accelerate development.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"8338236d17ce\",\"_type\":\"block\",\"children\":[{\"_key\":\"11e74ec7d3bc\",\"_type\":\"span\",\"marks\":[],\"text\":\"Evals also shape how quickly you can adopt new models. When more powerful models come out, teams without evals face weeks of testing while competitors with evals can quickly determine the model‚Äôs strengths, tune their prompts, and upgrade in days.¬†\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"83c427ff98fa\",\"_type\":\"block\",\"children\":[{\"_key\":\"19dd38a7ceb1\",\"_type\":\"span\",\"marks\":[],\"text\":\"\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"441268de49e9\",\"_type\":\"block\",\"children\":[{\"_key\":\"0782ac0fe304\",\"_type\":\"span\",\"marks\":[],\"text\":\"Once evals exist, you get baselines and regression tests for free: latency, token usage, cost per task, and error rates can be tracked on a static bank of tasks. Evals can also become the highest-bandwidth communication channel between product and research teams, defining metrics researchers can optimize against. Clearly, evals have wide-ranging benefits beyond tracking regressions and improvements. Their compounding value is easy to miss given that costs are visible upfront while benefits accumulate later.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"7b5886488333\",\"_type\":\"block\",\"children\":[{\"_key\":\"0ee0a3f57a06\",\"_type\":\"span\",\"marks\":[],\"text\":\"How to evaluate AI agents \"}],\"markDefs\":[],\"style\":\"h3\"},{\"_key\":\"8d800fa24c4a\",\"_type\":\"block\",\"children\":[{\"_key\":\"cf105f98ce0e\",\"_type\":\"span\",\"marks\":[],\"text\":\"We see several common types of agents deployed at scale today, including coding agents, research agents, computer use agents, and conversational agents. Each type may be deployed across a wide variety of industries, but they can be evaluated using similar techniques. You don‚Äôt need to invent an evaluation from scratch. The sections below describe proven techniques for several agent types. Use these methods as a foundation, then extend them to your domain.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"f29715a6c690\",\"_type\":\"block\",\"children\":[{\"_key\":\"3155606d3ffb\",\"_type\":\"span\",\"marks\":[],\"text\":\"Types of graders for agents\"}],\"markDefs\":[],\"style\":\"h4\"},{\"_key\":\"f4ecd471f2b0\",\"_type\":\"block\",\"children\":[{\"_key\":\"3155606d3ffb\",\"_type\":\"span\",\"marks\":[],\"text\":\"Agent evaluations typically combine three types of graders: code-based, model-based, and human. Each grader evaluates some portion of either the transcript or the outcome. An essential component of effective evaluation design is to choose the right graders for the job.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"99e8a766c92a\",\"_type\":\"table\",\"cta\":null,\"layout\":{\"direction\":\"horizontal\",\"fixedFirstColumn\":false},\"markDefs\":null,\"rows\":[{\"_key\":\"3a0210320e09\",\"_type\":\"tableRow\",\"cells\":[{\"_key\":\"f2a80fdca609\",\"_type\":\"tableCell\",\"content\":[{\"_key\":\"7acca4934d4f\",\"_type\":\"block\",\"children\":[{\"_key\":\"31dbcf65aa05\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Methods\"}],\"markDefs\":null,\"style\":\"normal\"}]},{\"_key\":\"4f47f7236c29\",\"_type\":\"tableCell\",\"content\":[{\"_key\":\"99ee0c0c3b75\",\"_type\":\"block\",\"children\":[{\"_key\":\"92efae9b5fa5\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Strengths\"}],\"markDefs\":null,\"style\":\"normal\"}]},{\"_key\":\"437d39166fe2\",\"_type\":\"tableCell\",\"content\":[{\"_key\":\"04a017595da2\",\"_type\":\"block\",\"children\":[{\"_key\":\"5fa15a11622d\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Weaknesses\"}],\"markDefs\":null,\"style\":\"normal\"}]}],\"isHeaderRow\":true},{\"_key\":\"5209b9c36b7a\",\"_type\":\"tableRow\",\"cells\":[{\"_key\":\"de9955ead27f\",\"_type\":\"tableCell\",\"content\":[{\"_key\":\"6073f3968de8\",\"_type\":\"block\",\"children\":[{\"_key\":\"37a516757d7b\",\"_type\":\"span\",\"marks\":[],\"text\":\"‚Ä¢  String match checks (exact, regex, fuzzy, etc.)\\n\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"8c44ee00dfc8\",\"_type\":\"block\",\"children\":[{\"_key\":\"c875c13d243e\",\"_type\":\"span\",\"marks\":[],\"text\":\"‚Ä¢  Binary tests (fail-to-pass, pass-to-pass)\\n\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"9b16ba750d7f\",\"_type\":\"block\",\"children\":[{\"_key\":\"92dc4d9473a1\",\"_type\":\"span\",\"marks\":[],\"text\":\"‚Ä¢  Static analysis (lint, type, security)\\n\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"dc2a3c0a0d7a\",\"_type\":\"block\",\"children\":[{\"_key\":\"6f6b82619771\",\"_type\":\"span\",\"marks\":[],\"text\":\"‚Ä¢  Outcome verification\\n\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"88b7b49594f4\",\"_type\":\"block\",\"children\":[{\"_key\":\"10766e52c1f1\",\"_type\":\"span\",\"marks\":[],\"text\":\"‚Ä¢  Tool calls verification (tools used, parameters)\\n\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"35ee433f8482\",\"_type\":\"block\",\"children\":[{\"_key\":\"4081bdfba7a6\",\"_type\":\"span\",\"marks\":[],\"text\":\"‚Ä¢  Transcript analysis (turns taken, token usage)\"}],\"markDefs\":[],\"style\":\"normal\"}]},{\"_key\":\"ef89d0047c57\",\"_type\":\"tableCell\",\"content\":[{\"_key\":\"180e3d624018\",\"_type\":\"block\",\"children\":[{\"_key\":\"6d6afcc0254c\",\"_type\":\"span\",\"text\":\"‚Ä¢  Fast\\n\"}],\"markDefs\":null,\"style\":\"normal\"},{\"_key\":\"60a4a71b316d\",\"_type\":\"block\",\"children\":[{\"_key\":\"1a39ae4957ec\",\"_type\":\"span\",\"text\":\"‚Ä¢  Cheap\\n\"}],\"markDefs\":null,\"style\":\"normal\"},{\"_key\":\"f4a4b764ff9c\",\"_type\":\"block\",\"children\":[{\"_key\":\"119b53536289\",\"_type\":\"span\",\"text\":\"‚Ä¢  Objective\\n\"}],\"markDefs\":null,\"style\":\"normal\"},{\"_key\":\"2adc647358db\",\"_type\":\"block\",\"children\":[{\"_key\":\"5da9b450d1ac\",\"_type\":\"span\",\"text\":\"‚Ä¢  Reproducible\\n\"}],\"markDefs\":null,\"style\":\"normal\"},{\"_key\":\"6a6b3d5b8cc7\",\"_type\":\"block\",\"children\":[{\"_key\":\"1aa90592e35f\",\"_type\":\"span\",\"text\":\"‚Ä¢  Easy to debug\\n\"}],\"markDefs\":null,\"style\":\"normal\"},{\"_key\":\"b7eef2d5d849\",\"_type\":\"block\",\"children\":[{\"_key\":\"f6c975ba1f5d\",\"_type\":\"span\",\"text\":\"‚Ä¢  Verify specific conditions\"}],\"markDefs\":null,\"style\":\"normal\"},{\"_key\":\"4ba7ab1ff373\",\"_type\":\"block\",\"children\":[{\"_key\":\"813c4cda9480\",\"_type\":\"span\",\"text\":\"\\n\"}],\"markDefs\":null,\"style\":\"normal\"}]},{\"_key\":\"6946059c0260\",\"_type\":\"tableCell\",\"content\":[{\"_key\":\"4bad82b3688f\",\"_type\":\"block\",\"children\":[{\"_key\":\"02e8236cf29c\",\"_type\":\"span\",\"text\":\"‚Ä¢  Brittle to valid variations that don‚Äôt match expected patterns exactly\\n\"}],\"markDefs\":null,\"style\":\"normal\"},{\"_key\":\"5661dc2785c0\",\"_type\":\"block\",\"children\":[{\"_key\":\"b015863411b6\",\"_type\":\"span\",\"text\":\"‚Ä¢  Lacking in nuance\\n\"}],\"markDefs\":null,\"style\":\"normal\"},{\"_key\":\"88ac69b8755c\",\"_type\":\"block\",\"children\":[{\"_key\":\"e1e22070d32c\",\"_type\":\"span\",\"text\":\"‚Ä¢  Limited for evaluating some more subjective tasks\\n\"}],\"markDefs\":null,\"style\":\"normal\"}]}],\"isHeaderRow\":false}],\"title\":\"Code-based graders\"},{\"_key\":\"1135f591bd42\",\"_type\":\"table\",\"cta\":null,\"layout\":{\"direction\":\"horizontal\",\"fixedFirstColumn\":false},\"markDefs\":null,\"rows\":[{\"_key\":\"82fdf428c8cd\",\"_type\":\"tableRow\",\"cells\":[{\"_key\":\"32f377ffbda6\",\"_type\":\"tableCell\",\"content\":[{\"_key\":\"c184ac201dc0\",\"_type\":\"block\",\"children\":[{\"_key\":\"6372a65af954\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Methods\"}],\"markDefs\":[],\"style\":\"normal\"}]},{\"_key\":\"83b97f30e4f4\",\"_type\":\"tableCell\",\"content\":[{\"_key\":\"b15e0b41d1b2\",\"_type\":\"block\",\"children\":[{\"_key\":\"2ce3ae20adfb\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Strengths\"}],\"markDefs\":[],\"style\":\"normal\"}]},{\"_key\":\"3dc23cc413b5\",\"_type\":\"tableCell\",\"content\":[{\"_key\":\"4d0555d6c71c\",\"_type\":\"block\",\"children\":[{\"_key\":\"5155a10d4454\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Weaknesses\"}],\"markDefs\":[],\"style\":\"normal\"}]}],\"isHeaderRow\":true},{\"_key\":\"50f17770155c\",\"_type\":\"tableRow\",\"cells\":[{\"_key\":\"aa3ea6b106ea\",\"_type\":\"tableCell\",\"content\":[{\"_key\":\"e9199bd04013\",\"_type\":\"block\",\"children\":[{\"_key\":\"0ee82338b0f3\",\"_type\":\"span\",\"marks\":[],\"text\":\"Rubric-based scoring\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"b83337a08660\",\"_type\":\"block\",\"children\":[{\"_key\":\"1552ac37682b\",\"_type\":\"span\",\"marks\":[],\"text\":\"Natural language assertions\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"ebe23827b71f\",\"_type\":\"block\",\"children\":[{\"_key\":\"1d24895e5c4f\",\"_type\":\"span\",\"marks\":[],\"text\":\"Pairwise comparison\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"dcfa3743a301\",\"_type\":\"block\",\"children\":[{\"_key\":\"f27c1cc0681b\",\"_type\":\"span\",\"marks\":[],\"text\":\"Reference-based evaluation\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"a6479df6913c\",\"_type\":\"block\",\"children\":[{\"_key\":\"b6b0f983ff13\",\"_type\":\"span\",\"marks\":[],\"text\":\"Multi-judge consensus\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"b40f24587c1a\",\"_type\":\"block\",\"children\":[{\"_key\":\"475e4765092d\",\"_type\":\"span\",\"marks\":[],\"text\":\"\\n\"}],\"markDefs\":[],\"style\":\"normal\"}]},{\"_key\":\"0d158021c31f\",\"_type\":\"tableCell\",\"content\":[{\"_key\":\"a2f193010631\",\"_type\":\"block\",\"children\":[{\"_key\":\"bfe0922ee9b1\",\"_type\":\"span\",\"marks\":[],\"text\":\"Flexible\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"b9ddab353c65\",\"_type\":\"block\",\"children\":[{\"_key\":\"83f48366e7eb\",\"_type\":\"span\",\"marks\":[],\"text\":\"Scalable\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"443ac1cc21e6\",\"_type\":\"block\",\"children\":[{\"_key\":\"6cfc12004834\",\"_type\":\"span\",\"marks\":[],\"text\":\"Captures nuance\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"7f4373e10f54\",\"_type\":\"block\",\"children\":[{\"_key\":\"30847109f56d\",\"_type\":\"span\",\"marks\":[],\"text\":\"Handles open-ended tasks\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"f36b7d8d8317\",\"_type\":\"block\",\"children\":[{\"_key\":\"150acab5e8b5\",\"_type\":\"span\",\"marks\":[],\"text\":\"Handles freeform output\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"9e66d04054a5\",\"_type\":\"block\",\"children\":[{\"_key\":\"87e79b174cd7\",\"_type\":\"span\",\"marks\":[],\"text\":\"\\n\"}],\"markDefs\":[],\"style\":\"normal\"}]},{\"_key\":\"c8ee27aee2a7\",\"_type\":\"tableCell\",\"content\":[{\"_key\":\"550a4bd17434\",\"_type\":\"block\",\"children\":[{\"_key\":\"668aa36cb80c\",\"_type\":\"span\",\"marks\":[],\"text\":\"Non-deterministic\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"97f1874cd741\",\"_type\":\"block\",\"children\":[{\"_key\":\"3ac95a0cd26f\",\"_type\":\"span\",\"marks\":[],\"text\":\"More expensive than code\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"6375ad3c6ad2\",\"_type\":\"block\",\"children\":[{\"_key\":\"2fe114d9a9b2\",\"_type\":\"span\",\"marks\":[],\"text\":\"Requires calibration with human graders for accuracy\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"570c02b8d9b8\",\"_type\":\"block\",\"children\":[{\"_key\":\"8cac844fd022\",\"_type\":\"span\",\"marks\":[],\"text\":\"\\n\"}],\"markDefs\":[],\"style\":\"normal\"}]}],\"isHeaderRow\":false}],\"title\":\"Model-based graders\"},{\"_key\":\"c7980f0c0fca\",\"_type\":\"table\",\"cta\":null,\"layout\":{\"direction\":\"horizontal\",\"fixedFirstColumn\":false},\"markDefs\":null,\"rows\":[{\"_key\":\"6eef372fdd15\",\"_type\":\"tableRow\",\"cells\":[{\"_key\":\"7e9d5ba7261d\",\"_type\":\"tableCell\",\"content\":[{\"_key\":\"66439b0bffd2\",\"_type\":\"block\",\"children\":[{\"_key\":\"769bf6978136\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Methods\"}],\"markDefs\":null,\"style\":\"normal\"}]},{\"_key\":\"4a074af19be2\",\"_type\":\"tableCell\",\"content\":[{\"_key\":\"cea7191e8d9e\",\"_type\":\"block\",\"children\":[{\"_key\":\"7947d753080b\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Strengths\"}],\"markDefs\":null,\"style\":\"normal\"}]},{\"_key\":\"661223501ac9\",\"_type\":\"tableCell\",\"content\":[{\"_key\":\"bdac7936fb06\",\"_type\":\"block\",\"children\":[{\"_key\":\"31b108fe408f\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Weaknesses\"}],\"markDefs\":null,\"style\":\"normal\"}]}],\"isHeaderRow\":true},{\"_key\":\"ecd36e19b1ac\",\"_type\":\"tableRow\",\"cells\":[{\"_key\":\"45e07769b1d4\",\"_type\":\"tableCell\",\"content\":[{\"_key\":\"7d72ab70c77e\",\"_type\":\"block\",\"children\":[{\"_key\":\"7fb8fa2ce0a2\",\"_type\":\"span\",\"marks\":[],\"text\":\"SME review\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"95c4dd7fcc86\",\"_type\":\"block\",\"children\":[{\"_key\":\"5b79098226bf\",\"_type\":\"span\",\"marks\":[],\"text\":\"Crowdsourced judgment\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"eb5a152ddc86\",\"_type\":\"block\",\"children\":[{\"_key\":\"ca1d07383c47\",\"_type\":\"span\",\"marks\":[],\"text\":\"Spot-check sampling\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"254f860e6342\",\"_type\":\"block\",\"children\":[{\"_key\":\"f31294c0666b\",\"_type\":\"span\",\"marks\":[],\"text\":\"A/B testing\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"d806c2e9b909\",\"_type\":\"block\",\"children\":[{\"_key\":\"40e9911235a5\",\"_type\":\"span\",\"marks\":[],\"text\":\"Inter-annotator agreement\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"579b1dc0f9ac\",\"_type\":\"block\",\"children\":[{\"_key\":\"019c0fde9e69\",\"_type\":\"span\",\"marks\":[],\"text\":\"\\n\"}],\"markDefs\":[],\"style\":\"normal\"}]},{\"_key\":\"e2be202ccc71\",\"_type\":\"tableCell\",\"content\":[{\"_key\":\"dca7a9f161b1\",\"_type\":\"block\",\"children\":[{\"_key\":\"add698213bb4\",\"_type\":\"span\",\"marks\":[],\"text\":\"Gold standard quality\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"49c5e4c2fe2d\",\"_type\":\"block\",\"children\":[{\"_key\":\"46512898d425\",\"_type\":\"span\",\"marks\":[],\"text\":\"Matches expert user judgment\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"95e7ee0ba146\",\"_type\":\"block\",\"children\":[{\"_key\":\"f71f837ff839\",\"_type\":\"span\",\"marks\":[],\"text\":\"Used to calibrate model-based graders\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"c3ce93ba830c\",\"_type\":\"block\",\"children\":[{\"_key\":\"bfc1a9f97e7a\",\"_type\":\"span\",\"marks\":[],\"text\":\"\\n\"}],\"markDefs\":[],\"style\":\"normal\"}]},{\"_key\":\"2aa845e8e6e0\",\"_type\":\"tableCell\",\"content\":[{\"_key\":\"fd5a16fb8433\",\"_type\":\"block\",\"children\":[{\"_key\":\"6be52a055001\",\"_type\":\"span\",\"marks\":[],\"text\":\"Expensive\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"10ddbc263604\",\"_type\":\"block\",\"children\":[{\"_key\":\"f6ab691d07e4\",\"_type\":\"span\",\"marks\":[],\"text\":\"Slow\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"3ec4d3106f62\",\"_type\":\"block\",\"children\":[{\"_key\":\"f70fc31e4377\",\"_type\":\"span\",\"marks\":[],\"text\":\"Often requires access to human experts at scale\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"6a6acf9e2740\",\"_type\":\"block\",\"children\":[{\"_key\":\"b4cab1d6565b\",\"_type\":\"span\",\"marks\":[],\"text\":\"\\n\"}],\"markDefs\":[],\"style\":\"normal\"}]}],\"isHeaderRow\":false}],\"title\":\"Human graders\"},{\"_key\":\"0267d1f81f67\",\"_type\":\"block\",\"children\":[{\"_key\":\"b8be8d668e69\",\"_type\":\"span\",\"marks\":[],\"text\":\"For each task, scoring can be weighted (combined grader scores must hit a threshold), binary (all graders must pass), or a hybrid.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"06598433aefa\",\"_type\":\"block\",\"children\":[{\"_key\":\"9b6c3a5c93ad\",\"_type\":\"span\",\"marks\":[],\"text\":\"Capability vs. regression evals\"}],\"markDefs\":[],\"style\":\"h4\"},{\"_key\":\"7f066d8de88b\",\"_type\":\"block\",\"children\":[{\"_key\":\"6b9051b0c3ce\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Capability or ‚Äúquality‚Äù evals\"},{\"_key\":\"3389298e618c\",\"_type\":\"span\",\"marks\":[],\"text\":\" ask, ‚ÄúWhat can this agent do well?‚Äù They should start at a low pass rate, targeting tasks the agent struggles with and giving teams a hill to climb.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"1e3b6400ab3a\",\"_type\":\"block\",\"children\":[{\"_key\":\"5ade7dedd704\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Regression evals\"},{\"_key\":\"d88f764747e2\",\"_type\":\"span\",\"marks\":[],\"text\":\" ask, ‚ÄúDoes the agent still handle all the tasks it used to?‚Äù and should have a nearly 100% pass rate. They protect against backsliding, as a decline in score signals that something is broken and needs to be improved. As teams hill-climb on capability evals, it‚Äôs important to also run regression evals to make sure changes don‚Äôt cause issues elsewhere.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"e4f9c8cd1efb\",\"_type\":\"block\",\"children\":[{\"_key\":\"d6c2759a62e4\",\"_type\":\"span\",\"marks\":[],\"text\":\"After an agent is launched and optimized, capability evals with high pass rates can ‚Äúgraduate‚Äù to become a regression suite that is run continuously to catch any drift. Tasks that once measured ‚ÄúCan we do this at all?‚Äù then measure ‚ÄúCan we still do this reliably?‚Äù\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"b05a5341d4ab\",\"_type\":\"block\",\"children\":[{\"_key\":\"4d5f692aff58\",\"_type\":\"span\",\"marks\":[],\"text\":\"Evaluating coding agents\"}],\"markDefs\":[],\"style\":\"h4\"},{\"_key\":\"d8b2bba3ec90\",\"_type\":\"block\",\"children\":[{\"_key\":\"e14bceddad05\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Coding agents\"},{\"_key\":\"1be500106757\",\"_type\":\"span\",\"marks\":[],\"text\":\" write, test, and debug code, navigating codebases and running commands much like a human developer. Effective evals for modern coding agents usually rely on well-specified tasks, stable test environments, and thorough tests for the generated code.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"c972d858552b\",\"_type\":\"block\",\"children\":[{\"_key\":\"eeb3cd70d09b\",\"_type\":\"span\",\"marks\":[],\"text\":\"Deterministic graders are natural for coding agents because software is generally straightforward to evaluate: does the code run and do the tests pass? Two widely used coding agent benchmarks, \"},{\"_key\":\"b0fee0e2dc84\",\"_type\":\"span\",\"marks\":[\"5a407f17f2b9\"],\"text\":\"SWE-bench Verified\"},{\"_key\":\"25d966867902\",\"_type\":\"span\",\"marks\":[],\"text\":\" and \"},{\"_key\":\"1e8c2077db5b\",\"_type\":\"span\",\"marks\":[\"5a1a5500fdd0\"],\"text\":\"Terminal-Bench\"},{\"_key\":\"dc77417b503a\",\"_type\":\"span\",\"marks\":[],\"text\":\", follow this approach. SWE-bench Verified gives agents GitHub issues from popular Python repositories and grades solutions by running the test suite; a solution passes only if it fixes the failing tests without breaking existing ones. LLMs have progressed from 40% to \u003e80% on this eval in just one year. Terminal-Bench takes a different track: it tests end-to-end technical tasks, such as building a Linux kernel from source or training an ML model.\"}],\"markDefs\":[{\"_key\":\"5a407f17f2b9\",\"_type\":\"link\",\"href\":\"https://www.swebench.com/SWE-bench/\"},{\"_key\":\"5a1a5500fdd0\",\"_type\":\"link\",\"href\":\"https://www.tbench.ai/\"}],\"style\":\"normal\"},{\"_key\":\"6974e626f32d\",\"_type\":\"block\",\"children\":[{\"_key\":\"ece74c1bb12d\",\"_type\":\"span\",\"marks\":[],\"text\":\"Once you have a set of pass-or-fail tests for validating the key \"},{\"_key\":\"027db5ab9097\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"outcomes\"},{\"_key\":\"fd7aa29216f6\",\"_type\":\"span\",\"marks\":[],\"text\":\" of a coding task, it‚Äôs often useful to also grade the transcript\"},{\"_key\":\"bddabf53e5c3\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\". \"},{\"_key\":\"c332c6185622\",\"_type\":\"span\",\"marks\":[],\"text\":\"For instance, heuristics-based code quality rules can evaluate the generated code based on more than passing tests, and model-based graders with clear rubrics can assess behaviors like how the agent calls tools or interacts with the user.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"d7ae40e7d28d\",\"_type\":\"block\",\"children\":[{\"_key\":\"33517418efa8\",\"_type\":\"span\",\"marks\":[],\"text\":\"\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"a1481b27dccc\",\"_type\":\"block\",\"children\":[{\"_key\":\"11424d0134d0\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Example: Theoretical evaluation for a coding agent\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"2e7bddb75c4b\",\"_type\":\"block\",\"children\":[{\"_key\":\"68f6f96fcaba\",\"_type\":\"span\",\"marks\":[],\"text\":\"Consider a coding task where the agent must fix an authentication bypass vulnerability. As shown in the illustrative YAML file below, one could evaluate this agent using both graders and metrics.¬†\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"f435296b97b4\",\"_type\":\"codeBlock\",\"code\":\"task:\\n  id: \\\"fix-auth-bypass_1\\\"\\n  desc: \\\"Fix authentication bypass when password field is empty and ...\\\"\\n  graders:\\n    - type: deterministic_tests\\n      required: [test_empty_pw_rejected.py, test_null_pw_rejected.py]\\n    - type: llm_rubric\\n      rubric: prompts/code_quality.md\\n    - type: static_analysis\\n      commands: [ruff, mypy, bandit]\\n    - type: state_check\\n      expect:\\n        security_logs: {event_type: \\\"auth_blocked\\\"}\\n    - type: tool_calls\\n      required:\\n        - {tool: read_file, params: {path: \\\"src/auth/*\\\"}}\\n        - {tool: edit_file}\\n        - {tool: run_tests}\\n  tracked_metrics:\\n    - type: transcript\\n      metrics:\\n        - n_turns\\n        - n_toolcalls\\n        - n_total_tokens\\n    - type: latency\\n      metrics:\\n        - time_to_first_token\\n        - output_tokens_per_sec\\n        - time_to_last_token\",\"markDefs\":null},{\"_key\":\"7bcb4dd53225\",\"_type\":\"block\",\"children\":[{\"_key\":\"b80065cf72a7\",\"_type\":\"span\",\"marks\":[],\"text\":\"Note that this example showcases the full range of available graders for illustration. In practice, coding evaluations typically rely on unit tests for correctness verification and an LLM rubric for assessing overall code quality, with additional graders and metrics added only as needed.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"0c7d1b2a0c1b\",\"_type\":\"block\",\"children\":[{\"_key\":\"81929d0b8aba\",\"_type\":\"span\",\"marks\":[],\"text\":\"Evaluating conversational agents\"}],\"markDefs\":[],\"style\":\"h4\"},{\"_key\":\"1c3425840bfd\",\"_type\":\"block\",\"children\":[{\"_key\":\"1b37006513a0\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Conversational agents \"},{\"_key\":\"0b7dfb900e6d\",\"_type\":\"span\",\"marks\":[],\"text\":\"interact with users in domains like support, sales, or coaching. Unlike traditional chatbots, they maintain state, use tools, and take actions mid-conversation. While coding and research agents can also involve many turns of interaction with the user, conversational agents present a distinct challenge: the quality of the interaction itself is part of what you're evaluating. Effective evals for conversational agents usually rely on verifiable end-state outcomes and rubrics that capture both task completion and interaction quality. Unlike most other evals, they often require a second LLM to simulate the user. We use this approach in our \"},{\"_key\":\"d2d764e002ed\",\"_type\":\"span\",\"marks\":[\"32c58edc2bfa\"],\"text\":\"alignment auditing agents\"},{\"_key\":\"ed844babc5ac\",\"_type\":\"span\",\"marks\":[],\"text\":\" to stress-test models through extended, adversarial conversations.\"}],\"markDefs\":[{\"_key\":\"32c58edc2bfa\",\"_type\":\"link\",\"href\":\"https://alignment.anthropic.com/2025/automated-auditing/\"}],\"style\":\"normal\"},{\"_key\":\"5fad9d8ddcb5\",\"_type\":\"block\",\"children\":[{\"_key\":\"866cb745c012\",\"_type\":\"span\",\"marks\":[],\"text\":\"Success for conversational agents can be multidimensional: is the ticket resolved (state check), did it finish in \u003c10 turns (transcript constraint), and was the tone appropriate (LLM rubric)? Two benchmarks that incorporate multidimensionality are \"},{\"_key\":\"349bf8324135\",\"_type\":\"span\",\"marks\":[\"6d4d728b98af\"],\"text\":\"ùúè-Bench\"},{\"_key\":\"b1af36e1fd07\",\"_type\":\"span\",\"marks\":[],\"text\":\" and its successor, \"},{\"_key\":\"49deef595e5e\",\"_type\":\"span\",\"marks\":[\"9d7f95b242b2\"],\"text\":\"œÑ2-Bench\"},{\"_key\":\"ee0089e79644\",\"_type\":\"span\",\"marks\":[],\"text\":\". These simulate multi-turn interactions across domains like retail support and airline booking, where one model plays a user persona while the agent navigates realistic scenarios.\"}],\"markDefs\":[{\"_key\":\"6d4d728b98af\",\"_type\":\"link\",\"href\":\"https://arxiv.org/abs/2406.12045\"},{\"_key\":\"9d7f95b242b2\",\"_type\":\"link\",\"href\":\"https://arxiv.org/abs/2506.07982\"}],\"style\":\"normal\"},{\"_key\":\"88da405e8d14\",\"_type\":\"block\",\"children\":[{\"_key\":\"4914d14d7a61\",\"_type\":\"span\",\"marks\":[],\"text\":\"\\n\"},{\"_key\":\"7459cf710ee5\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Example: Theoretical evaluation for a conversational agent\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"722519a3578f\",\"_type\":\"block\",\"children\":[{\"_key\":\"84e39b8c3357\",\"_type\":\"span\",\"marks\":[],\"text\":\"Consider a support task where the agent must handle a refund for a frustrated customer.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"ac2657211520\",\"_type\":\"block\",\"children\":[{\"_key\":\"59ced2732348\",\"_type\":\"span\",\"marks\":[],\"text\":\"\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"a2971d3a2ab0\",\"_type\":\"codeBlock\",\"code\":\"graders:\\n  - type: llm_rubric\\n    rubric: prompts/support_quality.md\\n    assertions:\\n      - \\\"Agent showed empathy for customer's frustration\\\"\\n      - \\\"Resolution was clearly explained\\\"\\n      - \\\"Agent's response grounded in fetch_policy tool results\\\"\\n  - type: state_check\\n    expect:\\n      tickets: {status: resolved}\\n      refunds: {status: processed}\\n  - type: tool_calls\\n    required:\\n      - {tool: verify_identity}\\n      - {tool: process_refund, params: {amount: \\\"\u003c=100\\\"}}\\n      - {tool: send_confirmation}\\n  - type: transcript\\n    max_turns: 10\\ntracked_metrics:\\n  - type: transcript\\n    metrics:\\n      - n_turns\\n      - n_toolcalls\\n      - n_total_tokens\\n  - type: latency\\n    metrics:\\n      - time_to_first_token\\n      - output_tokens_per_sec\\n      - time_to_last_token\",\"markDefs\":null},{\"_key\":\"3d21aef61b7e\",\"_type\":\"block\",\"children\":[{\"_key\":\"2d1ba545a865\",\"_type\":\"span\",\"marks\":[],\"text\":\"As in our coding agent example, this task showcases multiple grader types for illustration. In practice, conversational agent evaluations typically use model-based graders to assess both communication quality and goal completion, because many tasks‚Äîlike answering a question‚Äîmay have multiple ‚Äúcorrect‚Äù solutions.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"e058682a0cef\",\"_type\":\"block\",\"children\":[{\"_key\":\"183449ed17a9\",\"_type\":\"span\",\"marks\":[],\"text\":\"Evaluating research agents\"}],\"markDefs\":[],\"style\":\"h4\"},{\"_key\":\"cc5a6a94c7e2\",\"_type\":\"block\",\"children\":[{\"_key\":\"3769fe88c030\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Research agents\"},{\"_key\":\"3af9f3e6fd89\",\"_type\":\"span\",\"marks\":[],\"text\":\" gather, synthesize, and analyze information, then produce outputs like an answer or report. Unlike coding agents where unit tests provide binary pass/fail signals, research quality can only be judged relative to the task. What counts as ‚Äúcomprehensive,‚Äù ‚Äúwell-sourced,‚Äù or even ‚Äúcorrect‚Äù depends on context: a market scan, due diligence for an acquisition, and a scientific report each require different standards.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"99cfda49bfe7\",\"_type\":\"block\",\"children\":[{\"_key\":\"feeb0d73677d\",\"_type\":\"span\",\"marks\":[],\"text\":\"Research evals face unique challenges: experts may disagree on whether a synthesis is comprehensive, ground truth shifts as reference content changes constantly, and longer, more open-ended outputs create more room for mistakes. A benchmark like \"},{\"_key\":\"b013ae829f7a\",\"_type\":\"span\",\"marks\":[\"29d6fe6893a9\"],\"text\":\"BrowseComp\"},{\"_key\":\"0d95114647d0\",\"_type\":\"span\",\"marks\":[],\"text\":\", for example, tests whether AI agents can find needles in haystacks across the open web‚Äîquestions designed to be easy to verify but hard to solve.\"}],\"markDefs\":[{\"_key\":\"29d6fe6893a9\",\"_type\":\"link\",\"href\":\"http://arxiv.org/abs/2504.12516\"}],\"style\":\"normal\"},{\"_key\":\"d8684bf82503\",\"_type\":\"block\",\"children\":[{\"_key\":\"c06559ec2612\",\"_type\":\"span\",\"marks\":[],\"text\":\"One strategy to build research agent evals is to combine grader types. Groundedness checks verify that claims are supported by retrieved sources, coverage checks define key facts a good answer must include, and source quality checks confirm the consulted sources are authoritative, rather than simply the first retrieved. For tasks with objectively correct answers (‚ÄúWhat was Company X‚Äôs Q3 revenue?‚Äù), exact match works. An LLM can flag unsupported claims and gaps in coverage but also verify the open-ended synthesis for coherence and completeness.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"a5c7a38ff02d\",\"_type\":\"block\",\"children\":[{\"_key\":\"2d28ad505e66\",\"_type\":\"span\",\"marks\":[],\"text\":\"Given the subjective nature of research quality, LLM-based rubrics should be frequently calibrated against expert human judgment to grade these agents effectively.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"64fa67a64731\",\"_type\":\"block\",\"children\":[{\"_key\":\"7b34ebf15e48\",\"_type\":\"span\",\"marks\":[],\"text\":\"Computer use agents\"}],\"markDefs\":[],\"style\":\"h4\"},{\"_key\":\"377b5db2512a\",\"_type\":\"block\",\"children\":[{\"_key\":\"de26d198fb2b\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Computer use agents\"},{\"_key\":\"5d81071a047c\",\"_type\":\"span\",\"marks\":[],\"text\":\" interact with software through the same interface as humans‚Äîscreenshots, mouse clicks, keyboard inputs, and scrolling‚Äîrather than through APIs or code execution. They can use any application with a graphical user interface (GUI), from design tools to legacy enterprise software. Evaluation requires running the agent in a real or sandboxed environment where it can use software applications and checking whether it achieved the intended outcome. For instance, \"},{\"_key\":\"143ead00b733\",\"_type\":\"span\",\"marks\":[\"5c56a8b251ee\"],\"text\":\"WebArena\"},{\"_key\":\"9f302beefeb2\",\"_type\":\"span\",\"marks\":[],\"text\":\" tests browser-based tasks, using URL and page state checks to verify the agent navigated correctly, along with backend state verification for tasks that modify data (confirming an order was actually placed, not just that the confirmation page appeared). \"},{\"_key\":\"c8513401e403\",\"_type\":\"span\",\"marks\":[\"40856528d1e1\"],\"text\":\"OSWorld\"},{\"_key\":\"c5ab4a5df43e\",\"_type\":\"span\",\"marks\":[],\"text\":\" extends this to full operating system control, with evaluation scripts that inspect diverse artifacts after task completion: file system state, application configs, database contents, and UI element properties.\"}],\"markDefs\":[{\"_key\":\"5c56a8b251ee\",\"_type\":\"link\",\"href\":\"https://arxiv.org/abs/2307.13854\"},{\"_key\":\"40856528d1e1\",\"_type\":\"link\",\"href\":\"https://os-world.github.io/\"}],\"style\":\"normal\"},{\"_key\":\"55997e1842f3\",\"_type\":\"block\",\"children\":[{\"_key\":\"fca228e6c3e0\",\"_type\":\"span\",\"marks\":[],\"text\":\"Browser use agents require a balance between token efficiency and latency. DOM-based interactions execute quickly but consume many tokens, while screenshot-based interactions are slower but more token-efficient. For example, when asking Claude to summarize Wikipedia, it is more efficient to extract the text from the DOM. When finding a new laptop case on Amazon, it is more efficient to take screenshots (as extracting the entire DOM is token-intensive). In our Claude for Chrome product, we developed evals to check that the agent was selecting the right tool for each context. This enabled us to complete browser-based tasks faster and more accurately.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"cf04e8379496\",\"_type\":\"block\",\"children\":[{\"_key\":\"e717c9308eb3\",\"_type\":\"span\",\"marks\":[],\"text\":\"How to think about non-determinism in evaluations for agents\"}],\"markDefs\":[],\"style\":\"h4\"},{\"_key\":\"5b8ce29fd944\",\"_type\":\"block\",\"children\":[{\"_key\":\"324bdf1bd6c4\",\"_type\":\"span\",\"marks\":[],\"text\":\"Regardless of agent type, agent behavior varies between runs, which makes evaluation results harder to interpret than they first appear. Each task has its own success rate‚Äîmaybe 90% on one task, 50% on another‚Äîand a task that passed on one eval run might fail on the next. Sometimes, what we want to measure is how \"},{\"_key\":\"0913e6419438\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"often\"},{\"_key\":\"89df6cebc994\",\"_type\":\"span\",\"marks\":[],\"text\":\" (what proportion of the trials) an agent succeeds for a task.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"8f87775c5f94\",\"_type\":\"block\",\"children\":[{\"_key\":\"aaf91ad3d817\",\"_type\":\"span\",\"marks\":[],\"text\":\"Two metrics help capture this nuance:\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"1f82c8eca540\",\"_type\":\"block\",\"children\":[{\"_key\":\"fec67b713eb4\",\"_type\":\"span\",\"marks\":[\"b77048cd8944\",\"strong\"],\"text\":\"pass@k\"},{\"_key\":\"fc56300e2778\",\"_type\":\"span\",\"marks\":[],\"text\":\" measures the likelihood that an agent gets at least one correct solution in \"},{\"_key\":\"37576c189f80\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"k\"},{\"_key\":\"f3c260580f30\",\"_type\":\"span\",\"marks\":[],\"text\":\" attempts. As \"},{\"_key\":\"52d8f826f418\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"k\"},{\"_key\":\"9cfe19af2af5\",\"_type\":\"span\",\"marks\":[],\"text\":\" increases, pass@k score rises: more ‚Äúshots on goal‚Äù means higher odds of at least 1 success. A score of 50% pass@1 means that a model succeeds at half the tasks in the eval on its first try. In coding, we‚Äôre often most interested in the agent finding the solution on the first try‚Äîpass@1. In other cases, proposing many solutions is valid as long as one works.\"}],\"markDefs\":[{\"_key\":\"b77048cd8944\",\"_type\":\"link\",\"href\":\"https://proceedings.neurips.cc/paper/2019/file/7298332f04ac004a0ca44cc69ecf6f6b-Paper.pdf\"}],\"style\":\"normal\"},{\"_key\":\"71d97d309bfe\",\"_type\":\"block\",\"children\":[{\"_key\":\"b9f7c84ab150\",\"_type\":\"span\",\"marks\":[\"69483b92260d\",\"strong\"],\"text\":\"pass^k\"},{\"_key\":\"5e6cceb3738c\",\"_type\":\"span\",\"marks\":[],\"text\":\" measures the probability that \"},{\"_key\":\"3234762307bc\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"all k\"},{\"_key\":\"6074927b3378\",\"_type\":\"span\",\"marks\":[],\"text\":\" trials succeed. As \"},{\"_key\":\"661876f8f0d1\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"k\"},{\"_key\":\"9b2cc812f1c7\",\"_type\":\"span\",\"marks\":[],\"text\":\" increases, pass^k falls since demanding consistency across more trials is a harder bar to clear. If your agent has a 75% per-trial success rate and you run 3 trials, the probability of passing all three is (0.75)¬≥ ‚âà 42%. This metric especially matters for customer-facing agents where users expect reliable behavior every time.\"}],\"markDefs\":[{\"_key\":\"69483b92260d\",\"_type\":\"link\",\"href\":\"https://arxiv.org/abs/2406.12045\"}],\"style\":\"normal\"},{\"_key\":\"ffd566ae33c6\",\"_type\":\"image\",\"asset\":{\"_ref\":\"image-3ddac5be07a0773922ec9df06afec55922f8194a-4584x2580-png\",\"_type\":\"reference\"},\"caption\":[{\"_key\":\"d2aab759ce0c\",\"_type\":\"block\",\"children\":[{\"_key\":\"87169d88cb69\",\"_type\":\"span\",\"marks\":[],\"text\":\"pass@k and pass^k diverge as trials increase. At k=1, they're identical (both equal the per-trial success rate). By k=10, they tell opposite stories: pass@k approaches 100% while pass^k falls to 0%.\"}],\"markDefs\":[],\"style\":\"normal\"}],\"columnWidth\":\"inline-width\",\"height\":2580,\"markDefs\":null,\"url\":\"https://cdn.sanity.io/images/4zrzovbb/website/3ddac5be07a0773922ec9df06afec55922f8194a-4584x2580.png\",\"width\":4584},{\"_key\":\"3295990bdacf\",\"_type\":\"block\",\"children\":[{\"_key\":\"e3b0176b8899\",\"_type\":\"span\",\"marks\":[],\"text\":\"Both metrics are useful, and which to use depends on product requirements: pass@k for tools where one success matters, pass^k for agents where consistency is essential.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"e94be9295048\",\"_type\":\"block\",\"children\":[{\"_key\":\"5a26c28ca243\",\"_type\":\"span\",\"marks\":[],\"text\":\"Going from zero to one: a roadmap to great evals for agents\"}],\"markDefs\":[],\"style\":\"h3\"},{\"_key\":\"54988c136bd0\",\"_type\":\"block\",\"children\":[{\"_key\":\"cd42015d1ce1\",\"_type\":\"span\",\"marks\":[],\"text\":\"This section lays out our practical, field-tested advice for going from no evals to evals you can trust. Think of this as a roadmap for eval-driven agent development: define success early, measure it clearly, and iterate continuously.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"cd9699810616\",\"_type\":\"block\",\"children\":[{\"_key\":\"2ad7a24f9211\",\"_type\":\"span\",\"marks\":[],\"text\":\"Collect tasks for the initial eval dataset\"}],\"markDefs\":[],\"style\":\"h4\"},{\"_key\":\"8dcb14499285\",\"_type\":\"block\",\"children\":[{\"_key\":\"2d21f504a9d1\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Step 0. Start early\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"7d8fd0cd3501\",\"_type\":\"block\",\"children\":[{\"_key\":\"e515f93c7f92\",\"_type\":\"span\",\"marks\":[],\"text\":\"We see teams delay building evals because they think they need hundreds of tasks. In reality, 20-50 simple tasks drawn from real failures is a great start. After all, in early agent development, each change to the system often has a clear, noticeable impact, and this large effect size means small sample sizes suffice. More mature agents may need larger, more difficult evals to detect smaller effects, but it‚Äôs best to take the 80/20 approach in the beginning. Evals get harder to build the longer you wait. Early on, product requirements naturally translate into test cases. Wait too long and you're reverse-engineering success criteria from a live system.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"7761baeecef7\",\"_type\":\"block\",\"children\":[{\"_key\":\"d39a52fba98a\",\"_type\":\"span\",\"marks\":[],\"text\":\"\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"2541fdb72d07\",\"_type\":\"block\",\"children\":[{\"_key\":\"ff1eb938c3b2\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Step 1. Start with what you already test manually\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"cb6d2213d9fb\",\"_type\":\"block\",\"children\":[{\"_key\":\"3453430953df\",\"_type\":\"span\",\"marks\":[],\"text\":\"Begin with the manual checks you run during development‚Äîthe behaviors you verify before each release and common tasks end users try. If you're already in production, look at your bug tracker and support queue. Converting user-reported failures into test cases ensures your suite reflects actual usage; prioritizing by user impact helps you invest effort where it counts.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"2fe1878c8eb6\",\"_type\":\"block\",\"children\":[{\"_key\":\"186e4a9686e8\",\"_type\":\"span\",\"marks\":[],\"text\":\"\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"a357da327887\",\"_type\":\"block\",\"children\":[{\"_key\":\"55c3a36b6db0\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Step 2: Write unambiguous tasks with reference solutions\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"865d3604c2bb\",\"_type\":\"block\",\"children\":[{\"_key\":\"b766238a5327\",\"_type\":\"span\",\"marks\":[],\"text\":\"Getting task quality right is harder than it seems. A good task is one where two domain experts would independently reach the same pass/fail verdict. Could they pass the task themselves? If not, the task needs refinement. Ambiguity in task specifications becomes noise in metrics. The same applies to criteria for model-based graders: vague rubrics produce inconsistent judgments.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"2205d8079f3b\",\"_type\":\"block\",\"children\":[{\"_key\":\"97e43ae8d5b2\",\"_type\":\"span\",\"marks\":[],\"text\":\"Each task should be passable by an agent that follows instructions correctly. This can be subtle. For instance, auditing Terminal-Bench revealed that if a task asks the agent to write a script but doesn‚Äôt specify a filepath, and the tests assume a particular filepath for the script, the agent might fail through no fault of its own. Everything the grader checks should be clear from the task description; agents shouldn‚Äôt fail due to ambiguous specs. With frontier models, a 0% pass rate across many trials (i.e. 0% pass@100) is most often a signal of a broken task, not an incapable agent, and a sign to double-check your task specification and graders. For each task, it‚Äôs useful to create a reference solution: a known working output that passes all graders. This proves that the task is solvable and verifies graders are correctly configured.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"6cead64fba15\",\"_type\":\"block\",\"children\":[{\"_key\":\"3c6f8560c0f2\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"afb5cd8819a9\",\"_type\":\"block\",\"children\":[{\"_key\":\"2334331b01a1\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Step 3: Build balanced problem sets\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"824314221352\",\"_type\":\"block\",\"children\":[{\"_key\":\"fcf4942236b0\",\"_type\":\"span\",\"marks\":[],\"text\":\"Test both the cases where a behavior \"},{\"_key\":\"4682483788c9\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"should\"},{\"_key\":\"907f9c5a5be4\",\"_type\":\"span\",\"marks\":[],\"text\":\" occur and where it \"},{\"_key\":\"3870379831da\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"shouldn't\"},{\"_key\":\"d116ad562a25\",\"_type\":\"span\",\"marks\":[],\"text\":\". One-sided evals create one-sided optimization. For instance, if you only test whether the agent searches when it should, you might end up with an agent that searches for almost everything. Try to avoid \"},{\"_key\":\"011bfd8bb8eb\",\"_type\":\"span\",\"marks\":[\"6ae2b2a0de90\"],\"text\":\"class-imbalanced\"},{\"_key\":\"48e62f65324b\",\"_type\":\"span\",\"marks\":[],\"text\":\" evals. We learned this firsthand when building evals for web search in \"},{\"_key\":\"c8177574a1c9\",\"_type\":\"span\",\"marks\":[\"e8bee58e28ce\"],\"text\":\"Claude.ai\"},{\"_key\":\"e1b5dbb22c06\",\"_type\":\"span\",\"marks\":[],\"text\":\". The challenge was preventing the model from searching when it shouldn‚Äôt, while preserving its ability to do extensive research when appropriate. The team built evals covering both directions: queries where the model should search (like finding the weather) and queries where it should answer from existing knowledge (like ‚Äúwho founded Apple?‚Äù). Striking the right balance between undertriggering (not searching when it should) or overtriggering (searching when it shouldn‚Äôt) was difficult, and took many rounds of refinements to both the prompts and the eval. As more example problems come up, we continue to add to evals to improve our coverage.\"}],\"markDefs\":[{\"_key\":\"6ae2b2a0de90\",\"_type\":\"link\",\"href\":\"https://developers.google.com/machine-learning/crash-course/overfitting/imbalanced-datasets\"},{\"_key\":\"e8bee58e28ce\",\"_type\":\"link\",\"href\":\"http://claude.ai\"}],\"style\":\"normal\"},{\"_key\":\"e132fea8e906\",\"_type\":\"block\",\"children\":[{\"_key\":\"629d3ab4f99d\",\"_type\":\"span\",\"marks\":[],\"text\":\"\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"24ab4674e6ef\",\"_type\":\"block\",\"children\":[{\"_key\":\"a161d9d1c8b8\",\"_type\":\"span\",\"marks\":[],\"text\":\"Design the eval harness and graders\"}],\"markDefs\":[],\"style\":\"h4\"},{\"_key\":\"22537bddb71f\",\"_type\":\"block\",\"children\":[{\"_key\":\"8d8fee3fc597\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Step 4: Build a robust eval harness with a stable environment\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"7224cae07dbb\",\"_type\":\"block\",\"children\":[{\"_key\":\"cfcc5a5ae701\",\"_type\":\"span\",\"marks\":[],\"text\":\"It‚Äôs essential that the agent in the eval functions roughly the same as the agent used in production, and that the environment itself doesn‚Äôt introduce further noise. Each trial should be ‚Äúisolated‚Äù by starting from a clean environment. Unnecessary shared state between runs (leftover files, cached data, resource exhaustion) can cause correlated failures due to infrastructure flakiness rather than agent performance. Shared state can also artificially inflate performance. For example, in some internal evals we observed Claude gaining an unfair advantage on some tasks by examining the git history from previous trials. If multiple distinct trials fail because of the same limitation in the environment (like limited CPU memory), these trials are not independent because they‚Äôre affected by the same factor, and the eval results become unreliable for measuring agent performance.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"a4bf6efacc82\",\"_type\":\"block\",\"children\":[{\"_key\":\"aa6fabfed26f\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Step 5: Design graders thoughtfully\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"e176273a8a50\",\"_type\":\"block\",\"children\":[{\"_key\":\"191a0c2957fb\",\"_type\":\"span\",\"marks\":[],\"text\":\"\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"32efb0099e2e\",\"_type\":\"block\",\"children\":[{\"_key\":\"66511aa4fe8c\",\"_type\":\"span\",\"marks\":[],\"text\":\"As discussed above, great eval design involves choosing the best graders for the agent and the tasks. We recommend choosing deterministic graders where possible, LLM graders where necessary or for additional flexibility, and using human graders judiciously for additional validation.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"a1a15d581a6a\",\"_type\":\"block\",\"children\":[{\"_key\":\"708f55a15016\",\"_type\":\"span\",\"marks\":[],\"text\":\"There is a common instinct to check that agents followed very specific steps like a sequence of tool calls in the right order. We‚Äôve found this approach too rigid and results in overly brittle tests, as agents regularly find valid approaches that eval designers didn‚Äôt anticipate. So as not to unnecessarily punish creativity, it‚Äôs often better to grade what the agent produced, not the path it took.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"23db941294ff\",\"_type\":\"block\",\"children\":[{\"_key\":\"741f523140be\",\"_type\":\"span\",\"marks\":[],\"text\":\"For tasks with multiple components, build in partial credit\"},{\"_key\":\"aa91df68aa82\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\".\"},{\"_key\":\"a9facd5a831e\",\"_type\":\"span\",\"marks\":[],\"text\":\" A support agent that correctly identifies the problem and verifies the customer but fails to process a refund is meaningfully better than one that fails immediately. It‚Äôs important to represent this continuum of success in results.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"04b2497d3e55\",\"_type\":\"block\",\"children\":[{\"_key\":\"45930e2520f6\",\"_type\":\"span\",\"marks\":[],\"text\":\"Model grading often takes careful iteration to validate accuracy. LLM-as-judge graders should be closely calibrated with human experts to gain confidence that there is little divergence between the human grading and model grading. To avoid hallucinations, give the LLM a way out, like providing an instruction to return ‚ÄúUnknown‚Äù when it doesn‚Äôt have enough information. It can also help to create clear, structured rubrics to grade each dimension of a task, and then grade each dimension with an isolated LLM-as-judge rather than using one to grade all dimensions. Once the system is robust, it‚Äôs sufficient to use human review only occasionally.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"899d3f1d3b74\",\"_type\":\"block\",\"children\":[{\"_key\":\"438046827937\",\"_type\":\"span\",\"marks\":[],\"text\":\"Some evaluations have subtle failure modes that result in low scores even with good agent performance, as the agent fails to solve tasks due to grading bugs, agent harness constraints, or ambiguity. Even sophisticated teams can miss these issues. For example, \"},{\"_key\":\"325c7b55c2d7\",\"_type\":\"span\",\"marks\":[\"a6203f22a8a1\"],\"text\":\"Opus 4.5 initially scored 42% on CORE-Bench\"},{\"_key\":\"6dba6e1e1df3\",\"_type\":\"span\",\"marks\":[],\"text\":\", until an Anthropic researcher found multiple issues: rigid grading that penalized ‚Äú96.12‚Äù when expecting ‚Äú96.124991‚Ä¶‚Äù, ambiguous task specs, and stochastic tasks that were impossible to reproduce exactly. After fixing bugs and using a less constrained scaffold, Opus 4.5‚Äôs score jumped to 95%. Similarly, \"},{\"_key\":\"7e3281f7f7d6\",\"_type\":\"span\",\"marks\":[\"048fb1f7ba66\"],\"text\":\"METR discovered\"},{\"_key\":\"1ea22e3f57ab\",\"_type\":\"span\",\"marks\":[],\"text\":\" several misconfigured tasks in their time horizon benchmark that asked agents to optimize to a stated score threshold, but the grading required exceeding that threshold. This penalized models like Claude for following the instructions, while models that ignored the stated goal received better scores. Carefully double-checking tasks and graders can help avoid these problems.\"}],\"markDefs\":[{\"_key\":\"a6203f22a8a1\",\"_type\":\"link\",\"href\":\"https://x.com/sayashk/status/1996334941832089732?s=46\u0026t=c5pEvnVdVbMkcR_rcCHplg\"},{\"_key\":\"048fb1f7ba66\",\"_type\":\"link\",\"href\":\"https://x.com/metr_evals/status/2001473506442375645?s=46\"}],\"style\":\"normal\"},{\"_key\":\"5205b452eb1c\",\"_type\":\"block\",\"children\":[{\"_key\":\"33443d7d13f0\",\"_type\":\"span\",\"marks\":[],\"text\":\"Make your graders resistant to bypasses or hacks. The agent shouldn‚Äôt be able to easily ‚Äúcheat‚Äù the eval. Tasks and graders should be designed so that passing genuinely requires solving the problem rather than exploiting unintended loopholes.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"fc510f84f707\",\"_type\":\"block\",\"children\":[{\"_key\":\"49066a5e6e9d\",\"_type\":\"span\",\"marks\":[],\"text\":\"Maintain and use the eval long-term\"}],\"markDefs\":[],\"style\":\"h4\"},{\"_key\":\"230abd7df290\",\"_type\":\"block\",\"children\":[{\"_key\":\"b674ca6b8e03\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Step 6: Check the transcripts\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"f2973da96ef2\",\"_type\":\"block\",\"children\":[{\"_key\":\"007b7b6a2f53\",\"_type\":\"span\",\"marks\":[],\"text\":\"\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"3a403e77fe0f\",\"_type\":\"block\",\"children\":[{\"_key\":\"c7cedf73c96c\",\"_type\":\"span\",\"marks\":[],\"text\":\"You won't know if your graders are working well unless you read the transcripts and grades from many trials. At Anthropic, we invested in tooling for viewing eval transcripts and we regularly take the time to read them. When a task fails, the transcript tells you whether the agent made a genuine mistake or whether your graders rejected a valid solution. It also often surfaces key details about agent and eval behavior.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"c0c394a1ab99\",\"_type\":\"block\",\"children\":[{\"_key\":\"42533d3a869d\",\"_type\":\"span\",\"marks\":[],\"text\":\"\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"39d07e9b079d\",\"_type\":\"block\",\"children\":[{\"_key\":\"1a9d855c1e79\",\"_type\":\"span\",\"marks\":[],\"text\":\"Failures should seem fair: it‚Äôs clear what the agent got wrong and why. When scores don‚Äôt climb, we need confidence that it‚Äôs due to agent performance and not the eval. Reading transcripts is how you verify that your eval is measuring what actually matters, and is a critical skill for agent development.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"730b0e5f394d\",\"_type\":\"block\",\"children\":[{\"_key\":\"325b84f93e74\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Step 7: Monitor for capability eval saturation\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"ef3a91372716\",\"_type\":\"block\",\"children\":[{\"_key\":\"d6cc24afc42b\",\"_type\":\"span\",\"marks\":[],\"text\":\"An eval at 100% tracks regressions but provides no signal for improvement. \"},{\"_key\":\"fb7989f77c6a\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Eval saturation \"},{\"_key\":\"345f3eb6822d\",\"_type\":\"span\",\"marks\":[],\"text\":\"occurs when an agent passes all of the solvable tasks, leaving no room for improvement. For instance, SWE-Bench Verified scores started at 30% this year, and frontier models are now nearing saturation at \u003e80%. As evals approach saturation, progress will also slow, as only the most difficult tasks remain. This can make results deceptive, as large capability improvements appear as small increases in scores. For example, the code review startup \"},{\"_key\":\"44af13cb9790\",\"_type\":\"span\",\"marks\":[\"29a265235081\"],\"text\":\"Qodo\"},{\"_key\":\"ed1d049c6867\",\"_type\":\"span\",\"marks\":[],\"text\":\" was initially unimpressed by Opus 4.5 because their one-shot coding evals didn‚Äôt capture the gains on longer, more complex tasks. In response, they developed a new agentic eval framework, providing a much clearer picture of progress.\"}],\"markDefs\":[{\"_key\":\"29a265235081\",\"_type\":\"link\",\"href\":\"https://www.qodo.ai/\"}],\"style\":\"normal\"},{\"_key\":\"f29e20376776\",\"_type\":\"block\",\"children\":[{\"_key\":\"999fb862f41c\",\"_type\":\"span\",\"marks\":[],\"text\":\"As a rule, we do not take eval scores at face value until someone digs into the details of the eval and reads some transcripts. If grading is unfair, tasks are ambiguous, valid solutions are penalized, or the harness constrains the model, the eval should be revised.¬†\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"6efa79d7e5b3\",\"_type\":\"block\",\"children\":[{\"_key\":\"c4a057cc0286\",\"_type\":\"span\",\"marks\":[],\"text\":\"\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"1f06c3264070\",\"_type\":\"block\",\"children\":[{\"_key\":\"ad71ad360643\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Step 8: Keep evaluation suites healthy long-term through open contribution and maintenance\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"674d7a6b7410\",\"_type\":\"block\",\"children\":[{\"_key\":\"3f7a481876b1\",\"_type\":\"span\",\"marks\":[],\"text\":\"An eval suite is a living artifact that needs ongoing attention and clear ownership to remain useful.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"e293520658cc\",\"_type\":\"block\",\"children\":[{\"_key\":\"7a3f5a9c0966\",\"_type\":\"span\",\"marks\":[],\"text\":\"At Anthropic, we experimented with various approaches to eval maintenance. What proved most effective was establishing dedicated evals teams to own the core infrastructure, while domain experts and product teams contribute most eval tasks\"},{\"_key\":\"3a445cb8198f\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\" \"},{\"_key\":\"b49c15c43ca4\",\"_type\":\"span\",\"marks\":[],\"text\":\"and run the evaluations themselves.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"07ddbbeae960\",\"_type\":\"block\",\"children\":[{\"_key\":\"0c5ac15bf2b1\",\"_type\":\"span\",\"marks\":[],\"text\":\"For AI product teams, owning and iterating on evaluations should be as routine as maintaining unit tests. Teams can waste weeks on AI features that ‚Äúwork‚Äù in early testing but fail to meet unstated expectations that a well-designed eval would have surfaced early. Defining eval tasks is one of the best ways to stress-test whether the product requirements are concrete enough to start building.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"fd5c49516b94\",\"_type\":\"block\",\"children\":[{\"_key\":\"b7bf6315f751\",\"_type\":\"span\",\"marks\":[],\"text\":\"We recommend practicing eval-driven development: build evals to define planned capabilities before agents can fulfill them, then iterate until the agent performs well. Internally, we often build features that work ‚Äúwell enough‚Äù today but are bets on what models can do in a few months. Capability evals that start at a low pass rate make this visible. When a new model drops, running the suite quickly reveals which bets paid off.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"073b311f61a2\",\"_type\":\"block\",\"children\":[{\"_key\":\"df9c62e685d9\",\"_type\":\"span\",\"marks\":[],\"text\":\"The people closest to product requirements and users are best positioned to define success. With current model capabilities, product managers, customer success managers, or salespeople can use Claude Code to contribute an eval task as a PR‚Äîlet them! Or, even better, actively enable them.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"cbabb66245bc\",\"_type\":\"image\",\"asset\":{\"_ref\":\"image-0db40cc0e14402222a179fc6297b9c8818e97c8a-4584x2580-png\",\"_type\":\"reference\"},\"caption\":[{\"_key\":\"924e0a194c06\",\"_type\":\"block\",\"children\":[{\"_key\":\"a98a0522dab7\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"The process of creating an effective evaluation.\"}],\"markDefs\":[],\"style\":\"normal\"}],\"columnWidth\":\"inline-width\",\"height\":2580,\"markDefs\":null,\"url\":\"https://cdn.sanity.io/images/4zrzovbb/website/0db40cc0e14402222a179fc6297b9c8818e97c8a-4584x2580.png\",\"width\":4584},{\"_key\":\"a31cfad142c6\",\"_type\":\"block\",\"children\":[{\"_key\":\"704e5b50234a\",\"_type\":\"span\",\"marks\":[],\"text\":\"How evals fit with other methods for a holistic understanding of agents\"}],\"markDefs\":[],\"style\":\"h3\"},{\"_key\":\"d189514cbdb6\",\"_type\":\"block\",\"children\":[{\"_key\":\"9e56bb0ee610\",\"_type\":\"span\",\"marks\":[],\"text\":\"Automated evaluations can be run against an agent in thousands of tasks without deploying to production or affecting real users. But this is just one of many ways to understand agent performance. A complete picture includes production monitoring, user feedback, A/B testing, manual transcript review, and systematic human evaluation.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"a2e94c6cb31a\",\"_type\":\"table\",\"cta\":null,\"layout\":{\"direction\":\"horizontal\",\"fixedFirstColumn\":false},\"markDefs\":null,\"rows\":[{\"_key\":\"7cff250ae641\",\"_type\":\"tableRow\",\"cells\":[{\"_key\":\"ca90c75a993a\",\"_type\":\"tableCell\",\"content\":[{\"_key\":\"742bc4d48ee1\",\"_type\":\"block\",\"children\":[{\"_key\":\"94ab2a2ad1b6\",\"_type\":\"span\",\"marks\":[],\"text\":\"Method\"}],\"markDefs\":[],\"style\":\"normal\"}]},{\"_key\":\"f4e8f6a29035\",\"_type\":\"tableCell\",\"content\":[{\"_key\":\"1802586e72db\",\"_type\":\"block\",\"children\":[{\"_key\":\"838cb78af657\",\"_type\":\"span\",\"marks\":[],\"text\":\"Pros\"}],\"markDefs\":[],\"style\":\"normal\"}]},{\"_key\":\"97dbafd3b62c\",\"_type\":\"tableCell\",\"content\":[{\"_key\":\"7006ea910806\",\"_type\":\"block\",\"children\":[{\"_key\":\"8f9e5db1ce90\",\"_type\":\"span\",\"marks\":[],\"text\":\"Cons\"}],\"markDefs\":[],\"style\":\"normal\"}]}],\"isHeaderRow\":true},{\"_key\":\"141b91f37641\",\"_type\":\"tableRow\",\"cells\":[{\"_key\":\"778a0c714941\",\"_type\":\"tableCell\",\"content\":[{\"_key\":\"756302fc84a9\",\"_type\":\"block\",\"children\":[{\"_key\":\"e699eee748ef\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Automated evals\\n\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"cffe78231448\",\"_type\":\"block\",\"children\":[{\"_key\":\"e2da81b33659\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"Running tests programmatically without real users\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"e4424b8f3937\",\"_type\":\"block\",\"children\":[{\"_key\":\"ceb827457d49\",\"_type\":\"span\",\"marks\":[],\"text\":\"\\n\"}],\"markDefs\":[],\"style\":\"normal\"}]},{\"_key\":\"e052672cddfd\",\"_type\":\"tableCell\",\"content\":[{\"_key\":\"d0e2f701ebcf\",\"_type\":\"block\",\"children\":[{\"_key\":\"719eaa8df667\",\"_type\":\"span\",\"marks\":[],\"text\":\"Faster iteration\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"9c32b0266207\",\"_type\":\"block\",\"children\":[{\"_key\":\"c051a53942e8\",\"_type\":\"span\",\"marks\":[],\"text\":\"Fully reproducible\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"a85501cdd5d9\",\"_type\":\"block\",\"children\":[{\"_key\":\"f55303061828\",\"_type\":\"span\",\"marks\":[],\"text\":\"No user impact\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"ca11ff1aec96\",\"_type\":\"block\",\"children\":[{\"_key\":\"af676c5ada7b\",\"_type\":\"span\",\"marks\":[],\"text\":\"Can run on every commit\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"36bdc07ed32a\",\"_type\":\"block\",\"children\":[{\"_key\":\"3802a6c7187c\",\"_type\":\"span\",\"marks\":[],\"text\":\"Tests scenarios at scale without requiring a prod deployment\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"}]},{\"_key\":\"d5ecbc01057a\",\"_type\":\"tableCell\",\"content\":[{\"_key\":\"bd507a4ec162\",\"_type\":\"block\",\"children\":[{\"_key\":\"a5e6246a5f1c\",\"_type\":\"span\",\"marks\":[],\"text\":\"Requires more up-front investment to build\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"ae2cf8913fd4\",\"_type\":\"block\",\"children\":[{\"_key\":\"f5d4a7336249\",\"_type\":\"span\",\"marks\":[],\"text\":\"Requires ongoing maintenance as product and model evolves to avoid drift\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"8b42c3b1ad15\",\"_type\":\"block\",\"children\":[{\"_key\":\"52e7a8099fa7\",\"_type\":\"span\",\"marks\":[],\"text\":\"Can create false confidence if it doesn‚Äôt match real usage patterns\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"}]}],\"isHeaderRow\":false},{\"_key\":\"43629351ef6f\",\"_type\":\"tableRow\",\"cells\":[{\"_key\":\"ef47494bc7ec\",\"_type\":\"tableCell\",\"content\":[{\"_key\":\"bfcf069f3340\",\"_type\":\"block\",\"children\":[{\"_key\":\"4805b5c9aa58\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Production monitoring\\n\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"518863b0dc34\",\"_type\":\"block\",\"children\":[{\"_key\":\"71312e65a02e\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"Tracking metrics and errors in live systems\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"9f3cd137236a\",\"_type\":\"block\",\"children\":[{\"_key\":\"3a98ade60656\",\"_type\":\"span\",\"marks\":[],\"text\":\"\\n\"}],\"markDefs\":[],\"style\":\"normal\"}]},{\"_key\":\"1aabc392da9d\",\"_type\":\"tableCell\",\"content\":[{\"_key\":\"2ac035a9d5c1\",\"_type\":\"block\",\"children\":[{\"_key\":\"88535bfa8f85\",\"_type\":\"span\",\"marks\":[],\"text\":\"Reveals real user behavior at scale\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"02d5bd72dbda\",\"_type\":\"block\",\"children\":[{\"_key\":\"f0a38df9d2e9\",\"_type\":\"span\",\"marks\":[],\"text\":\"Catches issues that synthetic evals miss\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"0a1cfc4263f8\",\"_type\":\"block\",\"children\":[{\"_key\":\"dfdd615c005a\",\"_type\":\"span\",\"marks\":[],\"text\":\"Provides ground truth on how agents actually perform\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"}]},{\"_key\":\"2f258ff1bd9b\",\"_type\":\"tableCell\",\"content\":[{\"_key\":\"81e6dc1626af\",\"_type\":\"block\",\"children\":[{\"_key\":\"5fc13494a340\",\"_type\":\"span\",\"marks\":[],\"text\":\"Reactive; problems reach users before you know about them\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"6debaaa8d828\",\"_type\":\"block\",\"children\":[{\"_key\":\"99f3d3742197\",\"_type\":\"span\",\"marks\":[],\"text\":\"Signals can be noisy\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"75dc8e430ab1\",\"_type\":\"block\",\"children\":[{\"_key\":\"015abfef358f\",\"_type\":\"span\",\"marks\":[],\"text\":\"Requires investment in instrumentation\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"681afe221884\",\"_type\":\"block\",\"children\":[{\"_key\":\"c51ad08aa2b7\",\"_type\":\"span\",\"marks\":[],\"text\":\"Lacks ground truth for grading\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"}]}],\"isHeaderRow\":false},{\"_key\":\"96c0594242b2\",\"_type\":\"tableRow\",\"cells\":[{\"_key\":\"0f5bb6d6409d\",\"_type\":\"tableCell\",\"content\":[{\"_key\":\"74c73256e4e8\",\"_type\":\"block\",\"children\":[{\"_key\":\"33597ef95493\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"A/B testing\\n\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"2a6acc0a57c8\",\"_type\":\"block\",\"children\":[{\"_key\":\"f519bc4f3f27\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"Comparing variants with real user traffic\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"b250f54509b1\",\"_type\":\"block\",\"children\":[{\"_key\":\"d01666f58dbe\",\"_type\":\"span\",\"marks\":[],\"text\":\"\\n\"}],\"markDefs\":[],\"style\":\"normal\"}]},{\"_key\":\"c29a3c5641b9\",\"_type\":\"tableCell\",\"content\":[{\"_key\":\"e801cfd2989c\",\"_type\":\"block\",\"children\":[{\"_key\":\"2728db2c791a\",\"_type\":\"span\",\"marks\":[],\"text\":\"Measures actual user outcomes (retention, task completion)\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"d30d6e07b5be\",\"_type\":\"block\",\"children\":[{\"_key\":\"35e5c81fa95d\",\"_type\":\"span\",\"marks\":[],\"text\":\"Controls for confounds\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"ffbd49b3f671\",\"_type\":\"block\",\"children\":[{\"_key\":\"133249021539\",\"_type\":\"span\",\"marks\":[],\"text\":\"Scalable and systematic\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"}]},{\"_key\":\"8914cecbf19e\",\"_type\":\"tableCell\",\"content\":[{\"_key\":\"3a42d403a15b\",\"_type\":\"block\",\"children\":[{\"_key\":\"2d55f182a695\",\"_type\":\"span\",\"marks\":[],\"text\":\"Slow; days or weeks to reach significance and requires sufficient traffic\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"d88b69e00021\",\"_type\":\"block\",\"children\":[{\"_key\":\"52237597f980\",\"_type\":\"span\",\"marks\":[],\"text\":\"Only tests changes you deploy\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"eceb97fe2b7f\",\"_type\":\"block\",\"children\":[{\"_key\":\"ce144075f304\",\"_type\":\"span\",\"marks\":[],\"text\":\"Less signal on the underlying ‚Äúwhy‚Äù for changes in metrics without being able to thoroughly review the transcripts\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"}]}],\"isHeaderRow\":false},{\"_key\":\"bf60c0abeec3\",\"_type\":\"tableRow\",\"cells\":[{\"_key\":\"42fb12225363\",\"_type\":\"tableCell\",\"content\":[{\"_key\":\"9a1b9183d12a\",\"_type\":\"block\",\"children\":[{\"_key\":\"e4f50525b20c\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"User feedback\\n\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"556a9d4cf039\",\"_type\":\"block\",\"children\":[{\"_key\":\"2f4b0a43b2b4\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"Explicit signals like thumbs-down or bug reports\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"b7cf3896c160\",\"_type\":\"block\",\"children\":[{\"_key\":\"a3ebfc793f4e\",\"_type\":\"span\",\"marks\":[],\"text\":\"\\n\"}],\"markDefs\":[],\"style\":\"normal\"}]},{\"_key\":\"736cfc2dd09f\",\"_type\":\"tableCell\",\"content\":[{\"_key\":\"bd6691387ceb\",\"_type\":\"block\",\"children\":[{\"_key\":\"bd42b4d8620b\",\"_type\":\"span\",\"marks\":[],\"text\":\"Surfaces problems you didn't anticipate\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"c1e267065edc\",\"_type\":\"block\",\"children\":[{\"_key\":\"5ee5860cc40f\",\"_type\":\"span\",\"marks\":[],\"text\":\"Comes with real examples from actual human users\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"8fa61c918810\",\"_type\":\"block\",\"children\":[{\"_key\":\"746c65d74011\",\"_type\":\"span\",\"marks\":[],\"text\":\"The feedback often correlates with product goals\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"}]},{\"_key\":\"91f7462034e8\",\"_type\":\"tableCell\",\"content\":[{\"_key\":\"c03a5b6efc96\",\"_type\":\"block\",\"children\":[{\"_key\":\"47ad625b2973\",\"_type\":\"span\",\"marks\":[],\"text\":\"Sparse and self-selected\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"fc47f465f6db\",\"_type\":\"block\",\"children\":[{\"_key\":\"4af8e05c8606\",\"_type\":\"span\",\"marks\":[],\"text\":\"Skews toward severe issues\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"029ffe230afc\",\"_type\":\"block\",\"children\":[{\"_key\":\"39ddf8377b93\",\"_type\":\"span\",\"marks\":[],\"text\":\"Users rarely explain \"},{\"_key\":\"2f09fbebf620\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"why\"},{\"_key\":\"a4eec0d8faac\",\"_type\":\"span\",\"marks\":[],\"text\":\" something failed\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"a4a07bb13112\",\"_type\":\"block\",\"children\":[{\"_key\":\"e759b872ec3f\",\"_type\":\"span\",\"marks\":[],\"text\":\"Not automated\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"e0cdd855d3e3\",\"_type\":\"block\",\"children\":[{\"_key\":\"770f57ec0718\",\"_type\":\"span\",\"marks\":[],\"text\":\"Relying primarily on users to catch issues can have negative user impact\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"}]}],\"isHeaderRow\":false},{\"_key\":\"d53c058dea34\",\"_type\":\"tableRow\",\"cells\":[{\"_key\":\"8e8c89a301f9\",\"_type\":\"tableCell\",\"content\":[{\"_key\":\"bdd15a4e58a9\",\"_type\":\"block\",\"children\":[{\"_key\":\"f397721ae6ab\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Manual transcript review\\n\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"5b83448ee225\",\"_type\":\"block\",\"children\":[{\"_key\":\"9c06aa95cde2\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"Humans reading through agent conversations\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"670e22effb06\",\"_type\":\"block\",\"children\":[{\"_key\":\"667eb0d23fce\",\"_type\":\"span\",\"marks\":[],\"text\":\"\\n\"}],\"markDefs\":[],\"style\":\"normal\"}]},{\"_key\":\"bf77202a7b53\",\"_type\":\"tableCell\",\"content\":[{\"_key\":\"c9a90a4a50d7\",\"_type\":\"block\",\"children\":[{\"_key\":\"ac5732889569\",\"_type\":\"span\",\"marks\":[],\"text\":\"Builds intuition for failure modes\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"2e0eba289335\",\"_type\":\"block\",\"children\":[{\"_key\":\"9b647ee942b8\",\"_type\":\"span\",\"marks\":[],\"text\":\"Catches subtle quality issues automated checks miss\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"fc2442c0106a\",\"_type\":\"block\",\"children\":[{\"_key\":\"1038906d0dad\",\"_type\":\"span\",\"marks\":[],\"text\":\"Helps calibrate what \\\"good\\\" looks like and grasp details\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"}]},{\"_key\":\"f9864e35d1a3\",\"_type\":\"tableCell\",\"content\":[{\"_key\":\"ce857cb3f193\",\"_type\":\"block\",\"children\":[{\"_key\":\"a92a80d0f305\",\"_type\":\"span\",\"marks\":[],\"text\":\"Time-intensive\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"63088c6a47f0\",\"_type\":\"block\",\"children\":[{\"_key\":\"ba0adab2e472\",\"_type\":\"span\",\"marks\":[],\"text\":\"Doesn't scale\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"d5dd90e59427\",\"_type\":\"block\",\"children\":[{\"_key\":\"29ac040be2c3\",\"_type\":\"span\",\"marks\":[],\"text\":\"Coverage is inconsistent\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"a6b97181c2f3\",\"_type\":\"block\",\"children\":[{\"_key\":\"5a1c1389e6df\",\"_type\":\"span\",\"marks\":[],\"text\":\"Reviewer fatigue or different reviewers can affect the signal quality\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"6607d38ed976\",\"_type\":\"block\",\"children\":[{\"_key\":\"2d95820a2247\",\"_type\":\"span\",\"marks\":[],\"text\":\"Typically only gives qualitative signal rather than clear quantitative grading\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"}]}],\"isHeaderRow\":false},{\"_key\":\"937c3075040c\",\"_type\":\"tableRow\",\"cells\":[{\"_key\":\"ae53b3a7e050\",\"_type\":\"tableCell\",\"content\":[{\"_key\":\"441179cacbab\",\"_type\":\"block\",\"children\":[{\"_key\":\"4a96f36a02ff\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Systematic human studies\\n\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"fc3debff9d98\",\"_type\":\"block\",\"children\":[{\"_key\":\"a06b67dd09da\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"Structured grading of agent outputs by trained raters\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"0de3a3d021d2\",\"_type\":\"block\",\"children\":[{\"_key\":\"ed722131b416\",\"_type\":\"span\",\"marks\":[],\"text\":\"\\n\"}],\"markDefs\":[],\"style\":\"normal\"}]},{\"_key\":\"ae46d5d87b0e\",\"_type\":\"tableCell\",\"content\":[{\"_key\":\"40c6c6dc1e57\",\"_type\":\"block\",\"children\":[{\"_key\":\"5bbcaafcd0ee\",\"_type\":\"span\",\"marks\":[],\"text\":\"Gold-standard quality judgements from multiple human raters\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"3f4c2b89b833\",\"_type\":\"block\",\"children\":[{\"_key\":\"635886f95ce7\",\"_type\":\"span\",\"marks\":[],\"text\":\"Handles subjective or ambiguous tasks\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"949a7bb1777a\",\"_type\":\"block\",\"children\":[{\"_key\":\"bc3f91aae5b6\",\"_type\":\"span\",\"marks\":[],\"text\":\"Provides signal for improving model-based graders\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"}]},{\"_key\":\"a3879ee933b3\",\"_type\":\"tableCell\",\"content\":[{\"_key\":\"288161371357\",\"_type\":\"block\",\"children\":[{\"_key\":\"bc4544bedc07\",\"_type\":\"span\",\"marks\":[],\"text\":\"Relatively expensive and slow turnaround\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"46ac8e5213d0\",\"_type\":\"block\",\"children\":[{\"_key\":\"6afa049de16e\",\"_type\":\"span\",\"marks\":[],\"text\":\"Hard to run frequently\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"a4e6d466fc6e\",\"_type\":\"block\",\"children\":[{\"_key\":\"2f9cf23954e8\",\"_type\":\"span\",\"marks\":[],\"text\":\"Inter-rater disagreement requires reconciliation\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"4d8a40a2aa1c\",\"_type\":\"block\",\"children\":[{\"_key\":\"549741b66a3b\",\"_type\":\"span\",\"marks\":[],\"text\":\"Complex domains (legal, finance, healthcare) require human experts to conduct studies\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"}]}],\"isHeaderRow\":false}],\"title\":\"An overview of approaches for understanding AI agent performance\"},{\"_key\":\"0d11c1cdbbc8\",\"_type\":\"block\",\"children\":[{\"_key\":\"759528211e24\",\"_type\":\"span\",\"marks\":[],\"text\":\"These methods map to different stages of agent development. Automated evals are especially useful pre-launch and in CI/CD, running on each agent change and model upgrade as the first line of defense against quality problems. Production monitoring kicks in post-launch to detect distribution drift and unanticipated real-world failures. A/B testing validates significant changes once you have sufficient traffic. User feedback and transcript review are ongoing practices to fill the gaps: triage feedback constantly, sample transcripts to read weekly, and dig deeper as needed. Reserve systematic human studies for calibrating LLM graders or evaluating subjective outputs where human consensus serves as the reference standard.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"4c92d34e974e\",\"_type\":\"block\",\"children\":[{\"_key\":\"df7bad4a5391\",\"_type\":\"span\",\"marks\":[],\"text\":\"\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"6ee8716c952e\",\"_type\":\"image\",\"asset\":{\"_ref\":\"image-b77b8dbb7c2e57f063fbc8a087a853d5809b74b0-4584x2580-png\",\"_type\":\"reference\"},\"caption\":[{\"_key\":\"98be42766a4a\",\"_type\":\"block\",\"children\":[{\"_key\":\"42d20cef86b2\",\"_type\":\"span\",\"marks\":[],\"text\":\"Like the \"},{\"_key\":\"2a2efddd6bd2\",\"_type\":\"span\",\"marks\":[\"700b45102d9b\"],\"text\":\"Swiss Cheese Model\"},{\"_key\":\"7ad3f0724ab0\",\"_type\":\"span\",\"marks\":[],\"text\":\" from safety engineering, no single evaluation layer catches every issue. With multiple methods combined, failures that slip through one layer are caught by another.\"}],\"markDefs\":[{\"_key\":\"700b45102d9b\",\"_type\":\"link\",\"href\":\"https://en.wikipedia.org/wiki/Swiss_cheese_model\"}],\"style\":\"normal\"}],\"columnWidth\":\"inline-width\",\"height\":2580,\"markDefs\":null,\"url\":\"https://cdn.sanity.io/images/4zrzovbb/website/b77b8dbb7c2e57f063fbc8a087a853d5809b74b0-4584x2580.png\",\"width\":4584},{\"_key\":\"7b27d9eef307\",\"_type\":\"block\",\"children\":[{\"_key\":\"82513a1f6b2b\",\"_type\":\"span\",\"marks\":[],\"text\":\"The most effective teams combine these methods: automated evals for fast iteration, production monitoring for ground truth, and periodic human review for calibration.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"00f76af3cc55\",\"_type\":\"block\",\"children\":[{\"_key\":\"d2dbfbe5041a\",\"_type\":\"span\",\"marks\":[],\"text\":\"Conclusion\"}],\"markDefs\":[],\"style\":\"h3\"},{\"_key\":\"ad285a3b1f62\",\"_type\":\"block\",\"children\":[{\"_key\":\"71c0b124ca0d\",\"_type\":\"span\",\"marks\":[],\"text\":\"Teams without evals get bogged down in reactive loops‚Äîfixing one failure, creating another, unable to distinguish real regressions from noise. Teams that invest early find the opposite: development accelerates as failures become test cases, test cases prevent regressions, and metrics replace guesswork. Evals give the whole team a clear hill to climb, turning ‚Äúthe agent feels worse‚Äù into something actionable. The value compounds, but only if you treat evals as a core component, not an afterthought.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"d50d281d03ca\",\"_type\":\"block\",\"children\":[{\"_key\":\"56f2c3262a44\",\"_type\":\"span\",\"marks\":[],\"text\":\"The patterns vary by agent type, but the fundamentals described here are constant. Start early and don‚Äôt wait for the perfect suite. Source realistic tasks from the failures you see. Define unambiguous, robust success criteria. Design graders thoughtfully and combine multiple types. Make sure the problems are hard enough for the model. Iterate on the evaluations to improve their signal-to-noise ratio. Read the transcripts!\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"b80b52b5d183\",\"_type\":\"block\",\"children\":[{\"_key\":\"b41be47e487c\",\"_type\":\"span\",\"marks\":[],\"text\":\"AI agent evaluation is still a nascent, fast-evolving field. As agents take on longer tasks, collaborate in multi-agent systems, and handle increasingly subjective work, we will need to adapt our techniques. We‚Äôll keep sharing best practices as we learn more.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"1e7adaf6ed45\",\"_type\":\"block\",\"children\":[{\"_key\":\"a9978d9d6dde\",\"_type\":\"span\",\"marks\":[],\"text\":\"\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"88158dd4cfc7\",\"_type\":\"block\",\"children\":[{\"_key\":\"753f2ab552f2\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Acknowledgements\"}],\"markDefs\":[],\"style\":\"h4\"},{\"_key\":\"1284f3253eba\",\"_type\":\"block\",\"children\":[{\"_key\":\"0f36a2a64972\",\"_type\":\"span\",\"marks\":[],\"text\":\"Written by Mikaela Grace, Jeremy Hadfield, Rodrigo Olivares, and Jiri De Jonghe. We're also grateful to David Hershey, Gian Segato, Mike Merrill, Alex Shaw, Nicholas Carlini, Ethan Dixon, Pedram Navid, Jake Eaton, Alyssa Baum, Lina Tawfik, Karen Zhou, Alexander Bricken, Sam Kennedy, Robert Ying, and others for their contributions. Special thanks to the customers and partners we have learned from through collaborating on evals, including iGent, Cognition, Bolt, Sierra, Vals.ai, Macroscope, PromptLayer, Stripe, Shopify, the Terminal Bench team, and more. This work reflects the collective efforts of several teams who helped develop the practice of evaluations at Anthropic.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"9fb5976ae4a6\",\"_type\":\"block\",\"children\":[{\"_key\":\"41ce115fd53e\",\"_type\":\"span\",\"marks\":[],\"text\":\"\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"67d08ca3aa7a\",\"_type\":\"block\",\"children\":[{\"_key\":\"0f36a2a64972\",\"_type\":\"span\",\"marks\":[],\"text\":\"Appendix: Eval frameworks\"}],\"markDefs\":[],\"style\":\"h3\"},{\"_key\":\"34f701aca1a8\",\"_type\":\"block\",\"children\":[{\"_key\":\"0f36a2a64972\",\"_type\":\"span\",\"marks\":[],\"text\":\"Several open-source and commercial frameworks can help teams implement agent evaluations without building infrastructure from scratch. The right choice depends on your agent type, existing stack, and whether you need offline evaluation, production observability, or both.\\n\\n\"},{\"_key\":\"a58d3fbea017\",\"_type\":\"span\",\"marks\":[\"0a9b7e9babc6\"],\"text\":\"Harbor\"},{\"_key\":\"8bd2ad22dc33\",\"_type\":\"span\",\"marks\":[],\"text\":\" is designed for running agents in containerized environments, with infrastructure for running trials at scale across cloud providers and a standardized format for defining tasks and graders. Popular benchmarks like Terminal-Bench 2.0 ship through the Harbor registry, making it easy to run established benchmarks along with custom eval suites.\\n\\n\"},{\"_key\":\"f6d9313cab5a\",\"_type\":\"span\",\"marks\":[\"73d6db73deb1\"],\"text\":\"Promptfoo\"},{\"_key\":\"47883866ac7b\",\"_type\":\"span\",\"marks\":[],\"text\":\" is a lightweight, flexible, and open-source framework that focuses on declarative YAML configuration for prompt testing, with assertion types ranging from string matching to LLM-as-judge rubrics. We use a version of Promptfoo for many of our product evals. \\n\\n\"},{\"_key\":\"e2b6ae85fd73\",\"_type\":\"span\",\"marks\":[\"f39e7b6dd1b5\"],\"text\":\"Braintrust\"},{\"_key\":\"e16281991177\",\"_type\":\"span\",\"marks\":[],\"text\":\" is a platform that combines offline evaluation with production observability and experiment tracking‚Äîuseful for teams that need to both iterate during development and monitor quality in production. Its `autoevals` library includes pre-built scorers for factuality, relevance, and other common dimensions. \\n\\n\"},{\"_key\":\"7088f3c92f58\",\"_type\":\"span\",\"marks\":[\"3f7bc55ff275\"],\"text\":\"LangSmith\"},{\"_key\":\"29f94505c8dc\",\"_type\":\"span\",\"marks\":[],\"text\":\" offers tracing, offline and online evaluations, and dataset management with tight integration into the LangChain ecosystem. \"},{\"_key\":\"a5affbd598df\",\"_type\":\"span\",\"marks\":[\"577162b38eba\"],\"text\":\"Langfuse\"},{\"_key\":\"cbfcb8d41a2c\",\"_type\":\"span\",\"marks\":[],\"text\":\" provides similar capabilities as a self-hosted open-source alternative for teams with data residency requirements.\\n\\nMany teams combine multiple tools, roll their own eval framework, or just use simple evaluation scripts as a starting point. We find that while frameworks can be a valuable way to accelerate progress and standardize, they‚Äôre only as good as the eval tasks you run through them. It‚Äôs often best to quickly pick a framework that fits your workflow, then invest your energy in the evals themselves by iterating on high-quality test cases and graders.\"}],\"markDefs\":[{\"_key\":\"0a9b7e9babc6\",\"_type\":\"link\",\"blank\":false,\"href\":\"https://harborframework.com/\"},{\"_key\":\"73d6db73deb1\",\"_type\":\"link\",\"blank\":false,\"href\":\"https://www.promptfoo.dev/\"},{\"_key\":\"f39e7b6dd1b5\",\"_type\":\"link\",\"blank\":false,\"href\":\"https://www.braintrust.dev/\"},{\"_key\":\"3f7bc55ff275\",\"_type\":\"link\",\"blank\":false,\"href\":\"https://docs.langchain.com/langsmith/evaluation\"},{\"_key\":\"577162b38eba\",\"_type\":\"link\",\"blank\":false,\"href\":\"https://langfuse.com/\"}],\"style\":\"normal\"}],\"cardImage\":{\"_type\":\"image\",\"asset\":{\"_ref\":\"image-590360609ccdf39715a8ec6916b52447dcb31f16-1000x1000-svg\",\"_type\":\"reference\"},\"height\":1000,\"url\":\"https://cdn.sanity.io/images/4zrzovbb/website/590360609ccdf39715a8ec6916b52447dcb31f16-1000x1000.svg\",\"width\":1000},\"hero\":{\"_type\":\"image\",\"asset\":{\"_ref\":\"image-590360609ccdf39715a8ec6916b52447dcb31f16-1000x1000-svg\",\"_type\":\"reference\"},\"caption\":null,\"height\":1000,\"url\":\"https://cdn.sanity.io/images/4zrzovbb/website/590360609ccdf39715a8ec6916b52447dcb31f16-1000x1000.svg\",\"width\":1000},\"meta\":{\"robotsIndexable\":true,\"seoDescription\":\"Demystifying evals for AI agents \",\"socialImage\":{\"_type\":\"image\",\"asset\":{\"_createdAt\":\"2025-09-17T20:40:42Z\",\"_id\":\"image-412be842c5c6bae6b4bcd515c191b0aa5015e05f-2400x1260-png\",\"_rev\":\"3iV84Er4g6DWAMP1tCdTTj\",\"_type\":\"sanity.imageAsset\",\"_updatedAt\":\"2025-09-17T20:40:42Z\",\"assetId\":\"412be842c5c6bae6b4bcd515c191b0aa5015e05f\",\"extension\":\"png\",\"metadata\":{\"_type\":\"sanity.imageMetadata\",\"blurHash\":\"MJQvq3xa~qt79Fxuj[t7afRk-;fPD%j[%M\",\"dimensions\":{\"_type\":\"sanity.imageDimensions\",\"aspectRatio\":1.9047619047619047,\"height\":1260,\"width\":2400},\"hasAlpha\":true,\"isOpaque\":true,\"lqip\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAABQklEQVR4nJWTS0/CQBSF+ytdsDfGx69wo3EjCYkYEw0LDJFXSoQWpVoVwQcttYRqS6EWSqliC/ILjmFMjCbW2sVZzZnv3nvmDvU+GSFIs6nzpSAv5XcwcYcYv5gYDbswegqMrgKrr8Gxe/DerHDAqWeTi+qThEu+jPhOFNHtLRzsx1Eq5tHT28QTCmj1NUhiHZmjJFaWFxGJLGBtdYmAZek2fIcDU0NTqKHMFghkc2Mde7sxFOg0OqpMPL9lSvlVGpgqhPtrVK84cJUiTk+OccGzEBpVUuzfI8+mDjHrWgtnHEM6nMMYJoscnQDH52E+K+FfuaPKYEs0ctkU8rkUMtkEkukY2MohTLPtu0KUH9C2dLQe7iA0amgKdYhiFYJ4DuXxBq+OEb7D+dju2CKa792nBvBcyze/P4HfM/1RKOCnfACKHhlgx3G3iAAAAABJRU5ErkJggg==\",\"palette\":{\"_type\":\"sanity.imagePalette\",\"darkMuted\":{\"_type\":\"sanity.imagePaletteSwatch\",\"background\":\"#45423e\",\"foreground\":\"#fff\",\"population\":0.19,\"title\":\"#fff\"},\"darkVibrant\":{\"_type\":\"sanity.imagePaletteSwatch\",\"background\":\"#5d750e\",\"foreground\":\"#fff\",\"population\":0,\"title\":\"#fff\"},\"dominant\":{\"_type\":\"sanity.imagePaletteSwatch\",\"background\":\"#45423e\",\"foreground\":\"#fff\",\"population\":0.19,\"title\":\"#fff\"},\"lightMuted\":{\"_type\":\"sanity.imagePaletteSwatch\",\"background\":\"#bcc4a4\",\"foreground\":\"#000\",\"population\":0,\"title\":\"#fff\"},\"lightVibrant\":{\"_type\":\"sanity.imagePaletteSwatch\",\"background\":\"#f7fce7\",\"foreground\":\"#000\",\"population\":0.07,\"title\":\"#000\"},\"muted\":{\"_type\":\"sanity.imagePaletteSwatch\",\"background\":\"#748c5c\",\"foreground\":\"#fff\",\"population\":0,\"title\":\"#fff\"},\"vibrant\":{\"_type\":\"sanity.imagePaletteSwatch\",\"background\":\"#b3e21c\",\"foreground\":\"#000\",\"population\":0,\"title\":\"#fff\"}}},\"mimeType\":\"image/png\",\"originalFilename\":\"eng-blog-social-1.png\",\"path\":\"images/4zrzovbb/website/412be842c5c6bae6b4bcd515c191b0aa5015e05f-2400x1260.png\",\"sha1hash\":\"412be842c5c6bae6b4bcd515c191b0aa5015e05f\",\"size\":55655,\"uploadId\":\"172H63hrqp3mHSKNpFjm6PrmQvpgFUkd\",\"url\":\"https://cdn.sanity.io/images/4zrzovbb/website/412be842c5c6bae6b4bcd515c191b0aa5015e05f-2400x1260.png\"},\"description\":\"Demystifying evals for AI agents \"}},\"publishedOn\":\"2026-01-09\",\"showSidebar\":false,\"slug\":{\"_type\":\"slug\",\"current\":\"demystifying-evals-for-ai-agents\"},\"spotIllustration\":{\"_type\":\"image\",\"asset\":{\"_ref\":\"image-b87185e4d533134bc3f9b949a874396dcfcb2e80-500x500-svg\",\"_type\":\"reference\"},\"height\":500,\"url\":\"https://cdn.sanity.io/images/4zrzovbb/website/b87185e4d533134bc3f9b949a874396dcfcb2e80-500x500.svg\",\"width\":500},\"subjects\":[{\"_key\":\"testing\",\"_type\":\"tag\",\"label\":\"Testing\",\"value\":\"testing\"}],\"summary\":\"The capabilities that make agents useful also make them difficult to evaluate. The strategies that work across deployments combine techniques to match the complexity of the systems they measure. \\n\",\"title\":\"Demystifying evals for AI agents\"},\"siteSettings\":{\"_createdAt\":\"2023-11-03T16:49:36Z\",\"_id\":\"13c6e1a1-6f38-400c-ae18-89d73b6ba991\",\"_rev\":\"yTRh7AEP4D5VL0KlacX84u\",\"_system\":{\"base\":{\"id\":\"13c6e1a1-6f38-400c-ae18-89d73b6ba991\",\"rev\":\"l72pC1z2B6rbHkO6aMcVVS\"}},\"_type\":\"siteSettings\",\"_updatedAt\":\"2026-02-10T21:50:58Z\",\"announcement\":null,\"claudeCta\":{\"desktopCtas\":null,\"mobileCtas\":[{\"title\":\"Log in to Claude\",\"url\":\"https://claude.ai/login\"},{\"title\":\"Download app\",\"url\":\"https://claude.ai/download\"}],\"sections\":[{\"category\":\"Products\",\"links\":[{\"title\":\"Claude\",\"url\":\"https://claude.com/product/overview\"},{\"title\":\"Claude Code\",\"url\":\"https://claude.com/product/claude-code\"},{\"title\":\"Claude Developer Platform\",\"url\":\"https://claude.com/platform/api\"},{\"title\":\"Pricing\",\"url\":\"https://claude.com/pricing\"},{\"title\":\"Contact sales\",\"url\":\"https://claude.com/contact-sales\"}]},{\"category\":\"Models\",\"links\":[{\"title\":\"Opus\",\"url\":\"/claude/opus\"},{\"title\":\"Sonnet\",\"url\":\"/claude/sonnet\"},{\"title\":\"Haiku\",\"url\":\"/claude/haiku\"}]},{\"category\":\"Log in\",\"links\":[{\"title\":\"Claude.ai\",\"url\":\"https://claude.ai\"},{\"title\":\"Claude Console\",\"url\":\"https://platform.claude.com/\"}]}],\"title\":\"Try Claude\",\"url\":\"https://claude.ai/\"},\"copyright\":\"¬© 2026 Anthropic PBC\",\"footerNavigation\":[{\"_key\":\"716b96b62292\",\"links\":[{\"title\":\"Claude\",\"url\":\"https://claude.com/product/overview\"},{\"title\":\"Claude Code\",\"url\":\"https://claude.com/product/claude-code\"},{\"title\":\"Cowork\",\"url\":\"https://claude.com/product/cowork\"},{\"title\":\"Claude in Chrome\",\"url\":\"https://claude.com/chrome\"},{\"title\":\"Claude in Excel\",\"url\":\"https://claude.com/claude-in-excel\"},{\"title\":\"Claude in PowerPoint\",\"url\":\"https://claude.com/claude-in-powerpoint\"},{\"title\":\"Claude in Slack\",\"url\":\"https://claude.com/claude-in-slack\"},{\"title\":\"Skills\",\"url\":\"https://www.claude.com/skills\"},{\"title\":\"Max plan\",\"url\":\"https://claude.com/pricing/max\"},{\"title\":\"Team plan\",\"url\":\"https://claude.com/pricing/team\"},{\"title\":\"Enterprise plan\",\"url\":\"https://claude.com/pricing/enterprise\"},{\"title\":\"Download app\",\"url\":\"https://claude.ai/download\"},{\"title\":\"Pricing\",\"url\":\"https://claude.com/pricing\"},{\"title\":\"Log in to Claude\",\"url\":\"https://claude.ai/\"}],\"title\":\"Products\"},{\"_key\":\"0229138ff25d\",\"links\":[{\"title\":\"Opus\",\"url\":\"/www.anthropic.com/claude/opus\"},{\"title\":\"Sonnet\",\"url\":\"/www.anthropic.com/claude/sonnet\"},{\"title\":\"Haiku\",\"url\":\"/www.anthropic.com/claude/haiku\"}],\"title\":\"Models\"},{\"_key\":\"df2df9219e3abce95d6d83387e2d9bd6\",\"links\":[{\"title\":\"AI agents\",\"url\":\"https://claude.com/solutions/agents\"},{\"title\":\"Code modernization\",\"url\":\"https://claude.com/solutions/code-modernization\"},{\"title\":\"Coding\",\"url\":\"https://claude.com/solutions/coding\"},{\"title\":\"Customer support\",\"url\":\"https://claude.com/solutions/customer-support\"},{\"title\":\"Education\",\"url\":\"https://claude.com/solutions/education\"},{\"title\":\"Financial services\",\"url\":\"https://claude.com/solutions/financial-services\"},{\"title\":\"Government\",\"url\":\"https://claude.com/solutions/government\"},{\"title\":\"Healthcare\",\"url\":\"https://claude.com/solutions/healthcare\"},{\"title\":\"Life sciences\",\"url\":\"https://claude.com/solutions/life-sciences\"},{\"title\":\"Nonprofits\",\"url\":\"https://claude.com/solutions/nonprofits\"}],\"title\":\"Solutions\"},{\"_key\":\"f286ca01fc7aaabd131f347b711a971b\",\"links\":[{\"title\":\"Overview\",\"url\":\"https://claude.com/platform/api\"},{\"title\":\"Developer docs\",\"url\":\"https://platform.claude.com/docs\"},{\"title\":\"Pricing\",\"url\":\"https://claude.com/pricing#api\"},{\"title\":\"Regional compliance\",\"url\":\"https://claude.com/regional-compliance\"},{\"title\":\"Amazon Bedrock\",\"url\":\"https://claude.com/partners/amazon-bedrock\"},{\"title\":\"Google Cloud‚Äôs Vertex AI\",\"url\":\"https://claude.com/partners/google-cloud-vertex-ai\"},{\"title\":\"Console login\",\"url\":\"https://platform.claude.com/\"}],\"title\":\"Claude Developer Platform\"},{\"_key\":\"4b255e67f68c270e0072c7564e084e24\",\"links\":[{\"title\":\"Blog\",\"url\":\"https://claude.com/blog\"},{\"title\":\"Claude partner network\",\"url\":\"https://claude.com/partners\"},{\"title\":\"Connectors\",\"url\":\"https://claude.com/connectors\"},{\"title\":\"Courses\",\"url\":\"/learn\"},{\"title\":\"Customer stories\",\"url\":\"https://claude.com/customers\"},{\"title\":\"Engineering at Anthropic\",\"url\":\"/engineering\"},{\"title\":\"Events\",\"url\":\"/events\"},{\"title\":\"Plugins\",\"url\":\"https://claude.com/plugins\"},{\"title\":\"Powered by Claude\",\"url\":\"https://claude.com/partners/powered-by-claude\"},{\"title\":\"Service partners\",\"url\":\"https://claude.com/partners/services\"},{\"title\":\"Startups program\",\"url\":\"https://claude.com/programs/startups\"},{\"title\":\"Tutorials\",\"url\":\"https://claude.com/resources/tutorials\"},{\"title\":\"Use cases\",\"url\":\"https://claude.com/resources/use-cases\"}],\"title\":\"Learn\"},{\"_key\":\"4f2729951e15b0b870897e0444f5f3e1\",\"links\":[{\"title\":\"Anthropic\",\"url\":\"/company\"},{\"title\":\"Careers\",\"url\":\"/careers\"},{\"title\":\"Economic Futures\",\"url\":\"/economic-index\"},{\"title\":\"Research\",\"url\":\"/research\"},{\"title\":\"News\",\"url\":\"/news\"},{\"title\":\"Claude‚Äôs Constitution\",\"url\":\"/constitution\"},{\"title\":\"Responsible Scaling Policy\",\"url\":\"/www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy\"},{\"title\":\"Security and compliance\",\"url\":\"https://trust.anthropic.com/\"},{\"title\":\"Transparency\",\"url\":\"/transparency\"}],\"title\":\"Company\"},{\"_key\":\"a886dd1838335844d635f2857b25d66a\",\"links\":[{\"title\":\"Availability\",\"url\":\"/www.anthropic.com/supported-countries\"},{\"title\":\"Status\",\"url\":\"https://status.anthropic.com/\"},{\"title\":\"Support center\",\"url\":\"https://support.claude.com/en/\"}],\"title\":\"Help and security\"},{\"_key\":\"3c3b033c11fa832a35d43b87d55a5364\",\"links\":[{\"title\":\"Privacy choices\",\"url\":\"#\"},{\"title\":\"Privacy policy\",\"url\":\"/www.anthropic.com/legal/privacy\"},{\"title\":\"Consumer health data privacy policy\",\"url\":\"/www.anthropic.com/legal/consumer-health-data-privacy-policy\"},{\"title\":\"Responsible disclosure policy\",\"url\":\"/www.anthropic.com/responsible-disclosure-policy\"},{\"title\":\"Terms of service: Commercial\",\"url\":\"/www.anthropic.com/legal/commercial-terms\"},{\"title\":\"Terms of service: Consumer\",\"url\":\"/www.anthropic.com/legal/consumer-terms\"},{\"title\":\"Usage policy\",\"url\":\"/www.anthropic.com/legal/aup\"}],\"title\":\"Terms and policies\"}],\"headerNavigation\":[{\"_key\":\"a340f9d4b859\",\"category\":\"Research\",\"displayType\":\"singleLink\",\"sections\":null,\"url\":\"/research\"},{\"_key\":\"a483c7dfd38a\",\"category\":\"Economic Futures\",\"displayType\":\"singleLink\",\"sections\":null,\"url\":\"/economic-futures\"},{\"_key\":\"82c471bd311d\",\"category\":\"Commitments\",\"displayType\":\"sections\",\"sections\":[{\"_key\":\"675871636e4d\",\"links\":[{\"title\":\"Claude‚Äôs Constitution\",\"url\":\"/constitution\"},{\"title\":\" Transparency\",\"url\":\"/transparency\"},{\"title\":\"Responsible Scaling Policy\",\"url\":\"/news/announcing-our-updated-responsible-scaling-policy\"}],\"title\":\"Initiatives\"},{\"_key\":\"16af50a6e2dd\",\"links\":[{\"title\":\"Security and compliance\",\"url\":\"https://trust.anthropic.com/\"}],\"title\":\"Trust center\"}]},{\"_key\":\"861a11ed9931\",\"category\":\"Learn\",\"displayType\":\"sections\",\"sections\":[{\"_key\":\"9f9f720a8793\",\"links\":[{\"title\":\"Anthropic Academy\",\"url\":\"/learn\"},{\"title\":\"Tutorials\",\"url\":\"https://claude.com/resources/tutorials\"},{\"title\":\"Use cases\",\"url\":\"https://claude.com/resources/use-cases\"},{\"title\":\"Engineering at Anthropic\",\"url\":\"/engineering\"},{\"title\":\"Developer docs\",\"url\":\"https://docs.claude.com\"}],\"title\":\"Learn\"},{\"_key\":\"6bd061c46b10\",\"links\":[{\"title\":\"About\",\"url\":\"/company\"},{\"title\":\"Careers\",\"url\":\"/careers\"},{\"title\":\"Events\",\"url\":\"/events\"}],\"title\":\"Company\"}]},{\"_key\":\"22e8d8d2923d\",\"category\":\"News\",\"displayType\":\"singleLink\",\"sections\":null,\"url\":\"/news\"}],\"internalName\":\"anthropic.com Site Settings\",\"linkedInUsername\":\"anthropicresearch\",\"meta\":{\"_createdAt\":\"2023-11-20T21:56:31Z\",\"_id\":\"0f6290ad-6d21-407d-8deb-ce02815d1383\",\"_rev\":\"NyW74GU9ZzyWgAYa8qUSlF\",\"_type\":\"metadata\",\"_updatedAt\":\"2023-11-20T23:54:09Z\",\"robotsIndexable\":true,\"seoDescription\":\"Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.\",\"seoTitle\":\"Anthropic\",\"socialImage\":{\"_type\":\"image\",\"asset\":{\"_createdAt\":\"2025-05-23T14:14:18Z\",\"_id\":\"image-c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260-jpg\",\"_rev\":\"v1N2wBpLqoO2Q3HXueYiJi\",\"_type\":\"sanity.imageAsset\",\"_updatedAt\":\"2025-05-23T14:14:18Z\",\"assetId\":\"c07f638082c569e8ce1e89ae95ee6f332a98ec08\",\"extension\":\"jpg\",\"metadata\":{\"_type\":\"sanity.imageMetadata\",\"blurHash\":\"MASPU,%M?b%Ms:-;j[j[j[fQ~qj[9FayWB\",\"dimensions\":{\"_type\":\"sanity.imageDimensions\",\"aspectRatio\":1.9047619047619047,\"height\":1260,\"width\":2400},\"hasAlpha\":false,\"isOpaque\":true,\"lqip\":\"data:image/jpeg;base64,/9j/2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj/2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj/wAARCAAKABQDASIAAhEBAxEB/8QAFwABAAMAAAAAAAAAAAAAAAAAAAQFCP/EACAQAAEEAgEFAAAAAAAAAAAAAAABAgMEBRETEhQhIjH/xAAWAQEBAQAAAAAAAAAAAAAAAAAAAQL/xAAUEQEAAAAAAAAAAAAAAAAAAAAA/9oADAMBAAIRAxEAPwDS96axCjO2r8yqvn21oiR3ci6TT8d0t395ELUGVAAB/9k=\",\"palette\":{\"_type\":\"sanity.imagePalette\",\"darkMuted\":{\"_type\":\"sanity.imagePaletteSwatch\",\"background\":\"#444440\",\"foreground\":\"#fff\",\"population\":0.05,\"title\":\"#fff\"},\"darkVibrant\":{\"_type\":\"sanity.imagePaletteSwatch\",\"background\":\"#68681c\",\"foreground\":\"#fff\",\"population\":0,\"title\":\"#fff\"},\"dominant\":{\"_type\":\"sanity.imagePaletteSwatch\",\"background\":\"#fcfcf4\",\"foreground\":\"#000\",\"population\":90.85,\"title\":\"#000\"},\"lightMuted\":{\"_type\":\"sanity.imagePaletteSwatch\",\"background\":\"#bcbcb4\",\"foreground\":\"#000\",\"population\":0.03,\"title\":\"#fff\"},\"lightVibrant\":{\"_type\":\"sanity.imagePaletteSwatch\",\"background\":\"#fcfcf4\",\"foreground\":\"#000\",\"population\":90.85,\"title\":\"#000\"},\"muted\":{\"_type\":\"sanity.imagePaletteSwatch\",\"background\":\"#7c7c74\",\"foreground\":\"#fff\",\"population\":0.02,\"title\":\"#fff\"},\"vibrant\":{\"_type\":\"sanity.imagePaletteSwatch\",\"background\":\"#c8c836\",\"foreground\":\"#000\",\"population\":0,\"title\":\"#fff\"}}},\"mimeType\":\"image/jpeg\",\"originalFilename\":\"Anthropic-OG-image.jpg\",\"path\":\"images/4zrzovbb/website/c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260.jpg\",\"sha1hash\":\"c07f638082c569e8ce1e89ae95ee6f332a98ec08\",\"size\":132598,\"uploadId\":\"pxmJEaCvYm0cHoZTfnCcZYXxrWKBhHf0\",\"url\":\"https://cdn.sanity.io/images/4zrzovbb/website/c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260.jpg\"},\"description\":\"Anthropic logo\"}},\"siteName\":\"Anthropic\",\"sitemapUrls\":[\"/\",\"/careers\",\"/company\",\"/events\",\"/events/aws-summit-dc\",\"/events/aws-summit-nyc\",\"/events/aws-summit-london\",\"/events/aws-summit-tokyo\",\"/events/claude-for-finance\",\"/events/google-cloud-next-2025\",\"/events/paris-builder-summit\",\"/events/seoul-builder-summit\",\"/learn\",\"/supported-countries\",\"/unsubscribe\"],\"twitterUsername\":\"AnthropicAI\",\"youtubeUsername\":\"anthropic-ai\"}}]\n"])</script><script nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh">self.__next_f.push([1,"f:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"2\",{\"name\":\"theme-color\",\"content\":\"#141413\"}]]\n"])</script><script nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh">self.__next_f.push([1,"15:[\"$\",\"$L17\",null,{\"children\":[null,[\"$\",\"$L18\",null,{\"isMinimalNavigation\":\"$undefined\",\"siteSettings\":{\"_createdAt\":\"2023-11-03T16:49:36Z\",\"_id\":\"13c6e1a1-6f38-400c-ae18-89d73b6ba991\",\"_rev\":\"yTRh7AEP4D5VL0KlacX84u\",\"_system\":{\"base\":{\"id\":\"13c6e1a1-6f38-400c-ae18-89d73b6ba991\",\"rev\":\"l72pC1z2B6rbHkO6aMcVVS\"}},\"_type\":\"siteSettings\",\"_updatedAt\":\"2026-02-10T21:50:58Z\",\"announcement\":null,\"claudeCta\":{\"desktopCtas\":null,\"mobileCtas\":[{\"title\":\"Log in to Claude\",\"url\":\"https://claude.ai/login\"},{\"title\":\"Download app\",\"url\":\"https://claude.ai/download\"}],\"sections\":[{\"category\":\"Products\",\"links\":[{\"title\":\"Claude\",\"url\":\"https://claude.com/product/overview\"},{\"title\":\"Claude Code\",\"url\":\"https://claude.com/product/claude-code\"},{\"title\":\"Claude Developer Platform\",\"url\":\"https://claude.com/platform/api\"},{\"title\":\"Pricing\",\"url\":\"https://claude.com/pricing\"},{\"title\":\"Contact sales\",\"url\":\"https://claude.com/contact-sales\"}]},{\"category\":\"Models\",\"links\":[{\"title\":\"Opus\",\"url\":\"/claude/opus\"},{\"title\":\"Sonnet\",\"url\":\"/claude/sonnet\"},{\"title\":\"Haiku\",\"url\":\"/claude/haiku\"}]},{\"category\":\"Log in\",\"links\":[{\"title\":\"Claude.ai\",\"url\":\"https://claude.ai\"},{\"title\":\"Claude Console\",\"url\":\"https://platform.claude.com/\"}]}],\"title\":\"Try Claude\",\"url\":\"https://claude.ai/\"},\"copyright\":\"¬© 2026 Anthropic PBC\",\"footerNavigation\":[{\"_key\":\"716b96b62292\",\"links\":[{\"title\":\"Claude\",\"url\":\"https://claude.com/product/overview\"},{\"title\":\"Claude Code\",\"url\":\"https://claude.com/product/claude-code\"},{\"title\":\"Cowork\",\"url\":\"https://claude.com/product/cowork\"},{\"title\":\"Claude in Chrome\",\"url\":\"https://claude.com/chrome\"},{\"title\":\"Claude in Excel\",\"url\":\"https://claude.com/claude-in-excel\"},{\"title\":\"Claude in PowerPoint\",\"url\":\"https://claude.com/claude-in-powerpoint\"},{\"title\":\"Claude in Slack\",\"url\":\"https://claude.com/claude-in-slack\"},{\"title\":\"Skills\",\"url\":\"https://www.claude.com/skills\"},{\"title\":\"Max plan\",\"url\":\"https://claude.com/pricing/max\"},{\"title\":\"Team plan\",\"url\":\"https://claude.com/pricing/team\"},{\"title\":\"Enterprise plan\",\"url\":\"https://claude.com/pricing/enterprise\"},{\"title\":\"Download app\",\"url\":\"https://claude.ai/download\"},{\"title\":\"Pricing\",\"url\":\"https://claude.com/pricing\"},{\"title\":\"Log in to Claude\",\"url\":\"https://claude.ai/\"}],\"title\":\"Products\"},{\"_key\":\"0229138ff25d\",\"links\":[{\"title\":\"Opus\",\"url\":\"/www.anthropic.com/claude/opus\"},{\"title\":\"Sonnet\",\"url\":\"/www.anthropic.com/claude/sonnet\"},{\"title\":\"Haiku\",\"url\":\"/www.anthropic.com/claude/haiku\"}],\"title\":\"Models\"},{\"_key\":\"df2df9219e3abce95d6d83387e2d9bd6\",\"links\":[{\"title\":\"AI agents\",\"url\":\"https://claude.com/solutions/agents\"},{\"title\":\"Code modernization\",\"url\":\"https://claude.com/solutions/code-modernization\"},{\"title\":\"Coding\",\"url\":\"https://claude.com/solutions/coding\"},{\"title\":\"Customer support\",\"url\":\"https://claude.com/solutions/customer-support\"},{\"title\":\"Education\",\"url\":\"https://claude.com/solutions/education\"},{\"title\":\"Financial services\",\"url\":\"https://claude.com/solutions/financial-services\"},{\"title\":\"Government\",\"url\":\"https://claude.com/solutions/government\"},{\"title\":\"Healthcare\",\"url\":\"https://claude.com/solutions/healthcare\"},{\"title\":\"Life sciences\",\"url\":\"https://claude.com/solutions/life-sciences\"},{\"title\":\"Nonprofits\",\"url\":\"https://claude.com/solutions/nonprofits\"}],\"title\":\"Solutions\"},{\"_key\":\"f286ca01fc7aaabd131f347b711a971b\",\"links\":[{\"title\":\"Overview\",\"url\":\"https://claude.com/platform/api\"},{\"title\":\"Developer docs\",\"url\":\"https://platform.claude.com/docs\"},{\"title\":\"Pricing\",\"url\":\"https://claude.com/pricing#api\"},{\"title\":\"Regional compliance\",\"url\":\"https://claude.com/regional-compliance\"},{\"title\":\"Amazon Bedrock\",\"url\":\"https://claude.com/partners/amazon-bedrock\"},{\"title\":\"Google Cloud‚Äôs Vertex AI\",\"url\":\"https://claude.com/partners/google-cloud-vertex-ai\"},{\"title\":\"Console login\",\"url\":\"https://platform.claude.com/\"}],\"title\":\"Claude Developer Platform\"},{\"_key\":\"4b255e67f68c270e0072c7564e084e24\",\"links\":[{\"title\":\"Blog\",\"url\":\"https://claude.com/blog\"},{\"title\":\"Claude partner network\",\"url\":\"https://claude.com/partners\"},{\"title\":\"Connectors\",\"url\":\"https://claude.com/connectors\"},{\"title\":\"Courses\",\"url\":\"/learn\"},{\"title\":\"Customer stories\",\"url\":\"https://claude.com/customers\"},{\"title\":\"Engineering at Anthropic\",\"url\":\"/engineering\"},{\"title\":\"Events\",\"url\":\"/events\"},{\"title\":\"Plugins\",\"url\":\"https://claude.com/plugins\"},{\"title\":\"Powered by Claude\",\"url\":\"https://claude.com/partners/powered-by-claude\"},{\"title\":\"Service partners\",\"url\":\"https://claude.com/partners/services\"},{\"title\":\"Startups program\",\"url\":\"https://claude.com/programs/startups\"},{\"title\":\"Tutorials\",\"url\":\"https://claude.com/resources/tutorials\"},{\"title\":\"Use cases\",\"url\":\"https://claude.com/resources/use-cases\"}],\"title\":\"Learn\"},{\"_key\":\"4f2729951e15b0b870897e0444f5f3e1\",\"links\":[{\"title\":\"Anthropic\",\"url\":\"/company\"},{\"title\":\"Careers\",\"url\":\"/careers\"},{\"title\":\"Economic Futures\",\"url\":\"/economic-index\"},{\"title\":\"Research\",\"url\":\"/research\"},{\"title\":\"News\",\"url\":\"/news\"},{\"title\":\"Claude‚Äôs Constitution\",\"url\":\"/constitution\"},{\"title\":\"Responsible Scaling Policy\",\"url\":\"/www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy\"},{\"title\":\"Security and compliance\",\"url\":\"https://trust.anthropic.com/\"},{\"title\":\"Transparency\",\"url\":\"/transparency\"}],\"title\":\"Company\"},{\"_key\":\"a886dd1838335844d635f2857b25d66a\",\"links\":[{\"title\":\"Availability\",\"url\":\"/www.anthropic.com/supported-countries\"},{\"title\":\"Status\",\"url\":\"https://status.anthropic.com/\"},{\"title\":\"Support center\",\"url\":\"https://support.claude.com/en/\"}],\"title\":\"Help and security\"},{\"_key\":\"3c3b033c11fa832a35d43b87d55a5364\",\"links\":[{\"title\":\"Privacy choices\",\"url\":\"#\"},{\"title\":\"Privacy policy\",\"url\":\"/www.anthropic.com/legal/privacy\"},{\"title\":\"Consumer health data privacy policy\",\"url\":\"/www.anthropic.com/legal/consumer-health-data-privacy-policy\"},{\"title\":\"Responsible disclosure policy\",\"url\":\"/www.anthropic.com/responsible-disclosure-policy\"},{\"title\":\"Terms of service: Commercial\",\"url\":\"/www.anthropic.com/legal/commercial-terms\"},{\"title\":\"Terms of service: Consumer\",\"url\":\"/www.anthropic.com/legal/consumer-terms\"},{\"title\":\"Usage policy\",\"url\":\"/www.anthropic.com/legal/aup\"}],\"title\":\"Terms and policies\"}],\"headerNavigation\":[{\"_key\":\"a340f9d4b859\",\"category\":\"Research\",\"displayType\":\"singleLink\",\"sections\":null,\"url\":\"/research\"},{\"_key\":\"a483c7dfd38a\",\"category\":\"Economic Futures\",\"displayType\":\"singleLink\",\"sections\":null,\"url\":\"/economic-futures\"},{\"_key\":\"82c471bd311d\",\"category\":\"Commitments\",\"displayType\":\"sections\",\"sections\":[{\"_key\":\"675871636e4d\",\"links\":[{\"title\":\"Claude‚Äôs Constitution\",\"url\":\"/constitution\"},{\"title\":\" Transparency\",\"url\":\"/transparency\"},{\"title\":\"Responsible Scaling Policy\",\"url\":\"/news/announcing-our-updated-responsible-scaling-policy\"}],\"title\":\"Initiatives\"},{\"_key\":\"16af50a6e2dd\",\"links\":[{\"title\":\"Security and compliance\",\"url\":\"https://trust.anthropic.com/\"}],\"title\":\"Trust center\"}]},{\"_key\":\"861a11ed9931\",\"category\":\"Learn\",\"displayType\":\"sections\",\"sections\":[{\"_key\":\"9f9f720a8793\",\"links\":[{\"title\":\"Anthropic Academy\",\"url\":\"/learn\"},{\"title\":\"Tutorials\",\"url\":\"https://claude.com/resources/tutorials\"},{\"title\":\"Use cases\",\"url\":\"https://claude.com/resources/use-cases\"},{\"title\":\"Engineering at Anthropic\",\"url\":\"/engineering\"},{\"title\":\"Developer docs\",\"url\":\"https://docs.claude.com\"}],\"title\":\"Learn\"},{\"_key\":\"6bd061c46b10\",\"links\":[{\"title\":\"About\",\"url\":\"/company\"},{\"title\":\"Careers\",\"url\":\"/careers\"},{\"title\":\"Events\",\"url\":\"/events\"}],\"title\":\"Company\"}]},{\"_key\":\"22e8d8d2923d\",\"category\":\"News\",\"displayType\":\"singleLink\",\"sections\":null,\"url\":\"/news\"}],\"internalName\":\"anthropic.com Site Settings\",\"linkedInUsername\":\"anthropicresearch\",\"meta\":{\"_createdAt\":\"2023-11-20T21:56:31Z\",\"_id\":\"0f6290ad-6d21-407d-8deb-ce02815d1383\",\"_rev\":\"NyW74GU9ZzyWgAYa8qUSlF\",\"_type\":\"metadata\",\"_updatedAt\":\"2023-11-20T23:54:09Z\",\"robotsIndexable\":true,\"seoDescription\":\"Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.\",\"seoTitle\":\"Anthropic\",\"socialImage\":{\"_type\":\"image\",\"asset\":{\"_createdAt\":\"2025-05-23T14:14:18Z\",\"_id\":\"image-c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260-jpg\",\"_rev\":\"v1N2wBpLqoO2Q3HXueYiJi\",\"_type\":\"sanity.imageAsset\",\"_updatedAt\":\"2025-05-23T14:14:18Z\",\"assetId\":\"c07f638082c569e8ce1e89ae95ee6f332a98ec08\",\"extension\":\"jpg\",\"metadata\":{\"_type\":\"sanity.imageMetadata\",\"blurHash\":\"MASPU,%M?b%Ms:-;j[j[j[fQ~qj[9FayWB\",\"dimensions\":{\"_type\":\"sanity.imageDimensions\",\"aspectRatio\":1.9047619047619047,\"height\":1260,\"width\":2400},\"hasAlpha\":false,\"isOpaque\":true,\"lqip\":\"data:image/jpeg;base64,/9j/2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj/2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj/wAARCAAKABQDASIAAhEBAxEB/8QAFwABAAMAAAAAAAAAAAAAAAAAAAQFCP/EACAQAAEEAgEFAAAAAAAAAAAAAAABAgMEBRETEhQhIjH/xAAWAQEBAQAAAAAAAAAAAAAAAAAAAQL/xAAUEQEAAAAAAAAAAAAAAAAAAAAA/9oADAMBAAIRAxEAPwDS96axCjO2r8yqvn21oiR3ci6TT8d0t395ELUGVAAB/9k=\",\"palette\":{\"_type\":\"sanity.imagePalette\",\"darkMuted\":{\"_type\":\"sanity.imagePaletteSwatch\",\"background\":\"#444440\",\"foreground\":\"#fff\",\"population\":0.05,\"title\":\"#fff\"},\"darkVibrant\":{\"_type\":\"sanity.imagePaletteSwatch\",\"background\":\"#68681c\",\"foreground\":\"#fff\",\"population\":0,\"title\":\"#fff\"},\"dominant\":{\"_type\":\"sanity.imagePaletteSwatch\",\"background\":\"#fcfcf4\",\"foreground\":\"#000\",\"population\":90.85,\"title\":\"#000\"},\"lightMuted\":{\"_type\":\"sanity.imagePaletteSwatch\",\"background\":\"#bcbcb4\",\"foreground\":\"#000\",\"population\":0.03,\"title\":\"#fff\"},\"lightVibrant\":{\"_type\":\"sanity.imagePaletteSwatch\",\"background\":\"#fcfcf4\",\"foreground\":\"#000\",\"population\":90.85,\"title\":\"#000\"},\"muted\":{\"_type\":\"sanity.imagePaletteSwatch\",\"background\":\"#7c7c74\",\"foreground\":\"#fff\",\"population\":0.02,\"title\":\"#fff\"},\"vibrant\":{\"_type\":\"sanity.imagePaletteSwatch\",\"background\":\"#c8c836\",\"foreground\":\"#000\",\"population\":0,\"title\":\"#fff\"}}},\"mimeType\":\"image/jpeg\",\"originalFilename\":\"Anthropic-OG-image.jpg\",\"path\":\"images/4zrzovbb/website/c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260.jpg\",\"sha1hash\":\"c07f638082c569e8ce1e89ae95ee6f332a98ec08\",\"size\":132598,\"uploadId\":\"pxmJEaCvYm0cHoZTfnCcZYXxrWKBhHf0\",\"url\":\"https://cdn.sanity.io/images/4zrzovbb/website/c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260.jpg\"},\"description\":\"Anthropic logo\"}},\"siteName\":\"Anthropic\",\"sitemapUrls\":[\"/\",\"/careers\",\"/company\",\"/events\",\"/events/aws-summit-dc\",\"/events/aws-summit-nyc\",\"/events/aws-summit-london\",\"/events/aws-summit-tokyo\",\"/events/claude-for-finance\",\"/events/google-cloud-next-2025\",\"/events/paris-builder-summit\",\"/events/seoul-builder-summit\",\"/learn\",\"/supported-countries\",\"/unsubscribe\"],\"twitterUsername\":\"AnthropicAI\",\"youtubeUsername\":\"anthropic-ai\",\"hideFooter\":true},\"page\":{\"_type\":\"page\",\"_id\":\"not-found\",\"_rev\":\"\",\"_createdAt\":\"\",\"_updatedAt\":\"\",\"title\":\"Not Found\",\"slug\":{\"_type\":\"slug\",\"current\":\"not-found\"},\"meta\":{},\"sections\":[]},\"theme\":\"$undefined\"}],\"$L19\",null]}]\n"])</script><script nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh">self.__next_f.push([1,"19:[\"$\",\"main\",null,{\"id\":\"main-content\",\"className\":\"\",\"children\":[\"$\",\"$L1a\",null,{}]}]\n"])</script><script nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh">self.__next_f.push([1,"11:[[\"$\",\"title\",\"0\",{\"children\":\"Demystifying evals for AI agents \\\\ Anthropic\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Demystifying evals for AI agents \"}],[\"$\",\"meta\",\"2\",{\"name\":\"msapplication-TileColor\",\"content\":\"141413\"}],[\"$\",\"meta\",\"3\",{\"name\":\"msapplication-config\",\"content\":\"/browserconfig.xml\"}],[\"$\",\"meta\",\"4\",{\"property\":\"og:title\",\"content\":\"Demystifying evals for AI agents\"}],[\"$\",\"meta\",\"5\",{\"property\":\"og:description\",\"content\":\"Demystifying evals for AI agents \"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:image\",\"content\":\"https://cdn.sanity.io/images/4zrzovbb/website/412be842c5c6bae6b4bcd515c191b0aa5015e05f-2400x1260.png\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:image:alt\",\"content\":\"Demystifying evals for AI agents \"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"9\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"10\",{\"name\":\"twitter:site\",\"content\":\"@AnthropicAI\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:creator\",\"content\":\"@AnthropicAI\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:title\",\"content\":\"Demystifying evals for AI agents\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:description\",\"content\":\"Demystifying evals for AI agents \"}],[\"$\",\"meta\",\"14\",{\"name\":\"twitter:image\",\"content\":\"https://cdn.sanity.io/images/4zrzovbb/website/412be842c5c6bae6b4bcd515c191b0aa5015e05f-2400x1260.png\"}],[\"$\",\"meta\",\"15\",{\"name\":\"twitter:image:alt\",\"content\":\"Demystifying evals for AI agents \"}],[\"$\",\"link\",\"16\",{\"rel\":\"shortcut icon\",\"href\":\"/favicon.ico\"}],[\"$\",\"link\",\"17\",{\"rel\":\"icon\",\"href\":\"/images/icons/favicon-32x32.png\"}],[\"$\",\"link\",\"18\",{\"rel\":\"apple-touch-icon\",\"href\":\"/images/icons/apple-touch-icon.png\"}],[\"$\",\"link\",\"19\",{\"rel\":\"apple-touch-icon\",\"href\":\"/images/icons/apple-touch-icon.png\",\"sizes\":\"180x180\"}],[\"$\",\"link\",\"20\",{\"rel\":\"mask-icon\",\"href\":\"/images/icons/safari-pinned-tab.svg\",\"color\":\"141413\"}],[\"$\",\"$L1b\",\"21\",{}]]\n"])</script><script nonce="ZDEyN2NmZTktODczOC00MDFjLTg3ZGQtYjI4NDRlNThkNTZh">self.__next_f.push([1,"d:null\n"])</script></body></html>