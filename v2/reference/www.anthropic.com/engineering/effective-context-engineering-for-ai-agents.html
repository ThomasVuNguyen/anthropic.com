<!DOCTYPE html><html lang="en" class="anthropicsans_eac0b31f-module__tjnuGq__variable anthropicserif_87b6fa7d-module__quIBbW__variable anthropicmono_fae19af3-module__c5XAsG__variable copernicus_4da799c5-module__dijTSq__variable styrenea_f8492ab1-module__HimLXW__variable styreneb_278af5c6-module__wkOAdG__variable tiempostext_4eff4b4c-module__mpviCW__variable jetbrainsmono_7d7bdbc6-module__j_XgJq__variable"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/chunks/ec368b341879b233.css" nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx" data-precedence="next"/><link rel="stylesheet" href="/_next/static/chunks/38fee8473f816a4a.css" nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx" data-precedence="next"/><link rel="stylesheet" href="/_next/static/chunks/caf680e685668b99.css" nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx" data-precedence="next"/><link rel="stylesheet" href="/_next/static/chunks/ad266d0a6bc656af.css" nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx" data-precedence="next"/><link rel="stylesheet" href="/_next/static/chunks/e2c670ea67fc2bbb.css" nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx" data-precedence="next"/><link rel="stylesheet" href="/_next/static/chunks/758311c654d998de.css" nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx" href="/_next/static/chunks/f4386f5ba7642880.js"/><script src="/_next/static/chunks/573c649abe04b34a.js" async="" nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx"></script><script src="/_next/static/chunks/d0300bffb79131f2.js" async="" nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx"></script><script src="/_next/static/chunks/08dcfc3b15383cd6.js" async="" nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx"></script><script src="/_next/static/chunks/turbopack-07052ba808d12dd9.js" async="" nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx"></script><script src="/_next/static/chunks/d96012bcfc98706a.js" async="" nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx"></script><script src="/_next/static/chunks/d80b3790a119a285.js" async="" nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx"></script><script src="/_next/static/chunks/1fb574e7be3f9a05.js" async="" nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx"></script><script src="/_next/static/chunks/9a604444e87766dd.js" async="" nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx"></script><script src="/_next/static/chunks/c1896c986be1a2e2.js" async="" nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx"></script><script src="/_next/static/chunks/f5a33d7993e253c8.js" async="" nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx"></script><script src="/_next/static/chunks/496bc8a289f448d1.js" async="" nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx"></script><script src="/_next/static/chunks/b1040bb2d2fbd1e5.js" async="" nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx"></script><script src="/_next/static/chunks/2e3229a62c65aaec.js" async="" nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx"></script><script src="/_next/static/chunks/5c1988096a7b174a.js" async="" nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx"></script><script src="/_next/static/chunks/f563a58c137d4bc2.js" async="" nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx"></script><script src="/_next/static/chunks/2fd2aa01a4bc9178.js" async="" nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx"></script><script src="/_next/static/chunks/630870b77208f43d.js" async="" nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx"></script><script src="/_next/static/chunks/010986693eb1c9c2.js" async="" nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx"></script><script src="/_next/static/chunks/dabacb64939959b3.js" async="" nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx"></script><script src="/_next/static/chunks/c0d75d4ca01ae43d.js" async="" nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx"></script><script src="/_next/static/chunks/6c680011ff6c5ba2.js" async="" nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx"></script><script src="/_next/static/chunks/81716bb24f5a6f8f.js" async="" nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx"></script><script src="/_next/static/chunks/7c80d08c36d49463.js" async="" nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx"></script><script src="/_next/static/chunks/33647e5ba6496195.js" async="" nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx"></script><script src="/_next/static/chunks/667473da0b5c11bc.js" async="" nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx"></script><script src="/_next/static/chunks/2c9eb3077aa18f16.js" async="" nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx"></script><link rel="preload" href="/_next/static/chunks/7e4146583225b449.css" as="style" nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx"/><meta name="next-size-adjust" content=""/><meta name="theme-color" content="#141413"/><script src="/_next/static/chunks/a6dad97d9634a72d.js" noModule="" nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx"></script></head><body><div hidden=""><!--$?--><template id="B:0"></template><!--/$--></div><header class="SiteHeader-module-scss-module__zKj4Ca__header" data-theme="light"><div class="SiteHeader-module-scss-module__zKj4Ca__skipLinks"><a href="#main-content" class="SiteHeader-module-scss-module__zKj4Ca__skipLink">Skip to main content</a><a href="#footer" class="SiteHeader-module-scss-module__zKj4Ca__skipLink">Skip to footer</a></div><div class="page-wrapper SiteHeader-module-scss-module__zKj4Ca__root"><a href="/" aria-label="Home"><div class="SiteHeader-module-scss-module__zKj4Ca__logoDesktop"><div class="LogoWordmark-module-scss-module__Sdgt-q__logo-wrapper"><svg class="LogoWordmark-module-scss-module__Sdgt-q__logo-static" width="570" height="64" viewBox="0 0 570 64" fill="none" xmlns="http://www.w3.org/2000/svg" aria-label="Anthropic"><path d="M139.492 12.9945H160.265V62.9392H173.525V12.9945H194.298V1.06077H139.492V12.9945Z" fill="currentColor"></path><path d="M116.066 44.3757L88.221 1.06077H73.1934V62.9392H86.011V19.6243L113.856 62.9392H128.884V1.06077H116.066V44.3757Z" fill="currentColor"></path><path d="M247.337 25.7238H218.166V1.06077H204.906V62.9392H218.166V37.6575H247.337V62.9392H260.597V1.06077H247.337V25.7238Z" fill="currentColor"></path><path d="M24.663 1.06077L0 62.9392H13.7901L18.834 49.9447H44.6365L49.6796 62.9392H63.4696L38.8066 1.06077H24.663ZM23.2946 38.453L31.7348 16.7072L40.175 38.453H23.2946Z" fill="currentColor"></path><path d="M370.475 0C352.619 0 339.978 13.2597 339.978 32.0884C339.978 50.7403 352.619 64 370.475 64C388.243 64 400.796 50.7403 400.796 32.0884C400.796 13.2597 388.243 0 370.475 0ZM370.475 51.6243C360.044 51.6243 353.68 44.1989 353.68 32.0884C353.68 19.8011 360.044 12.3757 370.475 12.3757C380.818 12.3757 387.094 19.8011 387.094 32.0884C387.094 44.1989 380.818 51.6243 370.475 51.6243Z" fill="currentColor"></path><path d="M555.845 42.1657C553.547 48.1768 548.95 51.6243 542.674 51.6243C532.243 51.6243 525.878 44.1989 525.878 32.0884C525.878 19.8011 532.243 12.3757 542.674 12.3757C548.95 12.3757 553.547 15.8232 555.845 21.8343H569.901C566.453 8.57459 556.11 0 542.674 0C524.818 0 512.177 13.2597 512.177 32.0884C512.177 50.7403 524.818 64 542.674 64C556.199 64 566.541 55.337 569.989 42.1657H555.845Z" fill="currentColor"></path><path d="M471.337 1.06077L496 62.9392H509.525L484.862 1.06077H471.337Z" fill="currentColor"></path><path d="M443.403 1.06077H413.171V62.9392H426.431V40.4862H443.403C457.459 40.4862 466.033 33.0608 466.033 20.7735C466.033 8.48619 457.459 1.06077 443.403 1.06077ZM442.784 28.5525H426.431V12.9945H442.784C449.326 12.9945 452.773 15.6464 452.773 20.7735C452.773 25.9006 449.326 28.5525 442.784 28.5525Z" fill="currentColor"></path><path d="M329.812 19.8895C329.812 8.22099 321.238 1.06077 307.182 1.06077H276.95V62.9392H290.21V38.7182H304.971L318.232 62.9392H332.906L318.223 36.8734C325.593 34.0402 329.812 28.0743 329.812 19.8895ZM290.21 12.9945H306.564C313.105 12.9945 316.552 15.3812 316.552 19.8895C316.552 24.3978 313.105 26.7845 306.564 26.7845H290.21V12.9945Z" fill="currentColor"></path></svg><div class="LogoWordmark-module-scss-module__Sdgt-q__logo-lottie"></div></div></div><svg class="Icon-module-scss-module__lqbdHG__icon SiteHeader-module-scss-module__zKj4Ca__logoMobile" width="32" height="32" viewBox="0 0 46 32"><path d="M32.73 0h-6.945L38.45 32h6.945L32.73 0ZM12.665 0 0 32h7.082l2.59-6.72h13.25l2.59 6.72h7.082L19.929 0h-7.264Zm-.702 19.337 4.334-11.246 4.334 11.246h-8.668Z" fill="currentColor"></path></svg></a><div class="SiteHeader-module-scss-module__zKj4Ca__contentWrapper"><nav class="SiteHeader-module-scss-module__zKj4Ca__nav"><ul class="SiteHeader-module-scss-module__zKj4Ca__navList"><li class="body-3 SiteHeader-module-scss-module__zKj4Ca__navItem"><a href="/research" class="SiteHeader-module-scss-module__zKj4Ca__navText">Research</a></li><li class="body-3 SiteHeader-module-scss-module__zKj4Ca__navItem"><a href="/economic-futures" class="SiteHeader-module-scss-module__zKj4Ca__navText">Economic Futures</a></li><li class="body-3 SiteHeader-module-scss-module__zKj4Ca__navItem" data-category="Commitments"><button class="SiteHeader-module-scss-module__zKj4Ca__navText" aria-haspopup="menu" aria-expanded="false" aria-controls="nav-dropdown-Commitments"><span>Commitments</span><svg class="Icon-module-scss-module__lqbdHG__icon SiteHeader-module-scss-module__zKj4Ca__caretIcon" width="12" height="6.13" viewBox="0 0 8 5"><path d="M7.3016 0.231808C7.44932 0.0678162 7.70306 0.0546398 7.86724 0.20212C8.03137 0.349888 8.04461 0.603568 7.89692 0.767766L4.29684 4.76791L4.23434 4.82417C4.16662 4.87328 4.08425 4.89995 3.99918 4.89995C3.88588 4.89989 3.77733 4.85213 3.70152 4.76791L0.10144 0.767766L0.0537825 0.702139C-0.040206 0.541753 -0.0124254 0.331356 0.131128 0.20212C0.274775 0.0728844 0.486972 0.0674593 0.636608 0.1779L0.696765 0.231808L3.99918 3.90148L7.3016 0.231808Z" fill="currentColor"></path></svg></button></li><li class="body-3 SiteHeader-module-scss-module__zKj4Ca__navItem" data-category="Learn"><button class="SiteHeader-module-scss-module__zKj4Ca__navText" aria-haspopup="menu" aria-expanded="false" aria-controls="nav-dropdown-Learn"><span>Learn</span><svg class="Icon-module-scss-module__lqbdHG__icon SiteHeader-module-scss-module__zKj4Ca__caretIcon" width="12" height="6.13" viewBox="0 0 8 5"><path d="M7.3016 0.231808C7.44932 0.0678162 7.70306 0.0546398 7.86724 0.20212C8.03137 0.349888 8.04461 0.603568 7.89692 0.767766L4.29684 4.76791L4.23434 4.82417C4.16662 4.87328 4.08425 4.89995 3.99918 4.89995C3.88588 4.89989 3.77733 4.85213 3.70152 4.76791L0.10144 0.767766L0.0537825 0.702139C-0.040206 0.541753 -0.0124254 0.331356 0.131128 0.20212C0.274775 0.0728844 0.486972 0.0674593 0.636608 0.1779L0.696765 0.231808L3.99918 3.90148L7.3016 0.231808Z" fill="currentColor"></path></svg></button></li><li class="body-3 SiteHeader-module-scss-module__zKj4Ca__navItem"><a href="/news" class="SiteHeader-module-scss-module__zKj4Ca__navText">News</a></li></ul></nav><div class="SiteHeader-module-scss-module__zKj4Ca__claudeCtaWrapper"><a href="https://claude.ai/" class="SiteHeader-module-scss-module__zKj4Ca__claudeCtaButton body-3" target="_blank" rel="noopener noreferrer">Try Claude</a><div class="SiteHeader-module-scss-module__zKj4Ca__claudeCtaDropdownTrigger"><svg class="Icon-module-scss-module__lqbdHG__icon SiteHeader-module-scss-module__zKj4Ca__claudeCtaIcon" width="12" height="6.13" viewBox="0 0 8 5"><path d="M7.3016 0.231808C7.44932 0.0678162 7.70306 0.0546398 7.86724 0.20212C8.03137 0.349888 8.04461 0.603568 7.89692 0.767766L4.29684 4.76791L4.23434 4.82417C4.16662 4.87328 4.08425 4.89995 3.99918 4.89995C3.88588 4.89989 3.77733 4.85213 3.70152 4.76791L0.10144 0.767766L0.0537825 0.702139C-0.040206 0.541753 -0.0124254 0.331356 0.131128 0.20212C0.274775 0.0728844 0.486972 0.0674593 0.636608 0.1779L0.696765 0.231808L3.99918 3.90148L7.3016 0.231808Z" fill="currentColor"></path></svg></div></div><button class="SiteHeader-module-scss-module__zKj4Ca__mobileIcon" aria-label="Navigation menu"><svg class="Icon-module-scss-module__lqbdHG__icon" width="24" height="24" viewBox="0 0 40 40"><path d="M18.75 28C19.1641 28.0002 19.5 28.3359 19.5 28.75C19.4999 29.1641 19.164 29.4998 18.75 29.5H7.91699C7.50281 29.5 7.16705 29.1642 7.16699 28.75C7.16699 28.3358 7.50278 28 7.91699 28H18.75ZM32.084 19.25C32.4979 19.2504 32.834 19.586 32.834 20C32.8339 20.4139 32.4979 20.7496 32.084 20.75H7.91699C7.50281 20.75 7.16705 20.4142 7.16699 20C7.16699 19.5858 7.50278 19.25 7.91699 19.25H32.084ZM32.084 10.5C32.4979 10.5004 32.834 10.836 32.834 11.25C32.8339 11.6639 32.4979 11.9996 32.084 12H7.91699C7.50282 12 7.16706 11.6642 7.16699 11.25C7.16699 10.8358 7.50278 10.5 7.91699 10.5H32.084Z" fill="currentColor"></path></svg></button></div></div></header><main id="main-content" class=""><section class="page-wrapper HeroEngineering-module-scss-module__j1ivRa__hero" aria-label="Engineering Article Hero"><a class="body-2 bold HeroEngineering-module-scss-module__j1ivRa__hubLink" href="/engineering">Engineering at Anthropic</a><div class="HeroEngineering-module-scss-module__j1ivRa__content"><div class="HeroEngineering-module-scss-module__j1ivRa__header"><div class="HeroEngineering-module-scss-module__j1ivRa__heroImage"><img alt="" loading="lazy" width="1000" height="1000" decoding="async" data-nimg="1" style="color:transparent" src="https://www-cdn.anthropic.com/images/4zrzovbb/website/33b03d9dd1e6aec266ef22a616aec8f5676223f6-1000x1000.svg"/></div><h1 class="headline-1">Effective context engineering for AI agents</h1></div><div class="HeroEngineering-module-scss-module__j1ivRa__metadata"><p class="body-2 HeroEngineering-module-scss-module__j1ivRa__date">Published <!-- -->Sep 29, 2025</p><p class="body-large-1 HeroEngineering-module-scss-module__j1ivRa__summary">Context is a critical but finite resource for AI agents. In this post, we explore strategies for effectively curating and managing the context that powers them.</p></div></div></section><div class="page-wrapper"><article><div class=""><div class="Body-module-scss-module__z40yvW__body" data-theme="ivory"><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">After a few years of prompt engineering being the focus of attention in applied AI, a new term has come to prominence: <strong>context engineering</strong>. Building with language models is becoming less about finding the right words and phrases for your prompts, and more about answering the broader question of “what configuration of context is most likely to generate our model’s desired behavior?&quot;</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"></p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"><strong>Context</strong> refers to the set of tokens included when sampling from a large-language model (LLM). The <strong>engineering</strong> problem at hand is optimizing the utility of those tokens against the inherent constraints of LLMs in order to consistently achieve a desired outcome. Effectively wrangling LLMs often requires <em>thinking in context </em>— in other words: considering the holistic state available to the LLM at any given time and what potential behaviors that state might yield.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"></p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">In this post, we’ll explore the emerging art of context engineering and offer a refined mental model for building steerable, effective agents.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"></p><h2 class="Body-module-scss-module__z40yvW__reading-column headline-5 post-section" id="context-engineering-vs-prompt-engineering">Context engineering vs. prompt engineering</h2><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">At Anthropic, we view context engineering as the natural progression of prompt engineering. Prompt engineering refers to methods for writing and organizing LLM instructions for optimal outcomes (see <a href="https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview">our docs</a> for an overview and useful prompt engineering strategies). <strong>Context engineering</strong> refers to the set of strategies for curating and maintaining the optimal set of tokens (information) during LLM inference, including all the other information that may land there outside of the prompts.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">In the early days of engineering with LLMs, prompting was the biggest component of AI engineering work, as the majority of use cases outside of everyday chat interactions required prompts optimized for one-shot classification or text generation tasks. As the term implies, the primary focus of prompt engineering is how to write effective prompts, particularly system prompts. However, as we move towards engineering more capable agents that operate over multiple turns of inference and longer time horizons, we need strategies for managing the entire context state (system instructions, tools, <a href="https://modelcontextprotocol.io/docs/getting-started/intro">Model Context Protocol</a> (MCP), external data, message history, etc).</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">An agent running in a loop generates more and more data that <em>could</em> be relevant for the next turn of inference, and this information must be cyclically refined. Context engineering is the <a href="https://x.com/karpathy/status/1937902205765607626?lang=en">art and science</a> of curating what will go into the limited context window from that constantly evolving universe of possible information.</p><div class="Body-module-scss-module__z40yvW__media-column Body-module-scss-module__z40yvW__inline"><figure class="ImageWithCaption-module-scss-module__Duq99q__e-imageWithCaption"><img alt="Prompt engineering vs. context engineering" loading="lazy" width="2292" height="1290" decoding="async" data-nimg="1" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Ffaa261102e46c7f090a2402a49000ffae18c5dd6-2292x1290.png&amp;w=3840&amp;q=75 1x" src="/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Ffaa261102e46c7f090a2402a49000ffae18c5dd6-2292x1290.png&amp;w=3840&amp;q=75"/><figcaption class="caption"><em>In contrast to the discrete task of writing a prompt, context engineering is iterative and the curation phase happens each time we decide what to pass to the model.</em></figcaption></figure></div><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"></p><h2 class="Body-module-scss-module__z40yvW__reading-column headline-5 post-section" id="why-context-engineering-is-important-to-building-capable-agents">Why context engineering is important to building capable agents</h2><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Despite their speed and ability to manage larger and larger volumes of data, we’ve observed that LLMs, like humans, lose focus or experience confusion at a certain point. Studies on needle-in-a-haystack<em> </em>style benchmarking have uncovered the concept of <a href="https://research.trychroma.com/context-rot">context rot</a>: as the number of tokens in the context window increases, the model’s ability to accurately recall information from that context decreases.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">While some models exhibit more gentle degradation than others, this characteristic emerges across all models. Context, therefore, must be treated as a finite resource with diminishing marginal returns. Like humans, who have <a href="https://journals.sagepub.com/doi/abs/10.1177/0963721409359277">limited working memory capacity</a>, LLMs have an “attention budget” that they draw on when parsing large volumes of context. Every new token introduced depletes this budget by some amount, increasing the need to carefully curate the tokens available to the LLM.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">This attention scarcity stems from architectural constraints of LLMs. LLMs are based on the <a href="https://arxiv.org/abs/1706.03762">transformer architecture</a>, which enables every token to <a href="https://huggingface.co/blog/Esmail-AGumaan/attention-is-all-you-need">attend to every other token</a> across the entire context. This results in n² pairwise relationships for n tokens.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">As its context length increases, a model&#x27;s ability to capture these pairwise relationships gets stretched thin, creating a natural tension between context size and attention focus. Additionally, models develop their attention patterns from training data distributions where shorter sequences are typically more common than longer ones. This means models have less experience with, and fewer specialized parameters for, context-wide dependencies.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Techniques like <a href="https://arxiv.org/pdf/2306.15595">position encoding interpolation</a> allow models to handle longer sequences by adapting them to the originally trained smaller context, though with some degradation in token position understanding. These factors create a performance gradient rather than a hard cliff: models remain highly capable at longer contexts but may show reduced precision for information retrieval and long-range reasoning compared to their performance on shorter contexts.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">These realities mean that thoughtful context engineering is essential for building capable agents.</p><h2 class="Body-module-scss-module__z40yvW__reading-column headline-5 post-section" id="the-anatomy-of-effective-context">The anatomy of effective context</h2><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Given that LLMs are constrained by a finite attention budget, <em>good</em> context engineering means finding the <em>smallest</em> <em>possible</em> set of high-signal tokens that maximize the likelihood of some desired outcome. Implementing this practice is much easier said than done, but in the following section, we outline what this guiding principle means in practice across the different components of context.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"><strong>System prompts</strong> should be extremely clear and use simple, direct language that presents ideas at the <em>right altitude </em>for the agent. The right altitude is the Goldilocks zone between two common failure modes. At one extreme, we see engineers hardcoding complex, brittle logic in their prompts to elicit exact agentic behavior. This approach creates fragility and increases maintenance complexity over time. At the other extreme, engineers sometimes provide vague, high-level guidance that fails to give the LLM concrete signals for desired outputs or falsely assumes shared context. The optimal altitude strikes a balance: specific enough to guide behavior effectively, yet flexible enough to provide the model with strong heuristics to guide behavior.</p><div class="Body-module-scss-module__z40yvW__media-column Body-module-scss-module__z40yvW__inline"><figure class="ImageWithCaption-module-scss-module__Duq99q__e-imageWithCaption"><img alt="Calibrating the system prompt in the process of context engineering." loading="lazy" width="2292" height="1288" decoding="async" data-nimg="1" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0442fe138158e84ffce92bed1624dd09f37ac46f-2292x1288.png&amp;w=3840&amp;q=75 1x" src="/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0442fe138158e84ffce92bed1624dd09f37ac46f-2292x1288.png&amp;w=3840&amp;q=75"/><figcaption class="caption"><em>At one end of the spectrum, we see brittle if-else hardcoded prompts, and at the other end we see prompts that are overly general or falsely assume shared context.</em></figcaption></figure></div><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">We recommend organizing prompts into distinct sections (like <code class="InlineCodeBlock-module-scss-module__nsPAba__code">&lt;background_information&gt;</code>, <code class="InlineCodeBlock-module-scss-module__nsPAba__code">&lt;instructions&gt;</code>, <code class="InlineCodeBlock-module-scss-module__nsPAba__code">## Tool guidance</code>, <code class="InlineCodeBlock-module-scss-module__nsPAba__code">## Output description</code>, etc) and using techniques like XML tagging or Markdown headers to delineate these sections, although the exact formatting of prompts is likely becoming less important as models become more capable.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"></p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Regardless of how you decide to structure your system prompt, you should be striving for the minimal set of information that fully outlines your expected behavior. (Note that minimal does not necessarily mean short; you still need to give the agent sufficient information up front to ensure it adheres to the desired behavior.) It’s best to start by testing a minimal prompt with the best model available to see how it performs on your task, and then add clear instructions and examples to improve performance based on failure modes found during initial testing.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"><strong>Tools</strong> allow agents to operate with their environment and pull in new, additional context as they work. Because tools define the contract between agents and their information/action space, it’s extremely important that tools promote efficiency, both by returning information that is token efficient and by encouraging efficient agent behaviors.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">In <a href="/www.anthropic.com/engineering/writing-tools-for-agents">Writing tools for AI agents – with AI agents</a>, we discussed building tools that are well understood by LLMs and have minimal overlap in functionality. Similar to the functions of a well-designed codebase, tools should be self-contained, robust to error, and extremely clear with respect to their intended use. Input parameters should similarly be descriptive, unambiguous, and play to the inherent strengths of the model.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">One of the most common failure modes we see is bloated tool sets that cover too much functionality or lead to ambiguous decision points about which tool to use. If a human engineer can’t definitively say which tool should be used in a given situation, an AI agent can’t be expected to do better. As we’ll discuss later, curating a minimal viable set of tools for the agent can also lead to more reliable maintenance and pruning of context over long interactions.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Providing examples, otherwise known as few-shot prompting, is a well known best practice that we continue to strongly advise. However, teams will often stuff a laundry list of edge cases into a prompt in an attempt to articulate every possible rule the LLM should follow for a particular task. We do not recommend this. Instead, we recommend working to curate a set of diverse, canonical examples that effectively portray the expected behavior of the agent. For an LLM, examples are the “pictures” worth a thousand words.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Our overall guidance across the different components of context (system prompts<strong>, </strong>tools<strong>, </strong>examples<strong>, </strong>message history, etc) is to be thoughtful and keep your context informative, yet tight. Now let&#x27;s dive into dynamically retrieving context at runtime.</p><h2 class="Body-module-scss-module__z40yvW__reading-column headline-5 post-section" id="context-retrieval-and-agentic-search">Context retrieval and agentic search</h2><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">In <a href="/www.anthropic.com/research/building-effective-agents">Building effective AI agents</a>, we highlighted the differences between LLM-based workflows and agents. Since we wrote that post, we’ve gravitated towards a <a href="https://simonwillison.net/2025/Sep/18/agents/">simple definition</a> for agents: LLMs autonomously using tools in a loop.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Working alongside our customers, we’ve seen the field converging on this simple paradigm. As the underlying models become more capable, the level of autonomy of agents can scale: smarter models allow agents to independently navigate nuanced problem spaces and recover from errors.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">We’re now seeing a shift in how engineers think about designing context for agents. Today, many AI-native applications employ some form of embedding-based pre-inference time retrieval to surface important context for the agent to reason over. As the field transitions to more agentic approaches, we increasingly see teams augmenting these retrieval systems with “just in time” context strategies.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Rather than pre-processing all relevant data up front, agents built with the “just in time” approach maintain lightweight identifiers (file paths, stored queries, web links, etc.) and use these references to dynamically load data into context at runtime using tools. Anthropic’s agentic coding solution <a href="/www.anthropic.com/claude-code">Claude Code</a> uses this approach to perform complex data analysis over large databases. The model can write targeted queries, store results, and leverage Bash commands like head and tail to analyze large volumes of data without ever loading the full data objects into context. This approach mirrors human cognition: we generally don’t memorize entire corpuses of information, but rather introduce external organization and indexing systems like file systems, inboxes, and bookmarks to retrieve relevant information on demand.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Beyond storage efficiency, the metadata of these references provides a mechanism to efficiently refine behavior, whether explicitly provided or intuitive. To an agent operating in a file system, the presence of a file named <code class="InlineCodeBlock-module-scss-module__nsPAba__code">test_utils.py</code> in a <code class="InlineCodeBlock-module-scss-module__nsPAba__code">tests</code> folder implies a different purpose than a file with the same name located in <code class="InlineCodeBlock-module-scss-module__nsPAba__code">src/core_logic/</code> Folder hierarchies, naming conventions, and timestamps all provide important signals that help both humans and agents understand how and when to utilize information.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Letting agents navigate and retrieve data autonomously also enables progressive disclosure—in other words, allows agents to incrementally discover relevant context through exploration. Each interaction yields context that informs the next decision: file sizes suggest complexity; naming conventions hint at purpose; timestamps can be a proxy for relevance. Agents can assemble understanding layer by layer, maintaining only what&#x27;s necessary in working memory and leveraging note-taking strategies for additional persistence. This self-managed context window keeps the agent focused on relevant subsets rather than drowning in exhaustive but potentially irrelevant information.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Of course, there&#x27;s a trade-off: runtime exploration is slower than retrieving pre-computed data. Not only that, but opinionated and thoughtful engineering is required to ensure that an LLM has the right tools and heuristics for effectively navigating its information landscape. Without proper guidance, an agent can waste context by misusing tools, chasing dead-ends, or failing to identify key information.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">In certain settings, the most effective agents might employ a hybrid strategy, retrieving some data up front for speed, and pursuing further autonomous exploration at its discretion. The decision boundary for the ‘right’ level of autonomy depends on the task. Claude Code is an agent that employs this hybrid model: <a href="http://claude.md">CLAUDE.md</a> files are naively dropped into context up front, while primitives like glob and grep allow it to navigate its environment and retrieve files just-in-time, effectively bypassing the issues of stale indexing and complex syntax trees.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">The hybrid strategy might be better suited for contexts with less dynamic content, such as legal or finance work. As model capabilities improve, agentic design will trend towards letting intelligent models act intelligently, with progressively less human curation. Given the rapid pace of progress in the field, &quot;do the simplest thing that works&quot; will likely remain our best advice for teams building agents on top of Claude.</p><h3 class="Body-module-scss-module__z40yvW__reading-column headline-6 post-subsection" id="context-engineering-for-long-horizon-tasks">Context engineering for long-horizon tasks</h3><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Long-horizon tasks require agents to maintain coherence, context, and goal-directed behavior over sequences of actions where the token count exceeds the LLM’s context window. For tasks that span tens of minutes to multiple hours of continuous work, like large codebase migrations or comprehensive research projects, agents require specialized techniques to work around the context window size limitation.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Waiting for larger context windows might seem like an obvious tactic. But it&#x27;s likely that for the foreseeable future, context windows of all sizes will be subject to context pollution and information relevance concerns—at least for situations where the strongest agent performance is desired. To enable agents to work effectively across extended time horizons, we&#x27;ve developed a few techniques that address these context pollution constraints directly: compaction, structured note-taking, and multi-agent architectures.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"><strong>Compaction</strong></p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Compaction is the practice of taking a conversation nearing the context window limit, summarizing its contents, and reinitiating a new context window with the summary. Compaction typically serves as the first lever in context engineering to drive better long-term coherence. At its core, compaction distills the contents of a context window in a high-fidelity manner, enabling the agent to continue with minimal performance degradation.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">In Claude Code, for example, we implement this by passing the message history to the model to summarize and compress the most critical details. The model preserves architectural decisions, unresolved bugs, and implementation details while discarding redundant tool outputs or messages. The agent can then continue with this compressed context plus the five most recently accessed files. Users get continuity without worrying about context window limitations.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">The art of compaction lies in the selection of what to keep versus what to discard, as overly aggressive compaction can result in the loss of subtle but critical context whose importance only becomes apparent later. For engineers implementing compaction systems, we recommend carefully tuning your prompt on  complex agent traces. Start by maximizing recall to ensure your compaction prompt captures every relevant piece of information from the trace, then iterate to improve precision by eliminating superfluous content.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">An example of low-hanging superfluous content is clearing tool calls and results – once a tool has been called deep in the message history, why would the agent need to see the raw result again? One of the safest lightest touch forms of compaction is tool result clearing, most recently launched as a <a href="/www.anthropic.com/news/context-management">feature on the Claude Developer Platform</a>.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"><strong>Structured note-taking</strong></p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Structured note-taking, or agentic memory, is a technique where the agent regularly writes notes persisted to memory outside of the context window. These notes get pulled back into the context window at later times.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">This strategy provides persistent memory with minimal overhead. Like Claude Code creating a to-do list, or your custom agent maintaining a NOTES.md file, this simple pattern allows the agent to track progress across complex tasks, maintaining critical context and dependencies that would otherwise be lost across dozens of tool calls.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"><a href="https://www.twitch.tv/claudeplayspokemon">Claude playing Pokémon</a> demonstrates how memory transforms agent capabilities in non-coding domains. The agent maintains precise tallies across thousands of game steps—tracking objectives like &quot;for the last 1,234 steps I&#x27;ve been training my Pokémon in Route 1, Pikachu has gained 8 levels toward the target of 10.&quot; Without any prompting about memory structure, it develops maps of explored regions, remembers which key achievements it has unlocked, and maintains strategic notes of combat strategies that help it learn which attacks work best against different opponents.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">After context resets, the agent reads its own notes and continues multi-hour training sequences or dungeon explorations. This coherence across summarization steps enables long-horizon strategies that would be impossible when keeping all the information in the LLM’s context window alone.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">As part of our <a href="/www.anthropic.com/effective-context-engineering-for-ai-agents">Sonnet 4.5 launch</a>, we released <a href="/www.anthropic.com/news/context-management">a memory tool</a> in public beta on the Claude Developer Platform that makes it easier to store and consult information outside the context window through a file-based system. This allows agents to build up knowledge bases over time, maintain project state across sessions, and reference previous work without keeping everything in context.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"><strong>Sub-agent architectures</strong></p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Sub-agent architectures provide another way around context limitations. Rather than one agent attempting to maintain state across an entire project, specialized sub-agents can handle focused tasks with clean context windows. The main agent coordinates with a high-level plan while subagents perform deep technical work or use tools to find relevant information. Each subagent might explore extensively, using tens of thousands of tokens or more, but returns only a condensed, distilled summary of its work (often 1,000-2,000 tokens).</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">This approach achieves a clear separation of concerns—the detailed search context remains isolated within sub-agents, while the lead agent focuses on synthesizing and analyzing the results. This pattern, discussed in <a href="/www.anthropic.com/engineering/multi-agent-research-system">How we built our multi-agent research system</a>, showed a substantial improvement over single-agent systems on complex research tasks.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">The choice between these approaches depends on task characteristics. For example:</p><ul class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"><li>Compaction maintains conversational flow for tasks requiring extensive back-and-forth;</li><li>Note-taking excels for iterative development with clear milestones;</li><li>Multi-agent architectures handle complex research and analysis where parallel exploration pays dividends.</li></ul><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Even as models continue to improve, the challenge of maintaining coherence across extended interactions will remain central to building more effective agents.</p><h2 class="Body-module-scss-module__z40yvW__reading-column headline-5 post-section" id="conclusion">Conclusion</h2><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Context engineering represents a fundamental shift in how we build with LLMs. As models become more capable, the challenge isn&#x27;t just crafting the perfect prompt—it&#x27;s thoughtfully curating what information enters the model&#x27;s limited attention budget at each step. Whether you&#x27;re implementing compaction for long-horizon tasks, designing token-efficient tools, or enabling agents to explore their environment just-in-time, the guiding principle remains the same: find the smallest set of high-signal tokens that maximize the likelihood of your desired outcome.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">The techniques we&#x27;ve outlined will continue evolving as models improve. We&#x27;re already seeing that smarter models require less prescriptive engineering, allowing agents to operate with more autonomy. But even as capabilities scale, treating context as a precious, finite resource will remain central to building reliable, effective agents.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Get started with context engineering in the Claude Developer Platform today, and access helpful tips and best practices via our <a href="https://platform.claude.com/cookbook/tool-use-memory-cookbook" target="_blank" rel="noopener noreferrer">memory and context management</a> cookbook.</p><h2 class="Body-module-scss-module__z40yvW__reading-column headline-5 post-section" id="acknowledgements">Acknowledgements</h2><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text">Written by Anthropic&#x27;s Applied AI team: Prithvi Rajasekaran, Ethan Dixon, Carly Ryan, and Jeremy Hadfield, with contributions from team members Rafi Ayub, Hannah Moran, Cal Rueb, and Connor Jennings. Special thanks to Molly Vorwerck, Stuart Ritchie, and Maggie Vo for their support.</p><p class="Body-module-scss-module__z40yvW__reading-column body-2 serif post-text"><br/></p></div></div></article><div class="NewsletterEngineering-module-scss-module__AiizZa__wrapper"><div class="NewsletterEngineering-module-scss-module__AiizZa__content"><div class="NewsletterEngineering-module-scss-module__AiizZa__textContent"><h2 class="headline-5 NewsletterEngineering-module-scss-module__AiizZa__title">Get the developer newsletter</h2><div class="NewsletterEngineering-module-scss-module__AiizZa__body"><p class="body-1 serif tight">Product updates, how-tos, community spotlights, and more. Delivered monthly to your inbox.</p></div></div><div class="NewsletterEngineering-module-scss-module__AiizZa__formContainer"><form class="NewsletterEngineering-module-scss-module__AiizZa__emailForm"><div class="NewsletterEngineering-module-scss-module__AiizZa__inputWrapper"><input type="email" placeholder="Enter your email" class="NewsletterEngineering-module-scss-module__AiizZa__emailInput" required="" name="email" value=""/><button type="submit" class="NewsletterEngineering-module-scss-module__AiizZa__submitButton"><svg class="Icon-module-scss-module__lqbdHG__icon" width="20" height="20" viewBox="0 0 21 21"><path d="M4.14585 9.87492L14.4584 9.87492L9.60419 5.04158L10.5 4.14575L16.8542 10.4999L10.5 16.8541L9.60419 15.9583L14.4584 11.1249L4.14585 11.1249L4.14585 9.87492Z" fill="#ffffff"></path></svg></button></div><p class="body-3">Please provide your email address if you’d like to receive our monthly developer newsletter. You can unsubscribe at any time.</p></form></div></div></div></div></main><footer id="footer" class="SiteFooter-module-scss-module__JdOqwq__root" role="contentinfo" aria-label="Site footer"><div class="page-wrapper SiteFooter-module-scss-module__JdOqwq__footer"><div class="SiteFooter-module-scss-module__JdOqwq__logoWrapper"><a href="/" aria-label="Return to homepage"><svg class="Icon-module-scss-module__lqbdHG__icon" width="46" height="32" viewBox="0 0 46 32"><path d="M32.73 0h-6.945L38.45 32h6.945L32.73 0ZM12.665 0 0 32h7.082l2.59-6.72h13.25l2.59 6.72h7.082L19.929 0h-7.264Zm-.702 19.337 4.334-11.246 4.334 11.246h-8.668Z" fill="#faf9f5"></path></svg></a></div><nav class="SiteFooter-module-scss-module__JdOqwq__linksWrapper" aria-label="Footer navigation"><div class="SiteFooter-module-scss-module__JdOqwq__columnSection"><div class="SiteFooter-module-scss-module__JdOqwq__listSection"><h3 class="body-2 bold">Products</h3><ul class="SiteFooter-module-scss-module__JdOqwq__list"><li><a href="https://claude.com/product/overview" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Claude</a></li><li><a href="https://claude.com/product/claude-code" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Claude Code</a></li><li><a href="https://claude.com/product/cowork" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Cowork</a></li><li><a href="https://claude.com/chrome" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Claude in Chrome</a></li><li><a href="https://claude.com/claude-in-excel" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Claude in Excel</a></li><li><a href="https://claude.com/claude-in-powerpoint" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Claude in PowerPoint</a></li><li><a href="https://claude.com/claude-in-slack" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Claude in Slack</a></li><li><a href="https://www.claude.com/skills" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Skills</a></li><li><a href="https://claude.com/pricing/max" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Max plan</a></li><li><a href="https://claude.com/pricing/team" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Team plan</a></li><li><a href="https://claude.com/pricing/enterprise" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Enterprise plan</a></li><li><a href="https://claude.ai/download" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Download app</a></li><li><a href="https://claude.com/pricing" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Pricing</a></li><li><a href="https://claude.ai/" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Log in to Claude</a></li></ul></div><div class="SiteFooter-module-scss-module__JdOqwq__listSection"><h3 class="body-2 bold">Models</h3><ul class="SiteFooter-module-scss-module__JdOqwq__list"><li><a href="/www.anthropic.com/claude/opus" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">Opus</a></li><li><a href="/www.anthropic.com/claude/sonnet" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">Sonnet</a></li><li><a href="/www.anthropic.com/claude/haiku" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">Haiku</a></li></ul></div></div><div class="SiteFooter-module-scss-module__JdOqwq__columnSection"><div class="SiteFooter-module-scss-module__JdOqwq__listSection"><h3 class="body-2 bold">Solutions</h3><ul class="SiteFooter-module-scss-module__JdOqwq__list"><li><a href="https://claude.com/solutions/agents" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">AI agents</a></li><li><a href="https://claude.com/solutions/code-modernization" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Code modernization</a></li><li><a href="https://claude.com/solutions/coding" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Coding</a></li><li><a href="https://claude.com/solutions/customer-support" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Customer support</a></li><li><a href="https://claude.com/solutions/education" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Education</a></li><li><a href="https://claude.com/solutions/financial-services" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Financial services</a></li><li><a href="https://claude.com/solutions/government" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Government</a></li><li><a href="https://claude.com/solutions/healthcare" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Healthcare</a></li><li><a href="https://claude.com/solutions/life-sciences" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Life sciences</a></li><li><a href="https://claude.com/solutions/nonprofits" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Nonprofits</a></li></ul></div><div class="SiteFooter-module-scss-module__JdOqwq__listSection"><h3 class="body-2 bold">Claude Developer Platform</h3><ul class="SiteFooter-module-scss-module__JdOqwq__list"><li><a href="https://claude.com/platform/api" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Overview</a></li><li><a href="https://platform.claude.com/docs" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Developer docs</a></li><li><a href="https://claude.com/pricing#api" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Pricing</a></li><li><a href="https://claude.com/regional-compliance" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Regional compliance</a></li><li><a href="https://claude.com/partners/amazon-bedrock" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Amazon Bedrock</a></li><li><a href="https://claude.com/partners/google-cloud-vertex-ai" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Google Cloud’s Vertex AI</a></li><li><a href="https://platform.claude.com/" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Console login</a></li></ul></div></div><div class="SiteFooter-module-scss-module__JdOqwq__columnSection"><div class="SiteFooter-module-scss-module__JdOqwq__listSection"><h3 class="body-2 bold">Learn</h3><ul class="SiteFooter-module-scss-module__JdOqwq__list"><li><a href="https://claude.com/blog" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Blog</a></li><li><a href="https://claude.com/partners" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Claude partner network</a></li><li><a href="https://claude.com/connectors" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Connectors</a></li><li><a href="/learn" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">Courses</a></li><li><a href="https://claude.com/customers" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Customer stories</a></li><li><a href="/engineering" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">Engineering at Anthropic</a></li><li><a href="/events" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">Events</a></li><li><a href="https://claude.com/plugins" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Plugins</a></li><li><a href="https://claude.com/partners/powered-by-claude" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Powered by Claude</a></li><li><a href="https://claude.com/partners/services" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Service partners</a></li><li><a href="https://claude.com/programs/startups" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Startups program</a></li><li><a href="https://claude.com/resources/tutorials" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Tutorials</a></li><li><a href="https://claude.com/resources/use-cases" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Use cases</a></li></ul></div><div class="SiteFooter-module-scss-module__JdOqwq__listSection"><h3 class="body-2 bold">Company</h3><ul class="SiteFooter-module-scss-module__JdOqwq__list"><li><a href="/company" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">Anthropic</a></li><li><a href="/careers" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">Careers</a></li><li><a href="/economic-index" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">Economic Futures</a></li><li><a href="/research" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">Research</a></li><li><a href="/news" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">News</a></li><li><a href="/constitution" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">Claude’s Constitution</a></li><li><a href="/www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">Responsible Scaling Policy</a></li><li><a href="https://trust.anthropic.com/" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">Security and compliance</a></li><li><a href="/transparency" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">Transparency</a></li></ul></div></div><div class="SiteFooter-module-scss-module__JdOqwq__columnSection"><div class="SiteFooter-module-scss-module__JdOqwq__listSection"><h3 class="body-2 bold">Help and security</h3><ul class="SiteFooter-module-scss-module__JdOqwq__list"><li><a href="/www.anthropic.com/supported-countries" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">Availability</a></li><li><a href="https://status.anthropic.com/" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">Status</a></li><li><a href="https://support.claude.com/en/" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2" target="_blank" rel="noopener noreferrer">Support center</a></li></ul></div><div class="SiteFooter-module-scss-module__JdOqwq__listSection"><h3 class="body-2 bold">Terms and policies</h3><ul class="SiteFooter-module-scss-module__JdOqwq__list"><li><a href="/www.anthropic.com/legal/privacy" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">Privacy policy</a></li><li><a href="/www.anthropic.com/legal/consumer-health-data-privacy-policy" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">Consumer health data privacy policy</a></li><li><a href="/www.anthropic.com/responsible-disclosure-policy" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">Responsible disclosure policy</a></li><li><a href="/www.anthropic.com/legal/commercial-terms" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">Terms of service: Commercial</a></li><li><a href="/www.anthropic.com/legal/consumer-terms" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">Terms of service: Consumer</a></li><li><a href="/www.anthropic.com/legal/aup" class="SiteFooter-module-scss-module__JdOqwq__listItem body-2">Usage policy</a></li></ul></div></div></nav><div class="SiteFooter-module-scss-module__JdOqwq__socialWrapper"><small class="body-2 SiteFooter-module-scss-module__JdOqwq__copyright" role="contentinfo">© 2026 Anthropic PBC</small><ul class="SiteFooter-module-scss-module__JdOqwq__socialIcons" role="navigation" aria-label="Social media links"><li><a href="https://www.linkedin.com/company/anthropicresearch" aria-label="Visit our LinkedIn page" target="_blank" rel="noopener noreferrer"><svg class="Icon-module-scss-module__lqbdHG__icon" width="24" height="24" viewBox="0 0 32 32"><path d="M25.8182 4H6.18182C4.97636 4 4 4.97636 4 6.18182V25.8182C4 27.0236 4.97636 28 6.18182 28H25.8182C27.0236 28 28 27.0236 28 25.8182V6.18182C28 4.97636 27.0236 4 25.8182 4ZM11.5862 23.6364H8.368V13.2815H11.5862V23.6364ZM9.94436 11.8011C8.90691 11.8011 8.068 10.96 8.068 9.92473C8.068 8.88945 8.908 8.04945 9.94436 8.04945C10.9785 8.04945 11.8196 8.89055 11.8196 9.92473C11.8196 10.96 10.9785 11.8011 9.94436 11.8011ZM23.6407 23.6364H20.4247V18.6007C20.4247 17.3996 20.4029 15.8549 18.7524 15.8549C17.0778 15.8549 16.8204 17.1629 16.8204 18.5135V23.6364H13.6044V13.2815H16.6916V14.6964H16.7353C17.1651 13.8825 18.2145 13.024 19.78 13.024C23.0385 13.024 23.6407 15.1687 23.6407 17.9571V23.6364Z" fill="#b0aea5"></path></svg></a></li><li><a href="https://x.com/AnthropicAI" aria-label="Visit our X (formerly Twitter) profile" target="_blank" rel="noopener noreferrer"><svg class="Icon-module-scss-module__lqbdHG__icon" width="24" height="24" viewBox="0 0 32 32"><path d="M28 28L18.6145 14.0124L18.6305 14.0255L27.0929 4H24.265L17.3713 12.16L11.8968 4H4.48021L13.2425 17.0593L13.2414 17.0582L4 28H6.82792L14.4921 18.9215L20.5834 28H28ZM10.7763 6.18182L23.9449 25.8182H21.7039L8.52468 6.18182H10.7763Z" fill="#b0aea5"></path></svg></a></li><li><a href="https://www.youtube.com/@anthropic-ai" aria-label="Visit our YouTube channel" target="_blank" rel="noopener noreferrer"><svg class="Icon-module-scss-module__lqbdHG__icon" width="24" height="24" viewBox="0 0 32 32"><path d="M29.2184 9.4375C28.9596 8.06299 27.7263 7.06201 26.2951 6.74951C24.1533 6.3125 20.1896 6 15.901 6C11.615 6 7.58782 6.3125 5.44354 6.74951C4.01486 7.06201 2.77905 7.99951 2.52021 9.4375C2.25884 11 2 13.1875 2 16C2 18.8125 2.25884 21 2.58365 22.5625C2.84502 23.937 4.0783 24.938 5.50698 25.2505C7.78068 25.6875 11.6784 26 15.967 26C20.2556 26 24.1533 25.6875 26.427 25.2505C27.8557 24.938 29.089 24.0005 29.3504 22.5625C29.6092 21 29.934 18.749 30 16C29.868 13.1875 29.5432 11 29.2184 9.4375ZM12.3941 20.375V11.625L20.319 16L12.3941 20.375Z" fill="#b0aea5"></path></svg></a></li></ul></div></div></footer><!--$?--><template id="B:1"></template><!--/$--><script nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx">requestAnimationFrame(function(){$RT=performance.now()});</script><script src="/_next/static/chunks/f4386f5ba7642880.js" nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx" id="_R_" async=""></script><title>Effective context engineering for AI agents \ Anthropic</title><meta name="description" content="Anthropic is an AI safety and research company that&#x27;s working to build reliable, interpretable, and steerable AI systems."/><meta name="msapplication-TileColor" content="141413"/><meta name="msapplication-config" content="/browserconfig.xml"/><meta property="og:title" content="Effective context engineering for AI agents"/><meta property="og:description" content="Anthropic is an AI safety and research company that&#x27;s working to build reliable, interpretable, and steerable AI systems."/><meta property="og:image" content="https://cdn.sanity.io/images/4zrzovbb/website/ea2bf01aa874d7ab776453e97dfeed5d2bf5a116-2400x1260.png"/><meta property="og:image:alt" content="In this post, we explore strategies for effectively curating and managing their context."/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:site" content="@AnthropicAI"/><meta name="twitter:creator" content="@AnthropicAI"/><meta name="twitter:title" content="Effective context engineering for AI agents"/><meta name="twitter:description" content="Anthropic is an AI safety and research company that&#x27;s working to build reliable, interpretable, and steerable AI systems."/><meta name="twitter:image" content="https://cdn.sanity.io/images/4zrzovbb/website/ea2bf01aa874d7ab776453e97dfeed5d2bf5a116-2400x1260.png"/><meta name="twitter:image:alt" content="In this post, we explore strategies for effectively curating and managing their context."/><link rel="shortcut icon" href="/favicon.ico"/><link rel="icon" href="/images/icons/favicon-32x32.png"/><link rel="apple-touch-icon" href="/images/icons/apple-touch-icon.png"/><link rel="apple-touch-icon" href="/images/icons/apple-touch-icon.png" sizes="180x180"/><link rel="mask-icon" href="/images/icons/safari-pinned-tab.svg" color="141413"/><script nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx">document.querySelectorAll('body link[rel="icon"], body link[rel="apple-touch-icon"]').forEach(el => document.head.appendChild(el))</script><div hidden id="S:0"></div><script nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx">$RB=[];$RV=function(a){$RT=performance.now();for(var b=0;b<a.length;b+=2){var c=a[b],e=a[b+1];null!==e.parentNode&&e.parentNode.removeChild(e);var f=c.parentNode;if(f){var g=c.previousSibling,h=0;do{if(c&&8===c.nodeType){var d=c.data;if("/$"===d||"/&"===d)if(0===h)break;else h--;else"$"!==d&&"$?"!==d&&"$~"!==d&&"$!"!==d&&"&"!==d||h++}d=c.nextSibling;f.removeChild(c);c=d}while(c);for(;e.firstChild;)f.insertBefore(e.firstChild,c);g.data="$";g._reactRetry&&requestAnimationFrame(g._reactRetry)}}a.length=0};
$RC=function(a,b){if(b=document.getElementById(b))(a=document.getElementById(a))?(a.previousSibling.data="$~",$RB.push(a,b),2===$RB.length&&("number"!==typeof $RT?requestAnimationFrame($RV.bind(null,$RB)):(a=performance.now(),setTimeout($RV.bind(null,$RB),2300>a&&2E3<a?2300-a:$RT+300-a)))):b.parentNode.removeChild(b)};$RC("B:0","S:0")</script><div hidden id="S:1"></div><script nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx">$RC("B:1","S:1")</script><script nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx">(self.__next_f=self.__next_f||[]).push([0])</script><script nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx">self.__next_f.push([1,"1:\"$Sreact.fragment\"\n4:I[339756,[\"/_next/static/chunks/d96012bcfc98706a.js\",\"/_next/static/chunks/d80b3790a119a285.js\"],\"default\"]\n5:I[837457,[\"/_next/static/chunks/d96012bcfc98706a.js\",\"/_next/static/chunks/d80b3790a119a285.js\"],\"default\"]\na:I[168027,[\"/_next/static/chunks/d96012bcfc98706a.js\",\"/_next/static/chunks/d80b3790a119a285.js\"],\"default\"]\nb:I[897367,[\"/_next/static/chunks/d96012bcfc98706a.js\",\"/_next/static/chunks/d80b3790a119a285.js\"],\"OutletBoundary\"]\nc:\"$Sreact.suspense\"\ne:I[897367,[\"/_next/static/chunks/d96012bcfc98706a.js\",\"/_next/static/chunks/d80b3790a119a285.js\"],\"ViewportBoundary\"]\n10:I[897367,[\"/_next/static/chunks/d96012bcfc98706a.js\",\"/_next/static/chunks/d80b3790a119a285.js\"],\"MetadataBoundary\"]\n12:I[264900,[\"/_next/static/chunks/1fb574e7be3f9a05.js\",\"/_next/static/chunks/9a604444e87766dd.js\",\"/_next/static/chunks/c1896c986be1a2e2.js\"],\"default\"]\n13:I[649551,[\"/_next/static/chunks/1fb574e7be3f9a05.js\",\"/_next/static/chunks/9a604444e87766dd.js\",\"/_next/static/chunks/c1896c986be1a2e2.js\"],\"default\"]\n14:I[96155,[\"/_next/static/chunks/1fb574e7be3f9a05.js\",\"/_next/static/chunks/9a604444e87766dd.js\",\"/_next/static/chunks/c1896c986be1a2e2.js\"],\"default\"]\n16:I[775710,[\"/_next/static/chunks/1fb574e7be3f9a05.js\",\"/_next/static/chunks/9a604444e87766dd.js\",\"/_next/static/chunks/c1896c986be1a2e2.js\",\"/_next/static/chunks/f5a33d7993e253c8.js\",\"/_next/static/chunks/496bc8a289f448d1.js\",\"/_next/static/chunks/b1040bb2d2fbd1e5.js\",\"/_next/static/chunks/2e3229a62c65aaec.js\",\"/_next/static/chunks/5c1988096a7b174a.js\",\"/_next/static/chunks/f563a58c137d4bc2.js\",\"/_next/static/chunks/2fd2aa01a4bc9178.js\",\"/_next/static/chunks/630870b77208f43d.js\",\"/_next/static/chunks/010986693eb1c9c2.js\",\"/_next/static/chunks/dabacb64939959b3.js\",\"/_next/static/chunks/c0d75d4ca01ae43d.js\",\"/_next/static/chunks/6c680011ff6c5ba2.js\",\"/_next/static/chunks/81716bb24f5a6f8f.js\"],\"default\"]\n17:I[606617,[\"/_next/static/chunks/1fb574e7be3f9a05.js\",\"/_next/static/chunks/9a604444e87766dd.js\",\"/_next/static/chunks/c1896c986be1a2e2.js\",\"/_next/static/chunks/7c80d08c36d49463.js\",\"/_next/static/chunks/496bc8a289f448d1.js\",\"/_next/static/chunks/b1040bb2d2fbd1e5.js\",\"/_next/static/chunks/5c1988096a7b174a.js\",\"/_next/static/chunks/33647e5ba6496195.js\",\"/_next/static/chunks/667473da0b5c11bc.js\",\"/_next/static/chunks/2e3229a62c65aaec.js\",\"/_next/static/chunks/010986693eb1c9c2.js\",\"/_next/static/chunks/2c9eb3077aa18f16.js\",\"/_next/static/chunks/6c680011ff6c5ba2.js\",\"/_next/static/chunks/2fd2aa01a4bc9178.js\",\"/_next/static/chunks/630870b77208f43d.js\"],\"default\"]\n18:I[837061,[\"/_next/static/chunks/1fb574e7be3f9a05.js\",\"/_next/static/chunks/9a604444e87766dd.js\",\"/_next/static/chunks/c1896c986be1a2e2.js\",\"/_next/static/chunks/7c80d08c36d49463.js\",\"/_next/static/chunks/496bc8a289f448d1.js\",\"/_next/static/chunks/b1040bb2d2fbd1e5.js\",\"/_next/static/chunks/5c1988096a7b174a.js\",\"/_next/static/chunks/33647e5ba6496195.js\",\"/_next/static/chunks/667473da0b5c11bc.js\",\"/_next/static/chunks/2e3229a62c65aaec.js\",\"/_next/static/chunks/010986693eb1c9c2.js\",\"/_next/static/chunks/2c9eb3077aa18f16.js\",\"/_next/static/chunks/6c680011ff6c5ba2.js\",\"/_next/static/chunks/2fd2aa01a4bc9178.js\",\"/_next/static/chunks/630870b77208f43d.js\"],\"default\"]\n1a:I[307003,[\"/_next/static/chunks/1fb574e7be3f9a05.js\",\"/_next/static/chunks/9a604444e87766dd.js\",\"/_next/static/chunks/c1896c986be1a2e2.js\",\"/_next/static/chunks/7c80d08c36d49463.js\",\"/_next/static/chunks/496bc8a289f448d1.js\",\"/_next/static/chunks/b1040bb2d2fbd1e5.js\",\"/_next/static/chunks/5c1988096a7b174a.js\",\"/_next/static/chunks/33647e5ba6496195.js\",\"/_next/static/chunks/667473da0b5c11bc.js\",\"/_next/static/chunks/2e3229a62c65aaec.js\",\"/_next/static/chunks/010986693eb1c9c2.js\",\"/_next/static/chunks/2c9eb3077aa18f16.js\",\"/_next/static/chunks/6c680011ff6c5ba2.js\",\"/_next/static/chunks/2fd2aa01a4bc9178.js\",\"/_next/static/chunks/630870b77208f43d.js\"],\"default\"]\n1b:I[27201,[\"/_next/static/chunks/d96012bcfc98706a.js\",\"/_next/static/chunks/d80b3790a119a285.js\"],\"IconMark\"]\n:HL[\"/_next/static/chunks/ec368b341879b233.css\",\"style\",{\"nonce\":\"OTQ3MTg"])</script><script nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx">self.__next_f.push([1,"yNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\"}]\n:HL[\"/_next/static/chunks/38fee8473f816a4a.css\",\"style\",{\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\"}]\n:HL[\"/_next/static/media/AnthropicMono_Italic_Web-s.p.154bb54e.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/AnthropicMono_Roman_Web-s.p.e2998bbe.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/AnthropicSans_Italic_Variable-s.p.dfc8e235.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/AnthropicSans_Roman_Variable-s.p.52cc3a10.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/AnthropicSerif_Italic_Variable-s.p.9d7ca5ec.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/AnthropicSerif_Roman_Variable-s.p.55835b1f.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/Copernicus_Book-s.p.f166c0ba.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/Copernicus_Medium-s.p.59728346.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/JetBrainsMono_VF-s.p.8dac7c36.ttf\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\",\"type\":\"font/ttf\"}]\n:HL[\"/_next/static/media/StyreneA_MediumItalic_Web-s.p.e9bc3c6e.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/StyreneA_Medium_Web-s.p.e5135f7e.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/StyreneA_RegularItalic_Web-s.p.7c6a646d.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/StyreneA_Regular_Web-s.p.429c699d.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/StyreneB_Medium_Web-s.p.88fa5a67.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/StyreneB_Regular_Web-s.p.cb3cc1a3.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/TiemposText_Medium-s.p.520d99f8.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/TiemposText_MediumItalic-s.p.10f44518.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/TiemposText_Regular-s.p.7f1d46d6.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/TiemposText_RegularItalic-s.p.1a798fcf.woff2\",\"font\",{\"crossOrigin\":\"\",\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/chunks/caf680e685668b99.css\",\"style\",{\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\"}]\n:HL[\"/_next/static/chunks/ad266d0a6bc656af.css\",\"style\",{\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\"}]\n:HL[\"/_next/static/chunks/e2c670ea67fc2bbb.css\",\"style\",{\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\"}]\n:HL[\"/_next/static/chunks/758311c654d998de.css\",\"style\",{\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\"}]\n:HL[\"/_next/static/chunks/7e4146583225b449.css\",\"style\",{\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\"}]\n"])</script><script nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx">self.__next_f.push([1,"0:{\"P\":null,\"b\":\"Ev1insoJIZ1ve_aUh7493\",\"c\":[\"\",\"engineering\",\"effective-context-engineering-for-ai-agents\"],\"q\":\"\",\"i\":false,\"f\":[[[\"\",{\"children\":[\"(site)\",{\"children\":[\"engineering\",{\"children\":[[\"slug\",\"effective-context-engineering-for-ai-agents\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]}]},\"$undefined\",\"$undefined\",true],[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/ec368b341879b233.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\"}]],\"$L2\"]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/38fee8473f816a4a.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\"}],[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/1fb574e7be3f9a05.js\",\"async\":true,\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\"}],[\"$\",\"script\",\"script-1\",{\"src\":\"/_next/static/chunks/9a604444e87766dd.js\",\"async\":true,\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\"}],[\"$\",\"script\",\"script-2\",{\"src\":\"/_next/static/chunks/c1896c986be1a2e2.js\",\"async\":true,\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\"}]],\"$L3\"]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[\"$L6\",[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/caf680e685668b99.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/ad266d0a6bc656af.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\"}],[\"$\",\"link\",\"2\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/e2c670ea67fc2bbb.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\"}],[\"$\",\"link\",\"3\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/758311c654d998de.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\"}],[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/f5a33d7993e253c8.js\",\"async\":true,\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\"}],[\"$\",\"script\",\"script-1\",{\"src\":\"/_next/static/chunks/496bc8a289f448d1.js\",\"async\":true,\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\"}],[\"$\",\"script\",\"script-2\",{\"src\":\"/_next/static/chunks/b1040bb2d2fbd1e5.js\",\"async\":true,\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\"}],[\"$\",\"script\",\"script-3\",{\"src\":\"/_next/static/chunks/2e3229a62c65aaec.js\",\"async\":true,\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\"}],[\"$\",\"script\",\"script-4\",{\"src\":\"/_next/static/chunks/5c1988096a7b174a.js\",\"async\":true,\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\"}],[\"$\",\"script\",\"script-5\",{\"src\":\"/_next/static/chunks/f563a58c137d4bc2.js\",\"async\":true,\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\"}],[\"$\",\"script\",\"script-6\",{\"src\":\"/_next/static/chunks/2fd2aa01a4bc9178.js\",\"async\":true,\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\"}],[\"$\",\"script\",\"script-7\",{\"src\":\"/_next/static/chunks/630870b77208f43d.js\",\"async\":true,\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\"}],[\"$\",\"script\",\"script-8\",{\"src\":\"/_next/static/chunks/010986693eb1c9c2.js\",\"async\":true,\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\"}],[\"$\",\"script\",\"script-9\",{\"src\":\"/_next/static/chunks/dabacb64939959b3.js\",\"async\":true,\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\"}],[\"$\",\"script\",\"script-10\",{\"src\":\"/_next/static/chunks/c0d75d4ca01ae43d.js\",\"async\":true,\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\"}],[\"$\",\"script\",\"script-11\",{\"src\":\"/_next/static/chunks/6c680011ff6c5ba2.js\",\"async\":true,\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\"}],\"$L7\"],\"$L8\"]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false]},null,false,false],\"$L9\",false]],\"m\":\"$undefined\",\"G\":[\"$a\",[]],\"S\":false}\n"])</script><script nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx">self.__next_f.push([1,"7:[\"$\",\"script\",\"script-12\",{\"src\":\"/_next/static/chunks/81716bb24f5a6f8f.js\",\"async\":true,\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\"}]\n8:[\"$\",\"$Lb\",null,{\"children\":[\"$\",\"$c\",null,{\"name\":\"Next.MetadataOutlet\",\"children\":\"$@d\"}]}]\n9:[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$Le\",null,{\"children\":\"$@f\"}],[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$L10\",null,{\"children\":[\"$\",\"$c\",null,{\"name\":\"Next.Metadata\",\"children\":\"$@11\"}]}]}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}]\n"])</script><script nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx">self.__next_f.push([1,"2:[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"anthropicsans_eac0b31f-module__tjnuGq__variable anthropicserif_87b6fa7d-module__quIBbW__variable anthropicmono_fae19af3-module__c5XAsG__variable copernicus_4da799c5-module__dijTSq__variable styrenea_f8492ab1-module__HimLXW__variable styreneb_278af5c6-module__wkOAdG__variable tiempostext_4eff4b4c-module__mpviCW__variable jetbrainsmono_7d7bdbc6-module__j_XgJq__variable\",\"children\":[\"$\",\"body\",null,{\"children\":[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]\n"])</script><script nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx">self.__next_f.push([1,"3:[\"$\",\"$L12\",null,{\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\",\"children\":[\"$\",\"$L13\",null,{\"gpcDetected\":false,\"children\":[[\"$\",\"$L14\",null,{}],[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[\"$L15\",[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/caf680e685668b99.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/ad266d0a6bc656af.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\"}],[\"$\",\"link\",\"2\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/e2c670ea67fc2bbb.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\"}],[\"$\",\"link\",\"3\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/7e4146583225b449.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx\"}]]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}]}]\n"])</script><script nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx">self.__next_f.push([1,"6:[\"$\",\"$L16\",null,{\"article\":{\"_createdAt\":\"2025-04-22T15:57:31Z\",\"_id\":\"de6d63b0-1525-4a0c-9f26-14f4e355eb86\",\"_rev\":\"lKcsmHdx4q1IKN0yIb7ck5\",\"_system\":{\"base\":{\"id\":\"de6d63b0-1525-4a0c-9f26-14f4e355eb86\",\"rev\":\"5fOEcDhN95lNjtM2l9z8Cu\"}},\"_type\":\"engineeringArticle\",\"_updatedAt\":\"2026-01-06T15:31:12Z\",\"body\":[{\"_key\":\"feb908270b7c\",\"_type\":\"block\",\"children\":[{\"_key\":\"1ffb3f5449d70\",\"_type\":\"span\",\"marks\":[],\"text\":\"After a few years of prompt engineering being the focus of attention in applied AI, a new term has come to prominence: \"},{\"_key\":\"1ffb3f5449d71\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"context engineering\"},{\"_key\":\"1ffb3f5449d72\",\"_type\":\"span\",\"marks\":[],\"text\":\". Building with language models is becoming less about finding the right words and phrases for your prompts, and more about answering the broader question of “what configuration of context is most likely to generate our model’s desired behavior?\\\"\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"09a57859316a\",\"_type\":\"block\",\"children\":[{\"_key\":\"dc083121ff2d0\",\"_type\":\"span\",\"marks\":[],\"text\":\"\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"bfa97b338533\",\"_type\":\"block\",\"children\":[{\"_key\":\"e73f82d064dc0\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Context\"},{\"_key\":\"e73f82d064dc1\",\"_type\":\"span\",\"marks\":[],\"text\":\" refers to the set of tokens included when sampling from a large-language model (LLM). The \"},{\"_key\":\"e73f82d064dc2\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"engineering\"},{\"_key\":\"e73f82d064dc3\",\"_type\":\"span\",\"marks\":[],\"text\":\" problem at hand is optimizing the utility of those tokens against the inherent constraints of LLMs in order to consistently achieve a desired outcome. Effectively wrangling LLMs often requires \"},{\"_key\":\"e73f82d064dc4\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"thinking in context \"},{\"_key\":\"e73f82d064dc5\",\"_type\":\"span\",\"marks\":[],\"text\":\"— in other words: considering the holistic state available to the LLM at any given time and what potential behaviors that state might yield.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"fa4ee057367a\",\"_type\":\"block\",\"children\":[{\"_key\":\"e856c01079c20\",\"_type\":\"span\",\"marks\":[],\"text\":\"\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"70ae5cd2ba6e\",\"_type\":\"block\",\"children\":[{\"_key\":\"392279fe801f0\",\"_type\":\"span\",\"marks\":[],\"text\":\"In this post, we’ll explore the emerging art of context engineering and offer a refined mental model for building steerable, effective agents.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"14ec3837a136\",\"_type\":\"block\",\"children\":[{\"_key\":\"6fe031bc4a0e\",\"_type\":\"span\",\"marks\":[],\"text\":\"\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"f4b020e217ea\",\"_type\":\"block\",\"children\":[{\"_key\":\"a3663258dabf0\",\"_type\":\"span\",\"marks\":[],\"text\":\"Context engineering vs. prompt engineering\"}],\"markDefs\":[],\"style\":\"h3\"},{\"_key\":\"aaf023b2ef4f\",\"_type\":\"block\",\"children\":[{\"_key\":\"30b1ee8a72040\",\"_type\":\"span\",\"marks\":[],\"text\":\"At Anthropic, we view context engineering as the natural progression of prompt engineering. Prompt engineering refers to methods for writing and organizing LLM instructions for optimal outcomes (see \"},{\"_key\":\"30b1ee8a72041\",\"_type\":\"span\",\"marks\":[\"f93a88847c59\"],\"text\":\"our docs\"},{\"_key\":\"30b1ee8a72042\",\"_type\":\"span\",\"marks\":[],\"text\":\" for an overview and useful prompt engineering strategies). \"},{\"_key\":\"30b1ee8a72043\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Context engineering\"},{\"_key\":\"30b1ee8a72044\",\"_type\":\"span\",\"marks\":[],\"text\":\" refers to the set of strategies for curating and maintaining the optimal set of tokens (information) during LLM inference, including all the other information that may land there outside of the prompts.\"}],\"markDefs\":[{\"_key\":\"f93a88847c59\",\"_type\":\"link\",\"href\":\"https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview\"}],\"style\":\"normal\"},{\"_key\":\"7f7531b9e475\",\"_type\":\"block\",\"children\":[{\"_key\":\"a8b4f23665260\",\"_type\":\"span\",\"marks\":[],\"text\":\"In the early days of engineering with LLMs, prompting was the biggest component of AI engineering work, as the majority of use cases outside of everyday chat interactions required prompts optimized for one-shot classification or text generation tasks. As the term implies, the primary focus of prompt engineering is how to write effective prompts, particularly system prompts. However, as we move towards engineering more capable agents that operate over multiple turns of inference and longer time horizons, we need strategies for managing the entire context state (system instructions, tools, \"},{\"_key\":\"a8b4f23665261\",\"_type\":\"span\",\"marks\":[\"b7e3e856fc3a\"],\"text\":\"Model Context Protocol\"},{\"_key\":\"a8b4f23665262\",\"_type\":\"span\",\"marks\":[],\"text\":\" (MCP), external data, message history, etc).\"}],\"markDefs\":[{\"_key\":\"b7e3e856fc3a\",\"_type\":\"link\",\"href\":\"https://modelcontextprotocol.io/docs/getting-started/intro\"}],\"style\":\"normal\"},{\"_key\":\"d47914a229a2\",\"_type\":\"block\",\"children\":[{\"_key\":\"676ed63d9cd90\",\"_type\":\"span\",\"marks\":[],\"text\":\"An agent running in a loop generates more and more data that \"},{\"_key\":\"676ed63d9cd91\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"could\"},{\"_key\":\"676ed63d9cd92\",\"_type\":\"span\",\"marks\":[],\"text\":\" be relevant for the next turn of inference, and this information must be cyclically refined. Context engineering is the \"},{\"_key\":\"676ed63d9cd93\",\"_type\":\"span\",\"marks\":[\"864ea936115f\"],\"text\":\"art and science\"},{\"_key\":\"676ed63d9cd94\",\"_type\":\"span\",\"marks\":[],\"text\":\" of curating what will go into the limited context window from that constantly evolving universe of possible information.\"}],\"markDefs\":[{\"_key\":\"864ea936115f\",\"_type\":\"link\",\"href\":\"https://x.com/karpathy/status/1937902205765607626?lang=en\"}],\"style\":\"normal\"},{\"_key\":\"ffaa37c386b4\",\"_type\":\"image\",\"asset\":{\"_ref\":\"image-faa261102e46c7f090a2402a49000ffae18c5dd6-2292x1290-png\",\"_type\":\"reference\"},\"caption\":[{\"_key\":\"518069996bd9\",\"_type\":\"block\",\"children\":[{\"_key\":\"202ab894bffc\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"In contrast to the discrete task of writing a prompt, context engineering is iterative and the curation phase happens each time we decide what to pass to the model.\"}],\"markDefs\":[],\"style\":\"normal\"}],\"description\":\"Prompt engineering vs. context engineering\",\"height\":1290,\"markDefs\":null,\"style\":\"inline\",\"url\":\"https://cdn.sanity.io/images/4zrzovbb/website/faa261102e46c7f090a2402a49000ffae18c5dd6-2292x1290.png\",\"width\":2292},{\"_key\":\"ef898cfa1a88\",\"_type\":\"block\",\"children\":[{\"_key\":\"df6d684df7330\",\"_type\":\"span\",\"marks\":[],\"text\":\"\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"fdaf7cc06a1f\",\"_type\":\"block\",\"children\":[{\"_key\":\"48f13ec81d150\",\"_type\":\"span\",\"marks\":[],\"text\":\"Why context engineering is important to building capable agents\"}],\"markDefs\":[],\"style\":\"h3\"},{\"_key\":\"39669c32501e\",\"_type\":\"block\",\"children\":[{\"_key\":\"a71d1e9cd9fe0\",\"_type\":\"span\",\"marks\":[],\"text\":\"Despite their speed and ability to manage larger and larger volumes of data, we’ve observed that LLMs, like humans, lose focus or experience confusion at a certain point. Studies on needle-in-a-haystack\"},{\"_key\":\"a71d1e9cd9fe1\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\" \"},{\"_key\":\"a71d1e9cd9fe2\",\"_type\":\"span\",\"marks\":[],\"text\":\"style benchmarking have uncovered the concept of \"},{\"_key\":\"a71d1e9cd9fe3\",\"_type\":\"span\",\"marks\":[\"1e87a185c882\"],\"text\":\"context rot\"},{\"_key\":\"a71d1e9cd9fe4\",\"_type\":\"span\",\"marks\":[],\"text\":\": as the number of tokens in the context window increases, the model’s ability to accurately recall information from that context decreases.\"}],\"markDefs\":[{\"_key\":\"1e87a185c882\",\"_type\":\"link\",\"href\":\"https://research.trychroma.com/context-rot\"}],\"style\":\"normal\"},{\"_key\":\"5bd3abcfb223\",\"_type\":\"block\",\"children\":[{\"_key\":\"663537e41e240\",\"_type\":\"span\",\"marks\":[],\"text\":\"While some models exhibit more gentle degradation than others, this characteristic emerges across all models. Context, therefore, must be treated as a finite resource with diminishing marginal returns. Like humans, who have \"},{\"_key\":\"663537e41e241\",\"_type\":\"span\",\"marks\":[\"8024aae1bc50\"],\"text\":\"limited working memory capacity\"},{\"_key\":\"663537e41e242\",\"_type\":\"span\",\"marks\":[],\"text\":\", LLMs have an “attention budget” that they draw on when parsing large volumes of context. Every new token introduced depletes this budget by some amount, increasing the need to carefully curate the tokens available to the LLM.\"}],\"markDefs\":[{\"_key\":\"8024aae1bc50\",\"_type\":\"link\",\"href\":\"https://journals.sagepub.com/doi/abs/10.1177/0963721409359277\"}],\"style\":\"normal\"},{\"_key\":\"62d59e84fe55\",\"_type\":\"block\",\"children\":[{\"_key\":\"cd7c013adf3c0\",\"_type\":\"span\",\"marks\":[],\"text\":\"This attention scarcity stems from architectural constraints of LLMs. LLMs are based on the \"},{\"_key\":\"cd7c013adf3c1\",\"_type\":\"span\",\"marks\":[\"25c6c38c22ff\"],\"text\":\"transformer architecture\"},{\"_key\":\"cd7c013adf3c2\",\"_type\":\"span\",\"marks\":[],\"text\":\", which enables every token to \"},{\"_key\":\"cd7c013adf3c3\",\"_type\":\"span\",\"marks\":[\"a492277e79ae\"],\"text\":\"attend to every other token\"},{\"_key\":\"cd7c013adf3c4\",\"_type\":\"span\",\"marks\":[],\"text\":\" across the entire context. This results in n² pairwise relationships for n tokens.\"}],\"markDefs\":[{\"_key\":\"25c6c38c22ff\",\"_type\":\"link\",\"href\":\"https://arxiv.org/abs/1706.03762\"},{\"_key\":\"a492277e79ae\",\"_type\":\"link\",\"href\":\"https://huggingface.co/blog/Esmail-AGumaan/attention-is-all-you-need\"}],\"style\":\"normal\"},{\"_key\":\"a55ccb98d07f\",\"_type\":\"block\",\"children\":[{\"_key\":\"db563056d46a0\",\"_type\":\"span\",\"marks\":[],\"text\":\"As its context length increases, a model's ability to capture these pairwise relationships gets stretched thin, creating a natural tension between context size and attention focus. Additionally, models develop their attention patterns from training data distributions where shorter sequences are typically more common than longer ones. This means models have less experience with, and fewer specialized parameters for, context-wide dependencies.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"bc95c1e963da\",\"_type\":\"block\",\"children\":[{\"_key\":\"f9333bc645850\",\"_type\":\"span\",\"marks\":[],\"text\":\"Techniques like \"},{\"_key\":\"f9333bc645851\",\"_type\":\"span\",\"marks\":[\"2b1561413a2d\"],\"text\":\"position encoding interpolation\"},{\"_key\":\"f9333bc645852\",\"_type\":\"span\",\"marks\":[],\"text\":\" allow models to handle longer sequences by adapting them to the originally trained smaller context, though with some degradation in token position understanding. These factors create a performance gradient rather than a hard cliff: models remain highly capable at longer contexts but may show reduced precision for information retrieval and long-range reasoning compared to their performance on shorter contexts.\"}],\"markDefs\":[{\"_key\":\"2b1561413a2d\",\"_type\":\"link\",\"href\":\"https://arxiv.org/pdf/2306.15595\"}],\"style\":\"normal\"},{\"_key\":\"17ee97d569c4\",\"_type\":\"block\",\"children\":[{\"_key\":\"1e857123c82e0\",\"_type\":\"span\",\"marks\":[],\"text\":\"These realities mean that thoughtful context engineering is essential for building capable agents.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"c6a5312d3d3d\",\"_type\":\"block\",\"children\":[{\"_key\":\"e618f2b3eef10\",\"_type\":\"span\",\"marks\":[],\"text\":\"The anatomy of effective context\"}],\"markDefs\":[],\"style\":\"h3\"},{\"_key\":\"3e2f989ac3be\",\"_type\":\"block\",\"children\":[{\"_key\":\"176d03a4d8620\",\"_type\":\"span\",\"marks\":[],\"text\":\"Given that LLMs are constrained by a finite attention budget, \"},{\"_key\":\"176d03a4d8621\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"good\"},{\"_key\":\"176d03a4d8622\",\"_type\":\"span\",\"marks\":[],\"text\":\" context engineering means finding the \"},{\"_key\":\"176d03a4d8623\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"smallest\"},{\"_key\":\"176d03a4d8624\",\"_type\":\"span\",\"marks\":[],\"text\":\" \"},{\"_key\":\"176d03a4d8625\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"possible\"},{\"_key\":\"176d03a4d8626\",\"_type\":\"span\",\"marks\":[],\"text\":\" set of high-signal tokens that maximize the likelihood of some desired outcome. Implementing this practice is much easier said than done, but in the following section, we outline what this guiding principle means in practice across the different components of context.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"6384e71b2871\",\"_type\":\"block\",\"children\":[{\"_key\":\"7af5de56d8410\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"System prompts\"},{\"_key\":\"7af5de56d8411\",\"_type\":\"span\",\"marks\":[],\"text\":\" should be extremely clear and use simple, direct language that presents ideas at the \"},{\"_key\":\"7af5de56d8412\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"right altitude \"},{\"_key\":\"7af5de56d8413\",\"_type\":\"span\",\"marks\":[],\"text\":\"for the agent. The right altitude is the Goldilocks zone between two common failure modes. At one extreme, we see engineers hardcoding complex, brittle logic in their prompts to elicit exact agentic behavior. This approach creates fragility and increases maintenance complexity over time. At the other extreme, engineers sometimes provide vague, high-level guidance that fails to give the LLM concrete signals for desired outputs or falsely assumes shared context. The optimal altitude strikes a balance: specific enough to guide behavior effectively, yet flexible enough to provide the model with strong heuristics to guide behavior.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"86cf659c2ee9\",\"_type\":\"image\",\"asset\":{\"_ref\":\"image-0442fe138158e84ffce92bed1624dd09f37ac46f-2292x1288-png\",\"_type\":\"reference\"},\"caption\":[{\"_key\":\"8a9f30d8d169\",\"_type\":\"block\",\"children\":[{\"_key\":\"0b7b9a6fa423\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"At one end of the spectrum, we see brittle if-else hardcoded prompts, and at the other end we see prompts that are overly general or falsely assume shared context.\"}],\"markDefs\":[],\"style\":\"normal\"}],\"description\":\"Calibrating the system prompt in the process of context engineering.\",\"height\":1288,\"markDefs\":null,\"style\":\"inline\",\"url\":\"https://cdn.sanity.io/images/4zrzovbb/website/0442fe138158e84ffce92bed1624dd09f37ac46f-2292x1288.png\",\"width\":2292},{\"_key\":\"770e88fbeb4c\",\"_type\":\"block\",\"children\":[{\"_key\":\"772676ee600d0\",\"_type\":\"span\",\"marks\":[],\"text\":\"We recommend organizing prompts into distinct sections (like \"},{\"_key\":\"0761ae1ff84f\",\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"\u003cbackground_information\u003e\"},{\"_key\":\"c732a7a15cbc\",\"_type\":\"span\",\"marks\":[],\"text\":\", \"},{\"_key\":\"aa847d35d848\",\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"\u003cinstructions\u003e\"},{\"_key\":\"ca9058fd30ac\",\"_type\":\"span\",\"marks\":[],\"text\":\", \"},{\"_key\":\"a164268c6fc0\",\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"## Tool guidance\"},{\"_key\":\"35cf7eb91d33\",\"_type\":\"span\",\"marks\":[],\"text\":\", \"},{\"_key\":\"91bd3051081d\",\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"## Output description\"},{\"_key\":\"435974b7e788\",\"_type\":\"span\",\"marks\":[],\"text\":\", etc) and using techniques like XML tagging or Markdown headers to delineate these sections, although the exact formatting of prompts is likely becoming less important as models become more capable.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"5a6003e70d6d\",\"_type\":\"block\",\"children\":[{\"_key\":\"313c8c8b26540\",\"_type\":\"span\",\"marks\":[],\"text\":\"\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"0f99cf869a8d\",\"_type\":\"block\",\"children\":[{\"_key\":\"46f0d4cae82e0\",\"_type\":\"span\",\"marks\":[],\"text\":\"Regardless of how you decide to structure your system prompt, you should be striving for the minimal set of information that fully outlines your expected behavior. (Note that minimal does not necessarily mean short; you still need to give the agent sufficient information up front to ensure it adheres to the desired behavior.) It’s best to start by testing a minimal prompt with the best model available to see how it performs on your task, and then add clear instructions and examples to improve performance based on failure modes found during initial testing.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"bcd0f9bff8db\",\"_type\":\"block\",\"children\":[{\"_key\":\"782562ac09230\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Tools\"},{\"_key\":\"782562ac09231\",\"_type\":\"span\",\"marks\":[],\"text\":\" allow agents to operate with their environment and pull in new, additional context as they work. Because tools define the contract between agents and their information/action space, it’s extremely important that tools promote efficiency, both by returning information that is token efficient and by encouraging efficient agent behaviors.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"0459815b5e3b\",\"_type\":\"block\",\"children\":[{\"_key\":\"c0ca6f0b62fc0\",\"_type\":\"span\",\"marks\":[],\"text\":\"In \"},{\"_key\":\"c0ca6f0b62fc1\",\"_type\":\"span\",\"marks\":[\"96c154ced130\"],\"text\":\"Writing tools for AI agents – with AI agents\"},{\"_key\":\"c0ca6f0b62fc2\",\"_type\":\"span\",\"marks\":[],\"text\":\", we discussed building tools that are well understood by LLMs and have minimal overlap in functionality. Similar to the functions of a well-designed codebase, tools should be self-contained, robust to error, and extremely clear with respect to their intended use. Input parameters should similarly be descriptive, unambiguous, and play to the inherent strengths of the model.\"}],\"markDefs\":[{\"_key\":\"96c154ced130\",\"_type\":\"link\",\"href\":\"/www.anthropic.com/engineering/writing-tools-for-agents\"}],\"style\":\"normal\"},{\"_key\":\"b74e1f7d963b\",\"_type\":\"block\",\"children\":[{\"_key\":\"51a9f35ee1f70\",\"_type\":\"span\",\"marks\":[],\"text\":\"One of the most common failure modes we see is bloated tool sets that cover too much functionality or lead to ambiguous decision points about which tool to use. If a human engineer can’t definitively say which tool should be used in a given situation, an AI agent can’t be expected to do better. As we’ll discuss later, curating a minimal viable set of tools for the agent can also lead to more reliable maintenance and pruning of context over long interactions.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"a7f17d0e249f\",\"_type\":\"block\",\"children\":[{\"_key\":\"89d54e7e2b220\",\"_type\":\"span\",\"marks\":[],\"text\":\"Providing examples, otherwise known as few-shot prompting, is a well known best practice that we continue to strongly advise. However, teams will often stuff a laundry list of edge cases into a prompt in an attempt to articulate every possible rule the LLM should follow for a particular task. We do not recommend this. Instead, we recommend working to curate a set of diverse, canonical examples that effectively portray the expected behavior of the agent. For an LLM, examples are the “pictures” worth a thousand words.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"ba1d23eb0aaf\",\"_type\":\"block\",\"children\":[{\"_key\":\"6cc717cf2f3c0\",\"_type\":\"span\",\"marks\":[],\"text\":\"Our overall guidance across the different components of context (system prompts\"},{\"_key\":\"6cc717cf2f3c1\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\", \"},{\"_key\":\"6cc717cf2f3c2\",\"_type\":\"span\",\"marks\":[],\"text\":\"tools\"},{\"_key\":\"6cc717cf2f3c3\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\", \"},{\"_key\":\"6cc717cf2f3c4\",\"_type\":\"span\",\"marks\":[],\"text\":\"examples\"},{\"_key\":\"6cc717cf2f3c5\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\", \"},{\"_key\":\"6cc717cf2f3c6\",\"_type\":\"span\",\"marks\":[],\"text\":\"message history, etc) is to be thoughtful and keep your context informative, yet tight. Now let's dive into dynamically retrieving context at runtime.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"fea2cf80000d\",\"_type\":\"block\",\"children\":[{\"_key\":\"82649f8f7a4a0\",\"_type\":\"span\",\"marks\":[],\"text\":\"Context retrieval and agentic search\"}],\"markDefs\":[],\"style\":\"h3\"},{\"_key\":\"f6b50823dca9\",\"_type\":\"block\",\"children\":[{\"_key\":\"50ea57a12be10\",\"_type\":\"span\",\"marks\":[],\"text\":\"In \"},{\"_key\":\"50ea57a12be11\",\"_type\":\"span\",\"marks\":[\"dafefab75887\"],\"text\":\"Building effective AI agents\"},{\"_key\":\"50ea57a12be12\",\"_type\":\"span\",\"marks\":[],\"text\":\", we highlighted the differences between LLM-based workflows and agents. Since we wrote that post, we’ve gravitated towards a \"},{\"_key\":\"50ea57a12be13\",\"_type\":\"span\",\"marks\":[\"319d8d1c7f7c\"],\"text\":\"simple definition\"},{\"_key\":\"50ea57a12be14\",\"_type\":\"span\",\"marks\":[],\"text\":\" for agents: LLMs autonomously using tools in a loop.\"}],\"markDefs\":[{\"_key\":\"dafefab75887\",\"_type\":\"link\",\"href\":\"/www.anthropic.com/research/building-effective-agents\"},{\"_key\":\"319d8d1c7f7c\",\"_type\":\"link\",\"href\":\"https://simonwillison.net/2025/Sep/18/agents/\"}],\"style\":\"normal\"},{\"_key\":\"66126f7eb6dd\",\"_type\":\"block\",\"children\":[{\"_key\":\"f2f6faa673860\",\"_type\":\"span\",\"marks\":[],\"text\":\"Working alongside our customers, we’ve seen the field converging on this simple paradigm. As the underlying models become more capable, the level of autonomy of agents can scale: smarter models allow agents to independently navigate nuanced problem spaces and recover from errors.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"df4f96e5c84e\",\"_type\":\"block\",\"children\":[{\"_key\":\"96c6af7fa0ec0\",\"_type\":\"span\",\"marks\":[],\"text\":\"We’re now seeing a shift in how engineers think about designing context for agents. Today, many AI-native applications employ some form of embedding-based pre-inference time retrieval to surface important context for the agent to reason over. As the field transitions to more agentic approaches, we increasingly see teams augmenting these retrieval systems with “just in time” context strategies.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"1c27c300e0ea\",\"_type\":\"block\",\"children\":[{\"_key\":\"b853e18d9d800\",\"_type\":\"span\",\"marks\":[],\"text\":\"Rather than pre-processing all relevant data up front, agents built with the “just in time” approach maintain lightweight identifiers (file paths, stored queries, web links, etc.) and use these references to dynamically load data into context at runtime using tools. Anthropic’s agentic coding solution \"},{\"_key\":\"b853e18d9d801\",\"_type\":\"span\",\"marks\":[\"29b6ef96876e\"],\"text\":\"Claude Code\"},{\"_key\":\"b853e18d9d802\",\"_type\":\"span\",\"marks\":[],\"text\":\" uses this approach to perform complex data analysis over large databases. The model can write targeted queries, store results, and leverage Bash commands like head and tail to analyze large volumes of data without ever loading the full data objects into context. This approach mirrors human cognition: we generally don’t memorize entire corpuses of information, but rather introduce external organization and indexing systems like file systems, inboxes, and bookmarks to retrieve relevant information on demand.\"}],\"markDefs\":[{\"_key\":\"29b6ef96876e\",\"_type\":\"link\",\"href\":\"/www.anthropic.com/claude-code\"}],\"style\":\"normal\"},{\"_key\":\"9909dfade70d\",\"_type\":\"block\",\"children\":[{\"_key\":\"648843f596dc0\",\"_type\":\"span\",\"marks\":[],\"text\":\"Beyond storage efficiency, the metadata of these references provides a mechanism to efficiently refine behavior, whether explicitly provided or intuitive. To an agent operating in a file system, the presence of a file named \"},{\"_key\":\"4cf2129180fa\",\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"test_utils.py\"},{\"_key\":\"7b2db1717ec2\",\"_type\":\"span\",\"marks\":[],\"text\":\" in a \"},{\"_key\":\"b38d721bf40a\",\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"tests\"},{\"_key\":\"aabac2612900\",\"_type\":\"span\",\"marks\":[],\"text\":\" folder implies a different purpose than a file with the same name located in \"},{\"_key\":\"9b5d0833614e\",\"_type\":\"span\",\"marks\":[\"code\"],\"text\":\"src/core_logic/\"},{\"_key\":\"31261f6712b3\",\"_type\":\"span\",\"marks\":[],\"text\":\" Folder hierarchies, naming conventions, and timestamps all provide important signals that help both humans and agents understand how and when to utilize information.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"cf20dfbe9a58\",\"_type\":\"block\",\"children\":[{\"_key\":\"21d6649db3090\",\"_type\":\"span\",\"marks\":[],\"text\":\"Letting agents navigate and retrieve data autonomously also enables progressive disclosure—in other words, allows agents to incrementally discover relevant context through exploration. Each interaction yields context that informs the next decision: file sizes suggest complexity; naming conventions hint at purpose; timestamps can be a proxy for relevance. Agents can assemble understanding layer by layer, maintaining only what's necessary in working memory and leveraging note-taking strategies for additional persistence. This self-managed context window keeps the agent focused on relevant subsets rather than drowning in exhaustive but potentially irrelevant information.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"6d767c5d2159\",\"_type\":\"block\",\"children\":[{\"_key\":\"86b76bbca0ca0\",\"_type\":\"span\",\"marks\":[],\"text\":\"Of course, there's a trade-off: runtime exploration is slower than retrieving pre-computed data. Not only that, but opinionated and thoughtful engineering is required to ensure that an LLM has the right tools and heuristics for effectively navigating its information landscape. Without proper guidance, an agent can waste context by misusing tools, chasing dead-ends, or failing to identify key information.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"9b72d283905f\",\"_type\":\"block\",\"children\":[{\"_key\":\"8fd035cd27c90\",\"_type\":\"span\",\"marks\":[],\"text\":\"In certain settings, the most effective agents might employ a hybrid strategy, retrieving some data up front for speed, and pursuing further autonomous exploration at its discretion. The decision boundary for the ‘right’ level of autonomy depends on the task. Claude Code is an agent that employs this hybrid model: \"},{\"_key\":\"8fd035cd27c91\",\"_type\":\"span\",\"marks\":[\"4ca6fc70970c\"],\"text\":\"CLAUDE.md\"},{\"_key\":\"8fd035cd27c92\",\"_type\":\"span\",\"marks\":[],\"text\":\" files are naively dropped into context up front, while primitives like glob and grep allow it to navigate its environment and retrieve files just-in-time, effectively bypassing the issues of stale indexing and complex syntax trees.\"}],\"markDefs\":[{\"_key\":\"4ca6fc70970c\",\"_type\":\"link\",\"href\":\"http://claude.md\"}],\"style\":\"normal\"},{\"_key\":\"0924a4da0756\",\"_type\":\"block\",\"children\":[{\"_key\":\"0b779157c1090\",\"_type\":\"span\",\"marks\":[],\"text\":\"The hybrid strategy might be better suited for contexts with less dynamic content, such as legal or finance work. As model capabilities improve, agentic design will trend towards letting intelligent models act intelligently, with progressively less human curation. Given the rapid pace of progress in the field, \\\"do the simplest thing that works\\\" will likely remain our best advice for teams building agents on top of Claude.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"363ad12f9f8b\",\"_type\":\"block\",\"children\":[{\"_key\":\"14c77fdf94910\",\"_type\":\"span\",\"marks\":[],\"text\":\"Context engineering for long-horizon tasks\"}],\"markDefs\":[],\"style\":\"h4\"},{\"_key\":\"1da1b529affc\",\"_type\":\"block\",\"children\":[{\"_key\":\"181ee6eb89a70\",\"_type\":\"span\",\"marks\":[],\"text\":\"Long-horizon tasks require agents to maintain coherence, context, and goal-directed behavior over sequences of actions where the token count exceeds the LLM’s context window. For tasks that span tens of minutes to multiple hours of continuous work, like large codebase migrations or comprehensive research projects, agents require specialized techniques to work around the context window size limitation.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"2065a27df793\",\"_type\":\"block\",\"children\":[{\"_key\":\"026235ba9f5c0\",\"_type\":\"span\",\"marks\":[],\"text\":\"Waiting for larger context windows might seem like an obvious tactic. But it's likely that for the foreseeable future, context windows of all sizes will be subject to context pollution and information relevance concerns—at least for situations where the strongest agent performance is desired. To enable agents to work effectively across extended time horizons, we've developed a few techniques that address these context pollution constraints directly: compaction, structured note-taking, and multi-agent architectures.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"6791b9ccd709\",\"_type\":\"block\",\"children\":[{\"_key\":\"4f9cb21749e70\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Compaction\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"7cd653271247\",\"_type\":\"block\",\"children\":[{\"_key\":\"572849ce29140\",\"_type\":\"span\",\"marks\":[],\"text\":\"Compaction is the practice of taking a conversation nearing the context window limit, summarizing its contents, and reinitiating a new context window with the summary. Compaction typically serves as the first lever in context engineering to drive better long-term coherence. At its core, compaction distills the contents of a context window in a high-fidelity manner, enabling the agent to continue with minimal performance degradation.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"f67b34ca2aa7\",\"_type\":\"block\",\"children\":[{\"_key\":\"a7566344d0240\",\"_type\":\"span\",\"marks\":[],\"text\":\"In Claude Code, for example, we implement this by passing the message history to the model to summarize and compress the most critical details. The model preserves architectural decisions, unresolved bugs, and implementation details while discarding redundant tool outputs or messages. The agent can then continue with this compressed context plus the five most recently accessed files. Users get continuity without worrying about context window limitations.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"f077101477f5\",\"_type\":\"block\",\"children\":[{\"_key\":\"5f24a64701220\",\"_type\":\"span\",\"marks\":[],\"text\":\"The art of compaction lies in the selection of what to keep versus what to discard, as overly aggressive compaction can result in the loss of subtle but critical context whose importance only becomes apparent later. For engineers implementing compaction systems, we recommend carefully tuning your prompt on  complex agent traces. Start by maximizing recall to ensure your compaction prompt captures every relevant piece of information from the trace, then iterate to improve precision by eliminating superfluous content.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"2f8616fff150\",\"_type\":\"block\",\"children\":[{\"_key\":\"b4d4f7949f700\",\"_type\":\"span\",\"marks\":[],\"text\":\"An example of low-hanging superfluous content is clearing tool calls and results – once a tool has been called deep in the message history, why would the agent need to see the raw result again? One of the safest lightest touch forms of compaction is tool result clearing, most recently launched as a \"},{\"_key\":\"b4d4f7949f701\",\"_type\":\"span\",\"marks\":[\"4aef0832a124\"],\"text\":\"feature on the Claude Developer Platform\"},{\"_key\":\"b4d4f7949f702\",\"_type\":\"span\",\"marks\":[],\"text\":\".\"}],\"markDefs\":[{\"_key\":\"4aef0832a124\",\"_type\":\"link\",\"href\":\"/www.anthropic.com/news/context-management\"}],\"style\":\"normal\"},{\"_key\":\"b57a47adfbf2\",\"_type\":\"block\",\"children\":[{\"_key\":\"f876c490dd000\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Structured note-taking\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"efa97429ce2e\",\"_type\":\"block\",\"children\":[{\"_key\":\"b2dcbf98706d0\",\"_type\":\"span\",\"marks\":[],\"text\":\"Structured note-taking, or agentic memory, is a technique where the agent regularly writes notes persisted to memory outside of the context window. These notes get pulled back into the context window at later times.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"93043793eeaa\",\"_type\":\"block\",\"children\":[{\"_key\":\"8c377aad350c0\",\"_type\":\"span\",\"marks\":[],\"text\":\"This strategy provides persistent memory with minimal overhead. Like Claude Code creating a to-do list, or your custom agent maintaining a NOTES.md file, this simple pattern allows the agent to track progress across complex tasks, maintaining critical context and dependencies that would otherwise be lost across dozens of tool calls.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"57e7420644ff\",\"_type\":\"block\",\"children\":[{\"_key\":\"33c0bc33e1c30\",\"_type\":\"span\",\"marks\":[\"5e0c6d5d740e\"],\"text\":\"Claude playing Pokémon\"},{\"_key\":\"33c0bc33e1c31\",\"_type\":\"span\",\"marks\":[],\"text\":\" demonstrates how memory transforms agent capabilities in non-coding domains. The agent maintains precise tallies across thousands of game steps—tracking objectives like \\\"for the last 1,234 steps I've been training my Pokémon in Route 1, Pikachu has gained 8 levels toward the target of 10.\\\" Without any prompting about memory structure, it develops maps of explored regions, remembers which key achievements it has unlocked, and maintains strategic notes of combat strategies that help it learn which attacks work best against different opponents.\"}],\"markDefs\":[{\"_key\":\"5e0c6d5d740e\",\"_type\":\"link\",\"href\":\"https://www.twitch.tv/claudeplayspokemon\"}],\"style\":\"normal\"},{\"_key\":\"a2b33a7fe3f7\",\"_type\":\"block\",\"children\":[{\"_key\":\"31121c4d66a10\",\"_type\":\"span\",\"marks\":[],\"text\":\"After context resets, the agent reads its own notes and continues multi-hour training sequences or dungeon explorations. This coherence across summarization steps enables long-horizon strategies that would be impossible when keeping all the information in the LLM’s context window alone.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"a7b6e078cedc\",\"_type\":\"block\",\"children\":[{\"_key\":\"57fae38185fc0\",\"_type\":\"span\",\"marks\":[],\"text\":\"As part of our \"},{\"_key\":\"57fae38185fc1\",\"_type\":\"span\",\"marks\":[\"5f823832ba71\"],\"text\":\"Sonnet 4.5 launch\"},{\"_key\":\"57fae38185fc2\",\"_type\":\"span\",\"marks\":[],\"text\":\", we released \"},{\"_key\":\"57fae38185fc3\",\"_type\":\"span\",\"marks\":[\"5d128bf6950a\"],\"text\":\"a memory tool\"},{\"_key\":\"57fae38185fc4\",\"_type\":\"span\",\"marks\":[],\"text\":\" in public beta on the Claude Developer Platform that makes it easier to store and consult information outside the context window through a file-based system. This allows agents to build up knowledge bases over time, maintain project state across sessions, and reference previous work without keeping everything in context.\"}],\"markDefs\":[{\"_key\":\"5f823832ba71\",\"_type\":\"link\",\"href\":\"/www.anthropic.com/effective-context-engineering-for-ai-agents\"},{\"_key\":\"5d128bf6950a\",\"_type\":\"link\",\"href\":\"/www.anthropic.com/news/context-management\"}],\"style\":\"normal\"},{\"_key\":\"04d10748f87d\",\"_type\":\"block\",\"children\":[{\"_key\":\"a4113885065b0\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Sub-agent architectures\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"a653cca0e359\",\"_type\":\"block\",\"children\":[{\"_key\":\"7796df2eaee00\",\"_type\":\"span\",\"marks\":[],\"text\":\"Sub-agent architectures provide another way around context limitations. Rather than one agent attempting to maintain state across an entire project, specialized sub-agents can handle focused tasks with clean context windows. The main agent coordinates with a high-level plan while subagents perform deep technical work or use tools to find relevant information. Each subagent might explore extensively, using tens of thousands of tokens or more, but returns only a condensed, distilled summary of its work (often 1,000-2,000 tokens).\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"18bd64c4e067\",\"_type\":\"block\",\"children\":[{\"_key\":\"687719b7d5000\",\"_type\":\"span\",\"marks\":[],\"text\":\"This approach achieves a clear separation of concerns—the detailed search context remains isolated within sub-agents, while the lead agent focuses on synthesizing and analyzing the results. This pattern, discussed in \"},{\"_key\":\"687719b7d5001\",\"_type\":\"span\",\"marks\":[\"37fa60042cce\"],\"text\":\"How we built our multi-agent research system\"},{\"_key\":\"687719b7d5002\",\"_type\":\"span\",\"marks\":[],\"text\":\", showed a substantial improvement over single-agent systems on complex research tasks.\"}],\"markDefs\":[{\"_key\":\"37fa60042cce\",\"_type\":\"link\",\"href\":\"/www.anthropic.com/engineering/multi-agent-research-system\"}],\"style\":\"normal\"},{\"_key\":\"d15b260fd647\",\"_type\":\"block\",\"children\":[{\"_key\":\"b3b53b0601e40\",\"_type\":\"span\",\"marks\":[],\"text\":\"The choice between these approaches depends on task characteristics. For example:\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"fdba1004d614\",\"_type\":\"block\",\"children\":[{\"_key\":\"5f163c8826ee0\",\"_type\":\"span\",\"marks\":[],\"text\":\"Compaction maintains conversational flow for tasks requiring extensive back-and-forth;\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"24c14062e271\",\"_type\":\"block\",\"children\":[{\"_key\":\"3c852ce1b2280\",\"_type\":\"span\",\"marks\":[],\"text\":\"Note-taking excels for iterative development with clear milestones;\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"68648826e29c\",\"_type\":\"block\",\"children\":[{\"_key\":\"31f92c585a820\",\"_type\":\"span\",\"marks\":[],\"text\":\"Multi-agent architectures handle complex research and analysis where parallel exploration pays dividends.\"}],\"level\":1,\"listItem\":\"bullet\",\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"edd7a1fefed3\",\"_type\":\"block\",\"children\":[{\"_key\":\"271ee0fd59130\",\"_type\":\"span\",\"marks\":[],\"text\":\"Even as models continue to improve, the challenge of maintaining coherence across extended interactions will remain central to building more effective agents.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"da84b1cc7edc\",\"_type\":\"block\",\"children\":[{\"_key\":\"1dba494b9f840\",\"_type\":\"span\",\"marks\":[],\"text\":\"Conclusion\"}],\"markDefs\":[],\"style\":\"h3\"},{\"_key\":\"394cd589bb77\",\"_type\":\"block\",\"children\":[{\"_key\":\"d27702e5f1b20\",\"_type\":\"span\",\"marks\":[],\"text\":\"Context engineering represents a fundamental shift in how we build with LLMs. As models become more capable, the challenge isn't just crafting the perfect prompt—it's thoughtfully curating what information enters the model's limited attention budget at each step. Whether you're implementing compaction for long-horizon tasks, designing token-efficient tools, or enabling agents to explore their environment just-in-time, the guiding principle remains the same: find the smallest set of high-signal tokens that maximize the likelihood of your desired outcome.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"850a75b5b65a\",\"_type\":\"block\",\"children\":[{\"_key\":\"ff6b0f3288340\",\"_type\":\"span\",\"marks\":[],\"text\":\"The techniques we've outlined will continue evolving as models improve. We're already seeing that smarter models require less prescriptive engineering, allowing agents to operate with more autonomy. But even as capabilities scale, treating context as a precious, finite resource will remain central to building reliable, effective agents.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"44957dde53a5\",\"_type\":\"block\",\"children\":[{\"_key\":\"559ffc9c99fc0\",\"_type\":\"span\",\"marks\":[],\"text\":\"Get started with context engineering in the Claude Developer Platform today, and access helpful tips and best practices via our \"},{\"_key\":\"559ffc9c99fc1\",\"_type\":\"span\",\"marks\":[\"01feaa4d033a\"],\"text\":\"memory and context management\"},{\"_key\":\"559ffc9c99fc2\",\"_type\":\"span\",\"marks\":[],\"text\":\" cookbook.\"}],\"markDefs\":[{\"_key\":\"01feaa4d033a\",\"_type\":\"link\",\"blank\":true,\"href\":\"https://platform.claude.com/cookbook/tool-use-memory-cookbook\"}],\"style\":\"normal\"},{\"_key\":\"77d7960d1022\",\"_type\":\"block\",\"children\":[{\"_key\":\"bc2de389eaeb0\",\"_type\":\"span\",\"marks\":[],\"text\":\"Acknowledgements\"}],\"markDefs\":[],\"style\":\"h3\"},{\"_key\":\"1b41ef6d95ce\",\"_type\":\"block\",\"children\":[{\"_key\":\"3d5226d021f20\",\"_type\":\"span\",\"marks\":[],\"text\":\"Written by Anthropic's Applied AI team: Prithvi Rajasekaran, Ethan Dixon, Carly Ryan, and Jeremy Hadfield, with contributions from team members Rafi Ayub, Hannah Moran, Cal Rueb, and Connor Jennings. Special thanks to Molly Vorwerck, Stuart Ritchie, and Maggie Vo for their support.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"ca534ab92759\",\"_type\":\"block\",\"children\":[{\"_key\":\"119953de59e00\",\"_type\":\"span\",\"marks\":[],\"text\":\"\\n\"}],\"markDefs\":[],\"style\":\"normal\"}],\"cardImage\":{\"_type\":\"image\",\"asset\":{\"_ref\":\"image-33b03d9dd1e6aec266ef22a616aec8f5676223f6-1000x1000-svg\",\"_type\":\"reference\"},\"height\":1000,\"url\":\"https://cdn.sanity.io/images/4zrzovbb/website/33b03d9dd1e6aec266ef22a616aec8f5676223f6-1000x1000.svg\",\"width\":1000},\"hero\":{\"_type\":\"image\",\"asset\":{\"_ref\":\"image-33b03d9dd1e6aec266ef22a616aec8f5676223f6-1000x1000-svg\",\"_type\":\"reference\"},\"caption\":null,\"height\":1000,\"url\":\"https://cdn.sanity.io/images/4zrzovbb/website/33b03d9dd1e6aec266ef22a616aec8f5676223f6-1000x1000.svg\",\"width\":1000},\"meta\":{\"robotsIndexable\":true,\"seoTitle\":\"Effective context engineering for AI agents\",\"socialImage\":{\"_type\":\"image\",\"asset\":{\"_createdAt\":\"2025-04-22T16:49:55Z\",\"_id\":\"image-ea2bf01aa874d7ab776453e97dfeed5d2bf5a116-2400x1260-png\",\"_rev\":\"CI4NCzcHwrw4peRLS3aNwz\",\"_type\":\"sanity.imageAsset\",\"_updatedAt\":\"2025-04-22T16:49:55Z\",\"assetId\":\"ea2bf01aa874d7ab776453e97dfeed5d2bf5a116\",\"extension\":\"png\",\"metadata\":{\"_type\":\"sanity.imageMetadata\",\"blurHash\":\"MDQ]$hxu~pt69Gxtj@xufRRj?aj?D*a$%L\",\"dimensions\":{\"_type\":\"sanity.imageDimensions\",\"aspectRatio\":1.9047619047619047,\"height\":1260,\"width\":2400},\"hasAlpha\":true,\"isOpaque\":true,\"lqip\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAABIElEQVR4nKWS206DUBBF+Wwf9D+Mxmj1RX0wJsQEe7Gx0hsCUkqplrZQKNCLte0XLAOJicZgSnzYyUzOZM2cPSNsPiL+0nYd/4i/lFUvZD2slgHz2GMx83hf+MwiF3dk4437aZ4LuF5F+N4butpA1xrYPQ25Vub05JDbm6sUujNwu06AIcNBl3LpjntJTGHXlxcc7O9xXjhm4vYzvy1kdQomDqrS4ulRpt1sUSlJFM6OKBdF4nCU38NZ5GNbA5R2D121MY0OzXoVQ2+mfm7zAwN6loPS7qM9v2LoHeTaA0qrxtQf5J9wuZgyHrpYpoNtOZjGC0VJpFqR8NwcS9l82/RyPiUOk5PxU0/NjoLVVYnDcX7g7wZhepuJkvjfwF31CQ+KJllyak9qAAAAAElFTkSuQmCC\",\"palette\":{\"_type\":\"sanity.imagePalette\",\"darkMuted\":{\"_type\":\"sanity.imagePaletteSwatch\",\"background\":\"#443c3b\",\"foreground\":\"#fff\",\"population\":0.03,\"title\":\"#fff\"},\"darkVibrant\":{\"_type\":\"sanity.imagePaletteSwatch\",\"background\":\"#724212\",\"foreground\":\"#fff\",\"population\":0,\"title\":\"#fff\"},\"dominant\":{\"_type\":\"sanity.imagePaletteSwatch\",\"background\":\"#c4c3d2\",\"foreground\":\"#000\",\"population\":4.42,\"title\":\"#fff\"},\"lightMuted\":{\"_type\":\"sanity.imagePaletteSwatch\",\"background\":\"#c4c3d2\",\"foreground\":\"#000\",\"population\":4.42,\"title\":\"#fff\"},\"lightVibrant\":{\"_type\":\"sanity.imagePaletteSwatch\",\"background\":\"#fcf4ec\",\"foreground\":\"#000\",\"population\":0.07,\"title\":\"#000\"},\"muted\":{\"_type\":\"sanity.imagePaletteSwatch\",\"background\":\"#7c7c74\",\"foreground\":\"#fff\",\"population\":0.04,\"title\":\"#fff\"},\"vibrant\":{\"_type\":\"sanity.imagePaletteSwatch\",\"background\":\"#dc7f22\",\"foreground\":\"#fff\",\"population\":0,\"title\":\"#fff\"}}},\"mimeType\":\"image/png\",\"originalFilename\":\"eng-blog-social-5.png\",\"path\":\"images/4zrzovbb/website/ea2bf01aa874d7ab776453e97dfeed5d2bf5a116-2400x1260.png\",\"sha1hash\":\"ea2bf01aa874d7ab776453e97dfeed5d2bf5a116\",\"size\":49250,\"uploadId\":\"SkHRiND5vMIo1c83PtNMSa12sqSDS4b2\",\"url\":\"https://cdn.sanity.io/images/4zrzovbb/website/ea2bf01aa874d7ab776453e97dfeed5d2bf5a116-2400x1260.png\"},\"description\":\"In this post, we explore strategies for effectively curating and managing their context.\"}},\"publishedOn\":\"2025-09-29\",\"slug\":{\"_type\":\"slug\",\"current\":\"effective-context-engineering-for-ai-agents\"},\"spotIllustration\":{\"_type\":\"image\",\"asset\":{\"_ref\":\"image-a048404a96b599af98c05da5bdd1db07222e4e7b-500x500-svg\",\"_type\":\"reference\"},\"height\":500,\"url\":\"https://cdn.sanity.io/images/4zrzovbb/website/a048404a96b599af98c05da5bdd1db07222e4e7b-500x500.svg\",\"width\":500},\"subjects\":[{\"_key\":\"Agents\",\"_type\":\"tag\",\"label\":\"Agents\",\"value\":\"Agents\"}],\"summary\":\"Context is a critical but finite resource for AI agents. In this post, we explore strategies for effectively curating and managing the context that powers them.\",\"title\":\"Effective context engineering for AI agents\"},\"siteSettings\":{\"_createdAt\":\"2023-11-03T16:49:36Z\",\"_id\":\"13c6e1a1-6f38-400c-ae18-89d73b6ba991\",\"_rev\":\"yTRh7AEP4D5VL0KlacX84u\",\"_system\":{\"base\":{\"id\":\"13c6e1a1-6f38-400c-ae18-89d73b6ba991\",\"rev\":\"l72pC1z2B6rbHkO6aMcVVS\"}},\"_type\":\"siteSettings\",\"_updatedAt\":\"2026-02-10T21:50:58Z\",\"announcement\":null,\"claudeCta\":{\"desktopCtas\":null,\"mobileCtas\":[{\"title\":\"Log in to Claude\",\"url\":\"https://claude.ai/login\"},{\"title\":\"Download app\",\"url\":\"https://claude.ai/download\"}],\"sections\":[{\"category\":\"Products\",\"links\":[{\"title\":\"Claude\",\"url\":\"https://claude.com/product/overview\"},{\"title\":\"Claude Code\",\"url\":\"https://claude.com/product/claude-code\"},{\"title\":\"Claude Developer Platform\",\"url\":\"https://claude.com/platform/api\"},{\"title\":\"Pricing\",\"url\":\"https://claude.com/pricing\"},{\"title\":\"Contact sales\",\"url\":\"https://claude.com/contact-sales\"}]},{\"category\":\"Models\",\"links\":[{\"title\":\"Opus\",\"url\":\"/claude/opus\"},{\"title\":\"Sonnet\",\"url\":\"/claude/sonnet\"},{\"title\":\"Haiku\",\"url\":\"/claude/haiku\"}]},{\"category\":\"Log in\",\"links\":[{\"title\":\"Claude.ai\",\"url\":\"https://claude.ai\"},{\"title\":\"Claude Console\",\"url\":\"https://platform.claude.com/\"}]}],\"title\":\"Try Claude\",\"url\":\"https://claude.ai/\"},\"copyright\":\"© 2026 Anthropic PBC\",\"footerNavigation\":[{\"_key\":\"716b96b62292\",\"links\":[{\"title\":\"Claude\",\"url\":\"https://claude.com/product/overview\"},{\"title\":\"Claude Code\",\"url\":\"https://claude.com/product/claude-code\"},{\"title\":\"Cowork\",\"url\":\"https://claude.com/product/cowork\"},{\"title\":\"Claude in Chrome\",\"url\":\"https://claude.com/chrome\"},{\"title\":\"Claude in Excel\",\"url\":\"https://claude.com/claude-in-excel\"},{\"title\":\"Claude in PowerPoint\",\"url\":\"https://claude.com/claude-in-powerpoint\"},{\"title\":\"Claude in Slack\",\"url\":\"https://claude.com/claude-in-slack\"},{\"title\":\"Skills\",\"url\":\"https://www.claude.com/skills\"},{\"title\":\"Max plan\",\"url\":\"https://claude.com/pricing/max\"},{\"title\":\"Team plan\",\"url\":\"https://claude.com/pricing/team\"},{\"title\":\"Enterprise plan\",\"url\":\"https://claude.com/pricing/enterprise\"},{\"title\":\"Download app\",\"url\":\"https://claude.ai/download\"},{\"title\":\"Pricing\",\"url\":\"https://claude.com/pricing\"},{\"title\":\"Log in to Claude\",\"url\":\"https://claude.ai/\"}],\"title\":\"Products\"},{\"_key\":\"0229138ff25d\",\"links\":[{\"title\":\"Opus\",\"url\":\"/www.anthropic.com/claude/opus\"},{\"title\":\"Sonnet\",\"url\":\"/www.anthropic.com/claude/sonnet\"},{\"title\":\"Haiku\",\"url\":\"/www.anthropic.com/claude/haiku\"}],\"title\":\"Models\"},{\"_key\":\"df2df9219e3abce95d6d83387e2d9bd6\",\"links\":[{\"title\":\"AI agents\",\"url\":\"https://claude.com/solutions/agents\"},{\"title\":\"Code modernization\",\"url\":\"https://claude.com/solutions/code-modernization\"},{\"title\":\"Coding\",\"url\":\"https://claude.com/solutions/coding\"},{\"title\":\"Customer support\",\"url\":\"https://claude.com/solutions/customer-support\"},{\"title\":\"Education\",\"url\":\"https://claude.com/solutions/education\"},{\"title\":\"Financial services\",\"url\":\"https://claude.com/solutions/financial-services\"},{\"title\":\"Government\",\"url\":\"https://claude.com/solutions/government\"},{\"title\":\"Healthcare\",\"url\":\"https://claude.com/solutions/healthcare\"},{\"title\":\"Life sciences\",\"url\":\"https://claude.com/solutions/life-sciences\"},{\"title\":\"Nonprofits\",\"url\":\"https://claude.com/solutions/nonprofits\"}],\"title\":\"Solutions\"},{\"_key\":\"f286ca01fc7aaabd131f347b711a971b\",\"links\":[{\"title\":\"Overview\",\"url\":\"https://claude.com/platform/api\"},{\"title\":\"Developer docs\",\"url\":\"https://platform.claude.com/docs\"},{\"title\":\"Pricing\",\"url\":\"https://claude.com/pricing#api\"},{\"title\":\"Regional compliance\",\"url\":\"https://claude.com/regional-compliance\"},{\"title\":\"Amazon Bedrock\",\"url\":\"https://claude.com/partners/amazon-bedrock\"},{\"title\":\"Google Cloud’s Vertex AI\",\"url\":\"https://claude.com/partners/google-cloud-vertex-ai\"},{\"title\":\"Console login\",\"url\":\"https://platform.claude.com/\"}],\"title\":\"Claude Developer Platform\"},{\"_key\":\"4b255e67f68c270e0072c7564e084e24\",\"links\":[{\"title\":\"Blog\",\"url\":\"https://claude.com/blog\"},{\"title\":\"Claude partner network\",\"url\":\"https://claude.com/partners\"},{\"title\":\"Connectors\",\"url\":\"https://claude.com/connectors\"},{\"title\":\"Courses\",\"url\":\"/learn\"},{\"title\":\"Customer stories\",\"url\":\"https://claude.com/customers\"},{\"title\":\"Engineering at Anthropic\",\"url\":\"/engineering\"},{\"title\":\"Events\",\"url\":\"/events\"},{\"title\":\"Plugins\",\"url\":\"https://claude.com/plugins\"},{\"title\":\"Powered by Claude\",\"url\":\"https://claude.com/partners/powered-by-claude\"},{\"title\":\"Service partners\",\"url\":\"https://claude.com/partners/services\"},{\"title\":\"Startups program\",\"url\":\"https://claude.com/programs/startups\"},{\"title\":\"Tutorials\",\"url\":\"https://claude.com/resources/tutorials\"},{\"title\":\"Use cases\",\"url\":\"https://claude.com/resources/use-cases\"}],\"title\":\"Learn\"},{\"_key\":\"4f2729951e15b0b870897e0444f5f3e1\",\"links\":[{\"title\":\"Anthropic\",\"url\":\"/company\"},{\"title\":\"Careers\",\"url\":\"/careers\"},{\"title\":\"Economic Futures\",\"url\":\"/economic-index\"},{\"title\":\"Research\",\"url\":\"/research\"},{\"title\":\"News\",\"url\":\"/news\"},{\"title\":\"Claude’s Constitution\",\"url\":\"/constitution\"},{\"title\":\"Responsible Scaling Policy\",\"url\":\"/www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy\"},{\"title\":\"Security and compliance\",\"url\":\"https://trust.anthropic.com/\"},{\"title\":\"Transparency\",\"url\":\"/transparency\"}],\"title\":\"Company\"},{\"_key\":\"a886dd1838335844d635f2857b25d66a\",\"links\":[{\"title\":\"Availability\",\"url\":\"/www.anthropic.com/supported-countries\"},{\"title\":\"Status\",\"url\":\"https://status.anthropic.com/\"},{\"title\":\"Support center\",\"url\":\"https://support.claude.com/en/\"}],\"title\":\"Help and security\"},{\"_key\":\"3c3b033c11fa832a35d43b87d55a5364\",\"links\":[{\"title\":\"Privacy choices\",\"url\":\"#\"},{\"title\":\"Privacy policy\",\"url\":\"/www.anthropic.com/legal/privacy\"},{\"title\":\"Consumer health data privacy policy\",\"url\":\"/www.anthropic.com/legal/consumer-health-data-privacy-policy\"},{\"title\":\"Responsible disclosure policy\",\"url\":\"/www.anthropic.com/responsible-disclosure-policy\"},{\"title\":\"Terms of service: Commercial\",\"url\":\"/www.anthropic.com/legal/commercial-terms\"},{\"title\":\"Terms of service: Consumer\",\"url\":\"/www.anthropic.com/legal/consumer-terms\"},{\"title\":\"Usage policy\",\"url\":\"/www.anthropic.com/legal/aup\"}],\"title\":\"Terms and policies\"}],\"headerNavigation\":[{\"_key\":\"a340f9d4b859\",\"category\":\"Research\",\"displayType\":\"singleLink\",\"sections\":null,\"url\":\"/research\"},{\"_key\":\"a483c7dfd38a\",\"category\":\"Economic Futures\",\"displayType\":\"singleLink\",\"sections\":null,\"url\":\"/economic-futures\"},{\"_key\":\"82c471bd311d\",\"category\":\"Commitments\",\"displayType\":\"sections\",\"sections\":[{\"_key\":\"675871636e4d\",\"links\":[{\"title\":\"Claude’s Constitution\",\"url\":\"/constitution\"},{\"title\":\" Transparency\",\"url\":\"/transparency\"},{\"title\":\"Responsible Scaling Policy\",\"url\":\"/news/announcing-our-updated-responsible-scaling-policy\"}],\"title\":\"Initiatives\"},{\"_key\":\"16af50a6e2dd\",\"links\":[{\"title\":\"Security and compliance\",\"url\":\"https://trust.anthropic.com/\"}],\"title\":\"Trust center\"}]},{\"_key\":\"861a11ed9931\",\"category\":\"Learn\",\"displayType\":\"sections\",\"sections\":[{\"_key\":\"9f9f720a8793\",\"links\":[{\"title\":\"Anthropic Academy\",\"url\":\"/learn\"},{\"title\":\"Tutorials\",\"url\":\"https://claude.com/resources/tutorials\"},{\"title\":\"Use cases\",\"url\":\"https://claude.com/resources/use-cases\"},{\"title\":\"Engineering at Anthropic\",\"url\":\"/engineering\"},{\"title\":\"Developer docs\",\"url\":\"https://docs.claude.com\"}],\"title\":\"Learn\"},{\"_key\":\"6bd061c46b10\",\"links\":[{\"title\":\"About\",\"url\":\"/company\"},{\"title\":\"Careers\",\"url\":\"/careers\"},{\"title\":\"Events\",\"url\":\"/events\"}],\"title\":\"Company\"}]},{\"_key\":\"22e8d8d2923d\",\"category\":\"News\",\"displayType\":\"singleLink\",\"sections\":null,\"url\":\"/news\"}],\"internalName\":\"anthropic.com Site Settings\",\"linkedInUsername\":\"anthropicresearch\",\"meta\":{\"_createdAt\":\"2023-11-20T21:56:31Z\",\"_id\":\"0f6290ad-6d21-407d-8deb-ce02815d1383\",\"_rev\":\"NyW74GU9ZzyWgAYa8qUSlF\",\"_type\":\"metadata\",\"_updatedAt\":\"2023-11-20T23:54:09Z\",\"robotsIndexable\":true,\"seoDescription\":\"Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.\",\"seoTitle\":\"Anthropic\",\"socialImage\":{\"_type\":\"image\",\"asset\":{\"_createdAt\":\"2025-05-23T14:14:18Z\",\"_id\":\"image-c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260-jpg\",\"_rev\":\"v1N2wBpLqoO2Q3HXueYiJi\",\"_type\":\"sanity.imageAsset\",\"_updatedAt\":\"2025-05-23T14:14:18Z\",\"assetId\":\"c07f638082c569e8ce1e89ae95ee6f332a98ec08\",\"extension\":\"jpg\",\"metadata\":{\"_type\":\"sanity.imageMetadata\",\"blurHash\":\"MASPU,%M?b%Ms:-;j[j[j[fQ~qj[9FayWB\",\"dimensions\":{\"_type\":\"sanity.imageDimensions\",\"aspectRatio\":1.9047619047619047,\"height\":1260,\"width\":2400},\"hasAlpha\":false,\"isOpaque\":true,\"lqip\":\"data:image/jpeg;base64,/9j/2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj/2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj/wAARCAAKABQDASIAAhEBAxEB/8QAFwABAAMAAAAAAAAAAAAAAAAAAAQFCP/EACAQAAEEAgEFAAAAAAAAAAAAAAABAgMEBRETEhQhIjH/xAAWAQEBAQAAAAAAAAAAAAAAAAAAAQL/xAAUEQEAAAAAAAAAAAAAAAAAAAAA/9oADAMBAAIRAxEAPwDS96axCjO2r8yqvn21oiR3ci6TT8d0t395ELUGVAAB/9k=\",\"palette\":{\"_type\":\"sanity.imagePalette\",\"darkMuted\":{\"_type\":\"sanity.imagePaletteSwatch\",\"background\":\"#444440\",\"foreground\":\"#fff\",\"population\":0.05,\"title\":\"#fff\"},\"darkVibrant\":{\"_type\":\"sanity.imagePaletteSwatch\",\"background\":\"#68681c\",\"foreground\":\"#fff\",\"population\":0,\"title\":\"#fff\"},\"dominant\":{\"_type\":\"sanity.imagePaletteSwatch\",\"background\":\"#fcfcf4\",\"foreground\":\"#000\",\"population\":90.85,\"title\":\"#000\"},\"lightMuted\":{\"_type\":\"sanity.imagePaletteSwatch\",\"background\":\"#bcbcb4\",\"foreground\":\"#000\",\"population\":0.03,\"title\":\"#fff\"},\"lightVibrant\":{\"_type\":\"sanity.imagePaletteSwatch\",\"background\":\"#fcfcf4\",\"foreground\":\"#000\",\"population\":90.85,\"title\":\"#000\"},\"muted\":{\"_type\":\"sanity.imagePaletteSwatch\",\"background\":\"#7c7c74\",\"foreground\":\"#fff\",\"population\":0.02,\"title\":\"#fff\"},\"vibrant\":{\"_type\":\"sanity.imagePaletteSwatch\",\"background\":\"#c8c836\",\"foreground\":\"#000\",\"population\":0,\"title\":\"#fff\"}}},\"mimeType\":\"image/jpeg\",\"originalFilename\":\"Anthropic-OG-image.jpg\",\"path\":\"images/4zrzovbb/website/c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260.jpg\",\"sha1hash\":\"c07f638082c569e8ce1e89ae95ee6f332a98ec08\",\"size\":132598,\"uploadId\":\"pxmJEaCvYm0cHoZTfnCcZYXxrWKBhHf0\",\"url\":\"https://cdn.sanity.io/images/4zrzovbb/website/c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260.jpg\"},\"description\":\"Anthropic logo\"}},\"siteName\":\"Anthropic\",\"sitemapUrls\":[\"/\",\"/careers\",\"/company\",\"/events\",\"/events/aws-summit-dc\",\"/events/aws-summit-nyc\",\"/events/aws-summit-london\",\"/events/aws-summit-tokyo\",\"/events/claude-for-finance\",\"/events/google-cloud-next-2025\",\"/events/paris-builder-summit\",\"/events/seoul-builder-summit\",\"/learn\",\"/supported-countries\",\"/unsubscribe\"],\"twitterUsername\":\"AnthropicAI\",\"youtubeUsername\":\"anthropic-ai\"}}]\n"])</script><script nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx">self.__next_f.push([1,"f:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"2\",{\"name\":\"theme-color\",\"content\":\"#141413\"}]]\n"])</script><script nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx">self.__next_f.push([1,"15:[\"$\",\"$L17\",null,{\"children\":[null,[\"$\",\"$L18\",null,{\"isMinimalNavigation\":\"$undefined\",\"siteSettings\":{\"_createdAt\":\"2023-11-03T16:49:36Z\",\"_id\":\"13c6e1a1-6f38-400c-ae18-89d73b6ba991\",\"_rev\":\"yTRh7AEP4D5VL0KlacX84u\",\"_system\":{\"base\":{\"id\":\"13c6e1a1-6f38-400c-ae18-89d73b6ba991\",\"rev\":\"l72pC1z2B6rbHkO6aMcVVS\"}},\"_type\":\"siteSettings\",\"_updatedAt\":\"2026-02-10T21:50:58Z\",\"announcement\":null,\"claudeCta\":{\"desktopCtas\":null,\"mobileCtas\":[{\"title\":\"Log in to Claude\",\"url\":\"https://claude.ai/login\"},{\"title\":\"Download app\",\"url\":\"https://claude.ai/download\"}],\"sections\":[{\"category\":\"Products\",\"links\":[{\"title\":\"Claude\",\"url\":\"https://claude.com/product/overview\"},{\"title\":\"Claude Code\",\"url\":\"https://claude.com/product/claude-code\"},{\"title\":\"Claude Developer Platform\",\"url\":\"https://claude.com/platform/api\"},{\"title\":\"Pricing\",\"url\":\"https://claude.com/pricing\"},{\"title\":\"Contact sales\",\"url\":\"https://claude.com/contact-sales\"}]},{\"category\":\"Models\",\"links\":[{\"title\":\"Opus\",\"url\":\"/claude/opus\"},{\"title\":\"Sonnet\",\"url\":\"/claude/sonnet\"},{\"title\":\"Haiku\",\"url\":\"/claude/haiku\"}]},{\"category\":\"Log in\",\"links\":[{\"title\":\"Claude.ai\",\"url\":\"https://claude.ai\"},{\"title\":\"Claude Console\",\"url\":\"https://platform.claude.com/\"}]}],\"title\":\"Try Claude\",\"url\":\"https://claude.ai/\"},\"copyright\":\"© 2026 Anthropic PBC\",\"footerNavigation\":[{\"_key\":\"716b96b62292\",\"links\":[{\"title\":\"Claude\",\"url\":\"https://claude.com/product/overview\"},{\"title\":\"Claude Code\",\"url\":\"https://claude.com/product/claude-code\"},{\"title\":\"Cowork\",\"url\":\"https://claude.com/product/cowork\"},{\"title\":\"Claude in Chrome\",\"url\":\"https://claude.com/chrome\"},{\"title\":\"Claude in Excel\",\"url\":\"https://claude.com/claude-in-excel\"},{\"title\":\"Claude in PowerPoint\",\"url\":\"https://claude.com/claude-in-powerpoint\"},{\"title\":\"Claude in Slack\",\"url\":\"https://claude.com/claude-in-slack\"},{\"title\":\"Skills\",\"url\":\"https://www.claude.com/skills\"},{\"title\":\"Max plan\",\"url\":\"https://claude.com/pricing/max\"},{\"title\":\"Team plan\",\"url\":\"https://claude.com/pricing/team\"},{\"title\":\"Enterprise plan\",\"url\":\"https://claude.com/pricing/enterprise\"},{\"title\":\"Download app\",\"url\":\"https://claude.ai/download\"},{\"title\":\"Pricing\",\"url\":\"https://claude.com/pricing\"},{\"title\":\"Log in to Claude\",\"url\":\"https://claude.ai/\"}],\"title\":\"Products\"},{\"_key\":\"0229138ff25d\",\"links\":[{\"title\":\"Opus\",\"url\":\"/www.anthropic.com/claude/opus\"},{\"title\":\"Sonnet\",\"url\":\"/www.anthropic.com/claude/sonnet\"},{\"title\":\"Haiku\",\"url\":\"/www.anthropic.com/claude/haiku\"}],\"title\":\"Models\"},{\"_key\":\"df2df9219e3abce95d6d83387e2d9bd6\",\"links\":[{\"title\":\"AI agents\",\"url\":\"https://claude.com/solutions/agents\"},{\"title\":\"Code modernization\",\"url\":\"https://claude.com/solutions/code-modernization\"},{\"title\":\"Coding\",\"url\":\"https://claude.com/solutions/coding\"},{\"title\":\"Customer support\",\"url\":\"https://claude.com/solutions/customer-support\"},{\"title\":\"Education\",\"url\":\"https://claude.com/solutions/education\"},{\"title\":\"Financial services\",\"url\":\"https://claude.com/solutions/financial-services\"},{\"title\":\"Government\",\"url\":\"https://claude.com/solutions/government\"},{\"title\":\"Healthcare\",\"url\":\"https://claude.com/solutions/healthcare\"},{\"title\":\"Life sciences\",\"url\":\"https://claude.com/solutions/life-sciences\"},{\"title\":\"Nonprofits\",\"url\":\"https://claude.com/solutions/nonprofits\"}],\"title\":\"Solutions\"},{\"_key\":\"f286ca01fc7aaabd131f347b711a971b\",\"links\":[{\"title\":\"Overview\",\"url\":\"https://claude.com/platform/api\"},{\"title\":\"Developer docs\",\"url\":\"https://platform.claude.com/docs\"},{\"title\":\"Pricing\",\"url\":\"https://claude.com/pricing#api\"},{\"title\":\"Regional compliance\",\"url\":\"https://claude.com/regional-compliance\"},{\"title\":\"Amazon Bedrock\",\"url\":\"https://claude.com/partners/amazon-bedrock\"},{\"title\":\"Google Cloud’s Vertex AI\",\"url\":\"https://claude.com/partners/google-cloud-vertex-ai\"},{\"title\":\"Console login\",\"url\":\"https://platform.claude.com/\"}],\"title\":\"Claude Developer Platform\"},{\"_key\":\"4b255e67f68c270e0072c7564e084e24\",\"links\":[{\"title\":\"Blog\",\"url\":\"https://claude.com/blog\"},{\"title\":\"Claude partner network\",\"url\":\"https://claude.com/partners\"},{\"title\":\"Connectors\",\"url\":\"https://claude.com/connectors\"},{\"title\":\"Courses\",\"url\":\"/learn\"},{\"title\":\"Customer stories\",\"url\":\"https://claude.com/customers\"},{\"title\":\"Engineering at Anthropic\",\"url\":\"/engineering\"},{\"title\":\"Events\",\"url\":\"/events\"},{\"title\":\"Plugins\",\"url\":\"https://claude.com/plugins\"},{\"title\":\"Powered by Claude\",\"url\":\"https://claude.com/partners/powered-by-claude\"},{\"title\":\"Service partners\",\"url\":\"https://claude.com/partners/services\"},{\"title\":\"Startups program\",\"url\":\"https://claude.com/programs/startups\"},{\"title\":\"Tutorials\",\"url\":\"https://claude.com/resources/tutorials\"},{\"title\":\"Use cases\",\"url\":\"https://claude.com/resources/use-cases\"}],\"title\":\"Learn\"},{\"_key\":\"4f2729951e15b0b870897e0444f5f3e1\",\"links\":[{\"title\":\"Anthropic\",\"url\":\"/company\"},{\"title\":\"Careers\",\"url\":\"/careers\"},{\"title\":\"Economic Futures\",\"url\":\"/economic-index\"},{\"title\":\"Research\",\"url\":\"/research\"},{\"title\":\"News\",\"url\":\"/news\"},{\"title\":\"Claude’s Constitution\",\"url\":\"/constitution\"},{\"title\":\"Responsible Scaling Policy\",\"url\":\"/www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy\"},{\"title\":\"Security and compliance\",\"url\":\"https://trust.anthropic.com/\"},{\"title\":\"Transparency\",\"url\":\"/transparency\"}],\"title\":\"Company\"},{\"_key\":\"a886dd1838335844d635f2857b25d66a\",\"links\":[{\"title\":\"Availability\",\"url\":\"/www.anthropic.com/supported-countries\"},{\"title\":\"Status\",\"url\":\"https://status.anthropic.com/\"},{\"title\":\"Support center\",\"url\":\"https://support.claude.com/en/\"}],\"title\":\"Help and security\"},{\"_key\":\"3c3b033c11fa832a35d43b87d55a5364\",\"links\":[{\"title\":\"Privacy choices\",\"url\":\"#\"},{\"title\":\"Privacy policy\",\"url\":\"/www.anthropic.com/legal/privacy\"},{\"title\":\"Consumer health data privacy policy\",\"url\":\"/www.anthropic.com/legal/consumer-health-data-privacy-policy\"},{\"title\":\"Responsible disclosure policy\",\"url\":\"/www.anthropic.com/responsible-disclosure-policy\"},{\"title\":\"Terms of service: Commercial\",\"url\":\"/www.anthropic.com/legal/commercial-terms\"},{\"title\":\"Terms of service: Consumer\",\"url\":\"/www.anthropic.com/legal/consumer-terms\"},{\"title\":\"Usage policy\",\"url\":\"/www.anthropic.com/legal/aup\"}],\"title\":\"Terms and policies\"}],\"headerNavigation\":[{\"_key\":\"a340f9d4b859\",\"category\":\"Research\",\"displayType\":\"singleLink\",\"sections\":null,\"url\":\"/research\"},{\"_key\":\"a483c7dfd38a\",\"category\":\"Economic Futures\",\"displayType\":\"singleLink\",\"sections\":null,\"url\":\"/economic-futures\"},{\"_key\":\"82c471bd311d\",\"category\":\"Commitments\",\"displayType\":\"sections\",\"sections\":[{\"_key\":\"675871636e4d\",\"links\":[{\"title\":\"Claude’s Constitution\",\"url\":\"/constitution\"},{\"title\":\" Transparency\",\"url\":\"/transparency\"},{\"title\":\"Responsible Scaling Policy\",\"url\":\"/news/announcing-our-updated-responsible-scaling-policy\"}],\"title\":\"Initiatives\"},{\"_key\":\"16af50a6e2dd\",\"links\":[{\"title\":\"Security and compliance\",\"url\":\"https://trust.anthropic.com/\"}],\"title\":\"Trust center\"}]},{\"_key\":\"861a11ed9931\",\"category\":\"Learn\",\"displayType\":\"sections\",\"sections\":[{\"_key\":\"9f9f720a8793\",\"links\":[{\"title\":\"Anthropic Academy\",\"url\":\"/learn\"},{\"title\":\"Tutorials\",\"url\":\"https://claude.com/resources/tutorials\"},{\"title\":\"Use cases\",\"url\":\"https://claude.com/resources/use-cases\"},{\"title\":\"Engineering at Anthropic\",\"url\":\"/engineering\"},{\"title\":\"Developer docs\",\"url\":\"https://docs.claude.com\"}],\"title\":\"Learn\"},{\"_key\":\"6bd061c46b10\",\"links\":[{\"title\":\"About\",\"url\":\"/company\"},{\"title\":\"Careers\",\"url\":\"/careers\"},{\"title\":\"Events\",\"url\":\"/events\"}],\"title\":\"Company\"}]},{\"_key\":\"22e8d8d2923d\",\"category\":\"News\",\"displayType\":\"singleLink\",\"sections\":null,\"url\":\"/news\"}],\"internalName\":\"anthropic.com Site Settings\",\"linkedInUsername\":\"anthropicresearch\",\"meta\":{\"_createdAt\":\"2023-11-20T21:56:31Z\",\"_id\":\"0f6290ad-6d21-407d-8deb-ce02815d1383\",\"_rev\":\"NyW74GU9ZzyWgAYa8qUSlF\",\"_type\":\"metadata\",\"_updatedAt\":\"2023-11-20T23:54:09Z\",\"robotsIndexable\":true,\"seoDescription\":\"Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.\",\"seoTitle\":\"Anthropic\",\"socialImage\":{\"_type\":\"image\",\"asset\":{\"_createdAt\":\"2025-05-23T14:14:18Z\",\"_id\":\"image-c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260-jpg\",\"_rev\":\"v1N2wBpLqoO2Q3HXueYiJi\",\"_type\":\"sanity.imageAsset\",\"_updatedAt\":\"2025-05-23T14:14:18Z\",\"assetId\":\"c07f638082c569e8ce1e89ae95ee6f332a98ec08\",\"extension\":\"jpg\",\"metadata\":{\"_type\":\"sanity.imageMetadata\",\"blurHash\":\"MASPU,%M?b%Ms:-;j[j[j[fQ~qj[9FayWB\",\"dimensions\":{\"_type\":\"sanity.imageDimensions\",\"aspectRatio\":1.9047619047619047,\"height\":1260,\"width\":2400},\"hasAlpha\":false,\"isOpaque\":true,\"lqip\":\"data:image/jpeg;base64,/9j/2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj/2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj/wAARCAAKABQDASIAAhEBAxEB/8QAFwABAAMAAAAAAAAAAAAAAAAAAAQFCP/EACAQAAEEAgEFAAAAAAAAAAAAAAABAgMEBRETEhQhIjH/xAAWAQEBAQAAAAAAAAAAAAAAAAAAAQL/xAAUEQEAAAAAAAAAAAAAAAAAAAAA/9oADAMBAAIRAxEAPwDS96axCjO2r8yqvn21oiR3ci6TT8d0t395ELUGVAAB/9k=\",\"palette\":{\"_type\":\"sanity.imagePalette\",\"darkMuted\":{\"_type\":\"sanity.imagePaletteSwatch\",\"background\":\"#444440\",\"foreground\":\"#fff\",\"population\":0.05,\"title\":\"#fff\"},\"darkVibrant\":{\"_type\":\"sanity.imagePaletteSwatch\",\"background\":\"#68681c\",\"foreground\":\"#fff\",\"population\":0,\"title\":\"#fff\"},\"dominant\":{\"_type\":\"sanity.imagePaletteSwatch\",\"background\":\"#fcfcf4\",\"foreground\":\"#000\",\"population\":90.85,\"title\":\"#000\"},\"lightMuted\":{\"_type\":\"sanity.imagePaletteSwatch\",\"background\":\"#bcbcb4\",\"foreground\":\"#000\",\"population\":0.03,\"title\":\"#fff\"},\"lightVibrant\":{\"_type\":\"sanity.imagePaletteSwatch\",\"background\":\"#fcfcf4\",\"foreground\":\"#000\",\"population\":90.85,\"title\":\"#000\"},\"muted\":{\"_type\":\"sanity.imagePaletteSwatch\",\"background\":\"#7c7c74\",\"foreground\":\"#fff\",\"population\":0.02,\"title\":\"#fff\"},\"vibrant\":{\"_type\":\"sanity.imagePaletteSwatch\",\"background\":\"#c8c836\",\"foreground\":\"#000\",\"population\":0,\"title\":\"#fff\"}}},\"mimeType\":\"image/jpeg\",\"originalFilename\":\"Anthropic-OG-image.jpg\",\"path\":\"images/4zrzovbb/website/c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260.jpg\",\"sha1hash\":\"c07f638082c569e8ce1e89ae95ee6f332a98ec08\",\"size\":132598,\"uploadId\":\"pxmJEaCvYm0cHoZTfnCcZYXxrWKBhHf0\",\"url\":\"https://cdn.sanity.io/images/4zrzovbb/website/c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260.jpg\"},\"description\":\"Anthropic logo\"}},\"siteName\":\"Anthropic\",\"sitemapUrls\":[\"/\",\"/careers\",\"/company\",\"/events\",\"/events/aws-summit-dc\",\"/events/aws-summit-nyc\",\"/events/aws-summit-london\",\"/events/aws-summit-tokyo\",\"/events/claude-for-finance\",\"/events/google-cloud-next-2025\",\"/events/paris-builder-summit\",\"/events/seoul-builder-summit\",\"/learn\",\"/supported-countries\",\"/unsubscribe\"],\"twitterUsername\":\"AnthropicAI\",\"youtubeUsername\":\"anthropic-ai\",\"hideFooter\":true},\"page\":{\"_type\":\"page\",\"_id\":\"not-found\",\"_rev\":\"\",\"_createdAt\":\"\",\"_updatedAt\":\"\",\"title\":\"Not Found\",\"slug\":{\"_type\":\"slug\",\"current\":\"not-found\"},\"meta\":{},\"sections\":[]},\"theme\":\"$undefined\"}],\"$L19\",null]}]\n"])</script><script nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx">self.__next_f.push([1,"19:[\"$\",\"main\",null,{\"id\":\"main-content\",\"className\":\"\",\"children\":[\"$\",\"$L1a\",null,{}]}]\n"])</script><script nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx">self.__next_f.push([1,"11:[[\"$\",\"title\",\"0\",{\"children\":\"Effective context engineering for AI agents \\\\ Anthropic\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.\"}],[\"$\",\"meta\",\"2\",{\"name\":\"msapplication-TileColor\",\"content\":\"141413\"}],[\"$\",\"meta\",\"3\",{\"name\":\"msapplication-config\",\"content\":\"/browserconfig.xml\"}],[\"$\",\"meta\",\"4\",{\"property\":\"og:title\",\"content\":\"Effective context engineering for AI agents\"}],[\"$\",\"meta\",\"5\",{\"property\":\"og:description\",\"content\":\"Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:image\",\"content\":\"https://cdn.sanity.io/images/4zrzovbb/website/ea2bf01aa874d7ab776453e97dfeed5d2bf5a116-2400x1260.png\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:image:alt\",\"content\":\"In this post, we explore strategies for effectively curating and managing their context.\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"9\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"10\",{\"name\":\"twitter:site\",\"content\":\"@AnthropicAI\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:creator\",\"content\":\"@AnthropicAI\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:title\",\"content\":\"Effective context engineering for AI agents\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:description\",\"content\":\"Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.\"}],[\"$\",\"meta\",\"14\",{\"name\":\"twitter:image\",\"content\":\"https://cdn.sanity.io/images/4zrzovbb/website/ea2bf01aa874d7ab776453e97dfeed5d2bf5a116-2400x1260.png\"}],[\"$\",\"meta\",\"15\",{\"name\":\"twitter:image:alt\",\"content\":\"In this post, we explore strategies for effectively curating and managing their context.\"}],[\"$\",\"link\",\"16\",{\"rel\":\"shortcut icon\",\"href\":\"/favicon.ico\"}],[\"$\",\"link\",\"17\",{\"rel\":\"icon\",\"href\":\"/images/icons/favicon-32x32.png\"}],[\"$\",\"link\",\"18\",{\"rel\":\"apple-touch-icon\",\"href\":\"/images/icons/apple-touch-icon.png\"}],[\"$\",\"link\",\"19\",{\"rel\":\"apple-touch-icon\",\"href\":\"/images/icons/apple-touch-icon.png\",\"sizes\":\"180x180\"}],[\"$\",\"link\",\"20\",{\"rel\":\"mask-icon\",\"href\":\"/images/icons/safari-pinned-tab.svg\",\"color\":\"141413\"}],[\"$\",\"$L1b\",\"21\",{}]]\n"])</script><script nonce="OTQ3MTgyNjQtYmE0Mi00ZDBlLThkNmQtNGZlMmJlODgxMTYx">self.__next_f.push([1,"d:null\n"])</script></body></html>